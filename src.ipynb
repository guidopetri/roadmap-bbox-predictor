{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to keep the code here. And then export into a .py file.\n",
    "\n",
    "Do not try to modify the .py file directly until this is notebook is gone from each branch.\n",
    "\n",
    "Remember to clear output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils and calculating loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming Coordinates\n",
    "\n",
    "Define the given coordinates as world coordinates\n",
    "\n",
    "Define normalized from upper left bound of world coordinates (translate to there, rotate, and normalize) as our normalized image coordinates (or image coordinates for short).\n",
    "\n",
    "Always facing right in world coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 40\n",
    "WIDTH = 2 * 40\n",
    "HEIGHT = 2 * 40\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "device = 'cuda:0' if cuda else 'cpu'\n",
    "FloatTensor = FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "## input is the \n",
    "# want output that is\n",
    "# [batch_index, class_index, x_center, y_center, width, height, tx1, tx2, tx3, tx4, ty1, ty2, ty3, ty4]\n",
    "# where the tXN ranges from -1 to 1 and is the location of that coordinate in terms of +- w/2 or h/2\n",
    "def transform_target(in_target):\n",
    "    \n",
    "    out_target = []\n",
    "    \n",
    "    for tgt_index in range(len(in_target)):\n",
    "        \n",
    "        #how many boxes for these target\n",
    "        nbox = in_target[tgt_index]['bounding_box'].shape[0]\n",
    "        individual_target = FloatTensor(nbox, 14).fill_(0)\n",
    "        \n",
    "        # CONVERT ALL THE BOUNDING BOXES for an individual sample at once\n",
    "        \n",
    "        bbox = in_target[tgt_index]['bounding_box'].to(device)\n",
    "        translation = FloatTensor(bbox.shape[0], bbox.shape[1], bbox.shape[2])\n",
    "        translation[:, 0, :].fill_(-40)\n",
    "        translation[:, 1, :].fill_(40)\n",
    "\n",
    "        # translate to uppert left\n",
    "        box = bbox - translation\n",
    "        # reflect y\n",
    "        box[:, 1, :].mul_(-1)\n",
    "\n",
    "        x_min = box[:, 0].min(dim = 1)[0]\n",
    "        y_min = box[:, 1].min(dim = 1)[0]\n",
    "        x_max = box[:, 0].max(dim = 1)[0]\n",
    "        y_max = box[:, 1].max(dim = 1)[0]\n",
    "\n",
    "        x_center = ((x_min + x_max) / 2)\n",
    "        y_center = ((y_min + y_max) / 2)\n",
    "        width = (x_max - x_min)\n",
    "        height = (y_max - y_min)\n",
    "\n",
    "        # already normalized\n",
    "        tx = (box [:, 0, :] - x_center.view(-1, 1)) / (width.view(-1, 1) / 2)\n",
    "        ty = (box [:, 1, :] - y_center.view(-1, 1)) / (height.view(-1, 1) / 2)\n",
    "\n",
    "        x_center_n = x_center / WIDTH\n",
    "        y_center_n = y_center / HEIGHT\n",
    "        width_n = width / WIDTH\n",
    "        height_n = height / HEIGHT\n",
    "\n",
    "        individual_target[:, 2] = x_center_n\n",
    "        individual_target[:, 3] = y_center_n\n",
    "        individual_target[:, 4] = width_n\n",
    "        individual_target[:, 5] = height_n\n",
    "        \n",
    "        individual_target[:, 6:10] = tx\n",
    "        individual_target[:, 10:14] = ty\n",
    "        for box_index in range(nbox):\n",
    "            \n",
    "            \n",
    "            category = in_target[tgt_index]['category'][box_index]\n",
    "            \n",
    "            # from which sample in the batch\n",
    "            individual_target[box_index, 0] = tgt_index\n",
    "            # class\n",
    "            individual_target[box_index, 1] = category\n",
    "            \n",
    "        \n",
    "        out_target.append(individual_target)\n",
    "        \n",
    "    return torch.cat(out_target, dim = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load presaved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works by side effects\n",
    "def load_pretask_weight_from_model(model, presaved_encoder):\n",
    "    model.encoder.load_state_dict(presaved_encoder.state_dict())\n",
    "    \n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you want Initialize Our Model with encoder weights from an existing pretask encoder in memory\n",
    "def initialize_model_for_training(presaved_encoder):\n",
    "    model = KobeModel()\n",
    "    load_pretask_weight_from_model(model, presaved_encoder)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you want Initialize Our Model with encoder weights from a file\n",
    "def initialize_model_for_training_file(presaved_encoder_file):\n",
    "    presaved_encoder = PreTaskEncoder()\n",
    "    presaved_encoder.load_state_dict(torch.load(presaved_encoder_file))\n",
    "    presaved_encoder.eval()\n",
    "\n",
    "    \n",
    "    return initialize_model_for_training(presaved_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting predictions to the format for competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to calculate bounding boxes and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/master/utils/utils.py\n",
    "\n",
    "def bbox_wh_iou(wh1, wh2):\n",
    "    wh2 = wh2.t()\n",
    "    w1, h1 = wh1[0], wh1[1]\n",
    "    w2, h2 = wh2[0], wh2[1]\n",
    "    inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n",
    "    union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n",
    "    return inter_area / union_area\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes\n",
    "    \"\"\"\n",
    "    if not x1y1x2y2:\n",
    "        # Transform from center and width to exact coordinates\n",
    "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "    else:\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "\n",
    "    # get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "    # Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n",
    "        inter_rect_y2 - inter_rect_y1 + 1, min=0\n",
    "    )\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4):\n",
    "    \"\"\"\n",
    "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
    "    Non-Maximum Suppression to further filter detections.\n",
    "    Returns detections with shape:\n",
    "        (x1, y1, x2, y2, x1, x2, x3, x4, y1, y2, y3, y4, object_conf, class_score, class_pred)\n",
    "\n",
    "        \n",
    "        # where the x1, ..., x4 and y1, ... y4 are stll from -1 to 1\n",
    "        # first x1, y1, x2, y2 are in the grid coordinates and need to be converted back\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "    prediction[..., :4] = xywh2xyxy(prediction[..., :4])\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for image_i, image_pred in enumerate(prediction):\n",
    "        # Filter out confidence scores below threshold\n",
    "        image_pred = image_pred[image_pred[:, 12] >= conf_thres]\n",
    "        # If none are remaining => process next image\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        # Object confidence times class confidence\n",
    "        score = image_pred[:, 12] * image_pred[:, 13:].max(1)[0]\n",
    "        # Sort by it\n",
    "        image_pred = image_pred[(-score).argsort()]\n",
    "        class_confs, class_preds = image_pred[:, 13:].max(1, keepdim=True)\n",
    "        detections = torch.cat((image_pred[:, :13], class_confs.float(), class_preds.float()), 1)\n",
    "        # Perform non-maximum suppression\n",
    "        keep_boxes = []\n",
    "        while detections.size(0):\n",
    "            large_overlap = bbox_iou(detections[0, :4].unsqueeze(0), detections[:, :4]) > nms_thres\n",
    "            label_match = detections[0, -1] == detections[:, -1]\n",
    "            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n",
    "            invalid = large_overlap & label_match\n",
    "            weights = detections[invalid, 12:13]\n",
    "            # Merge overlapping bboxes by order of confidence\n",
    "            detections[0, :12] = (weights * detections[invalid, :12]).sum(0) / weights.sum()\n",
    "            keep_boxes += [detections[0]]\n",
    "            detections = detections[~invalid]\n",
    "        if keep_boxes:\n",
    "            output[image_i] = torch.stack(keep_boxes)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n",
    "\n",
    "    ByteTensor = torch.cuda.ByteTensor if pred_boxes.is_cuda else torch.ByteTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n",
    "\n",
    "    nB = pred_boxes.size(0)\n",
    "    nA = pred_boxes.size(1)\n",
    "    nC = pred_cls.size(-1)\n",
    "    nG = pred_boxes.size(2)\n",
    "\n",
    "    # Output tensors\n",
    "    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)\n",
    "    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)\n",
    "    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    \n",
    "    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n",
    "    \n",
    "    ### predict additional coordinates for the center within the anchor box\n",
    "    ##### THIS IS OUR ADDITION\n",
    "    ##### RANGE OF THESE VALUES TO BE DETERMINED\n",
    "    tx1 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx2 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx3 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx4 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty1 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty2 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty3 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty4 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "\n",
    "    # Convert to position relative to box\n",
    "    target_boxes = target[:, 2:6] * nG\n",
    "    gxy = target_boxes[:, :2]\n",
    "    gwh = target_boxes[:, 2:]\n",
    "\n",
    "    \n",
    "    \n",
    "    # Get anchors with best iou\n",
    "    ious = torch.stack([bbox_wh_iou(anchor, gwh) for anchor in anchors])\n",
    "    best_ious, best_n = ious.max(0)\n",
    "    # Separate target values\n",
    "    b, target_labels = target[:, :2].long().t()\n",
    "    gx, gy = gxy.t()\n",
    "    gw, gh = gwh.t()\n",
    "    gi, gj = gxy.long().t()\n",
    "    # Set masks\n",
    "    obj_mask[b, best_n, gj, gi] = 1\n",
    "    noobj_mask[b, best_n, gj, gi] = 0\n",
    "\n",
    "    # Set noobj mask to zero where iou exceeds ignore threshold\n",
    "    for i, anchor_ious in enumerate(ious.t()):\n",
    "        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n",
    "\n",
    "    # Coordinates\n",
    "    tx[b, best_n, gj, gi] = gx - gx.floor()\n",
    "    ty[b, best_n, gj, gi] = gy - gy.floor()\n",
    "    \n",
    "    \n",
    "    tx1[b, best_n, gj, gi] = target[:, 6]\n",
    "    tx2[b, best_n, gj, gi] = target[:, 7]\n",
    "    tx3[b, best_n, gj, gi] = target[:, 8]\n",
    "    tx4[b, best_n, gj, gi] = target[:, 9]\n",
    "    \n",
    "    ty1[b, best_n, gj, gi] = target[:, 10]\n",
    "    ty2[b, best_n, gj, gi] = target[:, 11]\n",
    "    ty3[b, best_n, gj, gi] = target[:, 12]\n",
    "    ty4[b, best_n, gj, gi] = target[:, 13]\n",
    "    \n",
    "    # Width and height\n",
    "    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n",
    "    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1] + 1e-16)\n",
    "    # One-hot encoding of label\n",
    "    tcls[b, best_n, gj, gi, target_labels] = 1\n",
    "    # Compute label correctness and iou at best anchor\n",
    "    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n",
    "    # iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=False)\n",
    "\n",
    "    tconf = obj_mask.float()\n",
    "    return (class_mask, obj_mask, noobj_mask, \n",
    "            tx, ty, tw, th, \n",
    "            tx1, tx2, tx3, tx4, \n",
    "            ty1, ty2, ty3, ty4, \n",
    "            tcls, tconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoadMapLoss(pred_rm, target_rm):\n",
    "    bce_loss = nn.BCELoss()\n",
    "\n",
    "    return bce_loss(pred_rm, target_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_joint_loss(yolo_loss, rm_loss, lambd):\n",
    "    return yolo_loss + lambd * rm_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop and test loops\n",
    "Not necessarily using data loader.\n",
    "\n",
    "Assuming targets are already pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo(data_loader, kobe_model, kobe_optimizer, n_epochs, lambd = 0.5):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        kobe_model.train()\n",
    "        start_time = time.time()\n",
    "        for i in range(math.ceil(len(df)/batch_size)):\n",
    "            batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "            batch_images = data[batch_ind, :]\n",
    "            batch_targets_bb = targets_bb[batch_ind, :]\n",
    "            batch_rms = targets_rm[batch_ind, :]\n",
    "            \n",
    "            kobe_optimizer.zero_grad()\n",
    "\n",
    "            imgs = Variable(batch_images.to(device))\n",
    "            targets = Variable(batch_targets.to(device), requires_grad=False)\n",
    "\n",
    "            outputs_yolo, outputs_rm = kobe_model(imgs)\n",
    "            \n",
    "            #yolo_loss = YoloLoss(outputs_yolo[0], outputs_yolo[1], outputs_yolo[2], targets, \n",
    "            #                     kobe_model.yolo_decoder.scaled_anchors, kobe_model.yolo_decoder.ignore_thres) \n",
    "            \n",
    "            #rm_loss = RoadMapLoss(outputs_rm, batch_rms)\n",
    "            \n",
    "            #loss = total_joint_loss(yolo_loss, rm_loss, lambd)\n",
    "            loss.backward()\n",
    "            \n",
    "            kobe_optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-defined anchors. Should honestly come from KMeans on detection boxes but let's see how this does before going complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the code uses only the last 3 anchors so let ssee what this does\n",
    "# width, height\n",
    "\n",
    "#### anchors are supposed to be in terms of number of grid points it would take\n",
    "#### in a 416x416 image (assuming using default of YOLO)\n",
    "#### we are given 80x80\n",
    "### we match in the 416x416 space though\n",
    "### so scale what esteban gave by 5 (5.2 actualy but wtv)\n",
    "anchors = [(5,5), (25, 12), (12, 25), (100, 25), (50, 12), (40, 60)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our YoloLayer for task of object localization\n",
    "\n",
    "Ignoring orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_HIDDEN = 26718\n",
    "class PreTaskEncoder(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(PreTaskEncoder, self).__init__()\n",
    "        # number of different kernels to use\n",
    "        self.n_features = n_features\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=n_features,\n",
    "                               kernel_size=5,\n",
    "                               )\n",
    "        self.conv2 = nn.Conv2d(n_features,\n",
    "                               n_features,\n",
    "                               kernel_size=5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # return an array shape\n",
    "        x = x.view(-1, ENCODER_HIDDEN)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, anchors, num_classes, img_dim=416):\n",
    "        \n",
    "        super(YoloDecoder, self).__init__()\n",
    "        \n",
    "        self.anchors = anchors\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_thres = 0.5\n",
    "        \n",
    "        self.obj_scale = 1\n",
    "        self.noobj_scale = 100\n",
    "        self.img_dim = img_dim\n",
    "        self.grid_size = 16\n",
    "        \n",
    "        # takes in dense output from encoder or shared decoder and puts into an\n",
    "        # image of dim img_dim\n",
    "        self.m = nn.Sequential(\n",
    "            nn.Linear(6 * ENCODER_HIDDEN, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, self.num_anchors * (self.num_classes + 5 + 8) * self.grid_size * self.grid_size)\n",
    "        )\n",
    "        \n",
    "    def compute_grid_offsets(self, grid_size, cuda=True):\n",
    "        self.grid_size = grid_size\n",
    "        g = self.grid_size\n",
    "        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "        self.stride = self.img_dim / self.grid_size\n",
    "        # Calculate offsets for each grid\n",
    "        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n",
    "        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n",
    "        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])\n",
    "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
    "        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
    "        \n",
    "    def forward(self, x, target = None):\n",
    "        # Tensors for cuda support\n",
    "        # Tensors for cuda support\n",
    "        x = self.m(x)\n",
    "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
    "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
    "        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n",
    "\n",
    "        self.img_dim = img_dim\n",
    "        num_samples = x.size(0)\n",
    "        grid_size = x.size(2)\n",
    "\n",
    "        prediction = (\n",
    "            x.view(num_samples, self.num_anchors, self.num_classes + 5 + 8, grid_size, grid_size)\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "            .contiguous()\n",
    "        )\n",
    "\n",
    "        # Get outputs\n",
    "        xc = torch.sigmoid(prediction[..., 0])  # Center x\n",
    "        yc = torch.sigmoid(prediction[..., 1])  # Center y\n",
    "        w = prediction[..., 2]  # Width\n",
    "        h = prediction[..., 3]  # Height\n",
    "        \n",
    "        #### get x1, x2, x3, x4, y1, y2, y3, y4\n",
    "        \n",
    "        x1 = torch.tanh(prediction[..., 4])\n",
    "        x2 = torch.tanh(prediction[..., 5])\n",
    "        x3 = torch.tanh(prediction[..., 6])\n",
    "        x4 = torch.tanh(prediction[..., 7])\n",
    "        y1 = torch.tanh(prediction[..., 8])\n",
    "        y2 = torch.tanh(prediction[..., 9])\n",
    "        y3 = torch.tanh(prediction[..., 10])\n",
    "        y4 = torch.tanh(prediction[..., 11])\n",
    "        \n",
    "        pred_conf = torch.sigmoid(prediction[..., 12])  # Conf\n",
    "        pred_cls = torch.sigmoid(prediction[..., 13:])  # Cls pred.\n",
    "\n",
    "        # If grid size does not match current we compute new offsets\n",
    "        if grid_size != self.grid_size:\n",
    "            self.compute_grid_offsets(grid_size, cuda=xc.is_cuda)\n",
    "\n",
    "        # Add offset and scale with anchors\n",
    "        # mulitply by stride to convert from grid to image coordinates (of YOLO img of 416)\n",
    "        pred_boxes = FloatTensor(prediction[..., :12].shape)\n",
    "        pred_boxes[..., 0] = (xc.data + self.grid_x) * self.stride\n",
    "        pred_boxes[..., 1] = (yc.data + self.grid_y) * self.stride\n",
    "        \n",
    "        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w * self.stride\n",
    "        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h * self.stride\n",
    "        \n",
    "        pred_boxes[..., 4] = x1\n",
    "        pred_boxes[..., 5] = x2\n",
    "        pred_boxes[..., 6] = x3\n",
    "        pred_boxes[..., 7] = x4\n",
    "        \n",
    "        pred_boxes[..., 8] = y1\n",
    "        pred_boxes[..., 9] = y2\n",
    "        pred_boxes[..., 10] = y3\n",
    "        pred_boxes[..., 11] = y4\n",
    "        ### need to figure out what to do with all the x1, x2, x3, x4 etc\n",
    "\n",
    "        # ORIGINAL OUTPUTS IN TERMS OF GRID SIZES, DO NOT FORGET TO CONVERT BACK\n",
    "        output = torch.cat(\n",
    "            (\n",
    "                pred_boxes.view(num_samples, -1, 12),\n",
    "                pred_conf.view(num_samples, -1, 1),\n",
    "                pred_cls.view(num_samples, -1, self.num_classes),\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "        if targets is None:\n",
    "            return output, 0\n",
    "        else:\n",
    "            mse_loss = nn.MSELoss()\n",
    "            bce_loss = nn.BCELoss()\n",
    "    \n",
    "            class_mask, obj_mask, noobj_mask, \n",
    "            tx, ty, tw, th, \n",
    "            tx1, tx2, tx3, tx4, \n",
    "            ty1, ty2, ty3, ty4, \n",
    "            tcls, tconf = build_targets(\n",
    "                        pred_boxes=pred_boxes,\n",
    "                        pred_cls=pred_cls,\n",
    "                        target=targets,\n",
    "                        anchors=scaled_anchors,\n",
    "                        ignore_thres=ignore_thres,\n",
    "                    )\n",
    "\n",
    "            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n",
    "            loss_xc = self.mse_loss(xc[obj_mask], tx[obj_mask])\n",
    "            loss_yc = self.mse_loss(yc[obj_mask], ty[obj_mask])\n",
    "\n",
    "            loss_x1 = self.mse_loss(x1[obj_mask], tx1[obj_mask])\n",
    "            loss_x2 = self.mse_loss(x2[obj_mask], tx2[obj_mask])\n",
    "            loss_x3 = self.mse_loss(x3[obj_mask], tx3[obj_mask])\n",
    "            loss_x4 = self.mse_loss(x4[obj_mask], tx4[obj_mask])\n",
    "            \n",
    "            loss_y1 = self.mse_loss(y1[obj_mask], ty1[obj_mask])\n",
    "            loss_y2 = self.mse_loss(y2[obj_mask], ty2[obj_mask])\n",
    "            loss_y3 = self.mse_loss(y3[obj_mask], ty3[obj_mask])\n",
    "            loss_y4 = self.mse_loss(y4[obj_mask], ty4[obj_mask])\n",
    "            \n",
    "            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n",
    "            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n",
    "\n",
    "            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n",
    "            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n",
    "            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n",
    "            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n",
    "            \n",
    "            total_loss = loss_xc + loss_yc + \\\n",
    "                         loss_x1 + loss_x2 + loss_x3 + loss_x4 + \\\n",
    "                         loss_y1 + loss_y2 + loss_y3 + loss_y4 + \\\n",
    "                         loss_w + loss_h + loss_conf + loss_cls\n",
    "\n",
    "            return output, total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Model does that does both tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KobeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, encoder_features, yolo_dim, rm_dim):\n",
    "        super(KobeModel, self).__init__()\n",
    "        \n",
    "        self.yolo_dim = yolo_dim\n",
    "        \n",
    "        self.encoder = PreTaskEncoder(encoder_features)\n",
    "        \n",
    "        \n",
    "        #self.shared_decoder = nn.Sequential()\n",
    "        \n",
    "        self.yolo_decoder = YoloDecoder(anchors, num_classes, img_dim=yolo_dim)\n",
    "        \n",
    "        #self.rm_decoder = RmDecoder(rm_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        print(\"BEFORE ENCODING SHAPE\")\n",
    "        print(x.shape)\n",
    "        x = self.encoder(x)\n",
    "        print(\"POST ENCODING SHAPE\")\n",
    "        print(x.shape)\n",
    "        \n",
    "        # get all the representations laid out like this\n",
    "        x = torch.cat([x[:, i, :] for i in range(6)], dim = 1)\n",
    "            \n",
    "            \n",
    "        #convert from dense representation from encoder into an image\n",
    "        # x.view(...)\n",
    "        \n",
    "        #x = self.shared_decoder(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, yolo_target = None, rm_target = None ):\n",
    "        encoding = self.encode(x)\n",
    "        \n",
    "        # output_1 first 12 corresponds to the bounding boxes\n",
    "        # output_1 13 corresponds to confidences\n",
    "        # output_1's and the rest corresponds to the class labels\n",
    "        # oputput_1[3] is them spread out\n",
    "        output_1, yolo_loss = self.get_bounding_boxes(x, encoding, target = yolo_target)\n",
    "        # roadmap decoder\n",
    "        #output_2, rm_loss = self.rm_decoder(x, encoding, target = rm_target)\n",
    "        \n",
    "        # output1 is not in the context of our bounding boxes\n",
    "        #return output_1, output_2, yolo_loss, rm_loss\n",
    "        return output_1, yolo_loss\n",
    "    \n",
    "    # for easy use for competition\n",
    "    # in competition, encoding is None\n",
    "    def get_bounding_boxes(self, x, encoding = None, target = None):\n",
    "        if encoding is None:\n",
    "            encoding = self.encode(x)\n",
    "        \n",
    "        output, yolo_loss = self.yolo_decoder(encoding, target)\n",
    "        \n",
    "        output = non_max_suppression(output)\n",
    "        outputs = output\n",
    "        \n",
    "        boxes = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            # let's convert it back to center_x, center_y, width and height\n",
    "            better_coordinates = FloatTensor(outputs.shape[0], 2, 4)\n",
    "            translation = FloatTensor(bbox.shape[0], 2, 4)\n",
    "            translation[:, 0, :].fill_(-40)\n",
    "            translation[:, 1, :].fill_(40)\n",
    "\n",
    "            translation\n",
    "            center_x = (output[:, 0] + output[:, 2]) / 2 / 416 * 80\n",
    "            center_y = (output[:, 1] + output[:, 3]) / 2 / 416 * 80\n",
    "            width = output[:, 2] - output[:,0] / 416 * 80\n",
    "            height = output[:, 3] - output[:,1] / 416 * 80\n",
    "            \n",
    "            x1 = center_x + output[:, 4] * width/2\n",
    "            x2 = center_x + output[:, 5] * width/2\n",
    "            x3 = center_x + output[:, 6] * width/2\n",
    "            x4 = center_x + output[:, 7] * width/2\n",
    "            y1 = center_y + output[:, 8] * height/2\n",
    "            y2 = center_y + output[:, 9] * height/2\n",
    "            y3 = center_y + output[:, 10] * height/2\n",
    "            y4 = center_y + output[:, 11] * height/2  \n",
    "            \n",
    "            better_coordinates[:, 0, 0] = x1\n",
    "            better_coordinates[:, 0, 1] = x2\n",
    "            better_coordinates[:, 0, 2] = x3\n",
    "            better_coordinates[:, 0, 3] = x4\n",
    "            \n",
    "            better_coordinates[:, 1, 0] = y1\n",
    "            better_coordinates[:, 1, 1] = y2\n",
    "            better_coordinates[:, 1, 2] = y3\n",
    "            better_coordinates[:, 1, 3] = y4\n",
    "            \n",
    "            better_coordinates[:, 1, :].mul_(-1)\n",
    "            # shift back!\n",
    "            better_coordinates += translation\n",
    "            \n",
    "            boxes.append(output)\n",
    "        return tuple(boxes)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kobe_model = KobeModel(10, 6, 416, 800)\n",
    "\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "\n",
    "kobe_optimizer = torch.optim.Adam(kobe_model.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, n_epochs + 1):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'data'\n",
    "annotation_csv = 'data/annotation.csv'\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight 6 3 5 5, but got 5-dimensional input of size [2, 6, 3, 256, 306] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-bb8630df0b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mroad_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroad_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mkobe_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-f56bb3019091>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# get all the representations laid out like this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f909cb3d7861>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight 6 3 5 5, but got 5-dimensional input of size [2, 6, 3, 256, 306] instead"
     ]
    }
   ],
   "source": [
    "for sample, target, road_image, extra in trainloader:\n",
    "    sample = torch.stack(sample)\n",
    "    target = transform_target(target)\n",
    "    road_image = torch.stack(road_image)\n",
    "    \n",
    "    kobe_model.encode(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
