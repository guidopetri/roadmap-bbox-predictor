{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to keep the code here. And then export into a .py file.\n",
    "\n",
    "Do not try to modify the .py file directly until this is notebook is gone from each branch.\n",
    "\n",
    "Remember to clear output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils and calculating loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming Coordinates\n",
    "\n",
    "Define the given coordinates as world coordinates\n",
    "\n",
    "Define normalized from upper left bound of world coordinates (translate to there, rotate, and normalize) as our normalized image coordinates (or image coordinates for short).\n",
    "\n",
    "Always facing right in world coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 40\n",
    "WIDTH = 2 * 40\n",
    "HEIGHT = 2 * 40\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "S = 7\n",
    "B = 2\n",
    "l_coord = 5\n",
    "l_noobj = 0.5\n",
    "\n",
    "#cuda = torch.cuda.is_available()\n",
    "cuda = False\n",
    "\n",
    "device = 'cuda:0' if cuda else 'cpu'\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "IntTensor = torch.cuda.IntTensor if cuda else torch.IntTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "BoolTensor = torch.cuda.BoolTensor if cuda else torch.BoolTensor\n",
    "\n",
    "def target_encode(boxes, labels):\n",
    "        \"\"\" Encode box coordinates and class labels as one target tensor.\n",
    "        Args:\n",
    "            boxes: (tensor) [[x1, y1, x2, y2]_obj1, ...], normalized from 0.0 to 1.0 w.r.t. image width/height.\n",
    "            labels: (tensor) [c_obj1, c_obj2, ...]\n",
    "        Returns:\n",
    "            An encoded tensor sized [S, S, 5 x B + C], 5=(x, y, w, h, conf)\n",
    "        \"\"\"\n",
    "\n",
    "        C = NUM_CLASSES\n",
    "        N = 5 * B + C\n",
    "\n",
    "        target = torch.zeros(S, S, N)\n",
    "        cell_size = 1.0 / float(S)\n",
    "        boxes_wh = boxes[:, 2:] - boxes[:, :2] # width and height for each box, [n, 2]\n",
    "        boxes_xy = (boxes[:, 2:] + boxes[:, :2]) / 2.0 # center x & y for each box, [n, 2]\n",
    "        for b in range(boxes.size(0)):\n",
    "            xy, wh, label = boxes_xy[b], boxes_wh[b], int(labels[b])\n",
    "\n",
    "            ij = (xy / cell_size).ceil() - 1.0\n",
    "            i, j = int(ij[0]), int(ij[1]) # y & x index which represents its location on the grid.\n",
    "            x0y0 = ij * cell_size # x & y of the cell left-top corner.\n",
    "            xy_normalized = (xy - x0y0) / cell_size # x & y of the box on the cell, normalized from 0.0 to 1.0.\n",
    "\n",
    "            # TBM, remove redundant dimensions from target tensor.\n",
    "            # To remove these, loss implementation also has to be modified.\n",
    "            for k in range(B):\n",
    "                s = 5 * k\n",
    "                target[j, i, s  :s+2] = xy_normalized\n",
    "                target[j, i, s+2:s+4] = wh\n",
    "                target[j, i, s+4    ] = 1.0\n",
    "            target[j, i, 5*B + label] = 1.0\n",
    "\n",
    "        return target\n",
    "    \n",
    "def pred_decode(pred_tensor, conf_thresh=0.1, prob_thresh=0.1):\n",
    "        \"\"\" Decode tensor into box coordinates, class labels, and probs_detected.\n",
    "        Args:\n",
    "            pred_tensor: (tensor) tensor to decode sized [S, S, 5 x B + C], 5=(x, y, w, h, conf)\n",
    "        Returns:\n",
    "            boxes: (tensor) [[x1, y1, x2, y2]_obj1, ...]. Normalized from 0.0 to 1.0 w.r.t. image width/height, sized [n_boxes, 4].\n",
    "            labels: (tensor) class labels for each detected boxe, sized [n_boxes,].\n",
    "            confidences: (tensor) objectness confidences for each detected box, sized [n_boxes,].\n",
    "            class_scores: (tensor) scores for most likely class for each detected box, sized [n_boxes,].\n",
    "        \"\"\"\n",
    "        C = NUM_CLASSES\n",
    "        boxes, labels, confidences, class_scores = [], [], [], []\n",
    "\n",
    "        cell_size = 1.0 / float(S)\n",
    "\n",
    "        conf = pred_tensor[:, :, 4].unsqueeze(2) # [S, S, 1]\n",
    "        for b in range(1, B):\n",
    "            conf = torch.cat((conf, pred_tensor[:, :, 5*b + 4].unsqueeze(2)), 2)\n",
    "        conf_mask = conf > conf_thresh # [S, S, B]\n",
    "\n",
    "        # TBM, further optimization may be possible by replacing the following for-loops with tensor operations.\n",
    "        for i in range(S): # for x-dimension.\n",
    "            for j in range(S): # for y-dimension.\n",
    "                class_score, class_label = torch.max(pred_tensor[j, i, 5*B:], 0)\n",
    "\n",
    "                for b in range(B):\n",
    "                    conf = pred_tensor[j, i, 5*b + 4]\n",
    "                    prob = conf * class_score\n",
    "                    if float(prob) < prob_thresh:\n",
    "                        continue\n",
    "\n",
    "                    # Compute box corner (x1, y1, x2, y2) from tensor.\n",
    "                    box = pred_tensor[j, i, 5*b : 5*b + 4]\n",
    "                    x0y0_normalized = FloatTensor([i, j]) * cell_size # cell left-top corner. Normalized from 0.0 to 1.0 w.r.t. image width/height.\n",
    "                    xy_normalized = box[:2] * cell_size + x0y0_normalized   # box center. Normalized from 0.0 to 1.0 w.r.t. image width/height.\n",
    "                    wh_normalized = box[2:] # Box width and height. Normalized from 0.0 to 1.0 w.r.t. image width/height.\n",
    "                    box_xyxy = FloatTensor(4) # [4,]\n",
    "                    box_xyxy[:2] = xy_normalized - 0.5 * wh_normalized # left-top corner (x1, y1).\n",
    "                    box_xyxy[2:] = xy_normalized + 0.5 * wh_normalized # right-bottom corner (x2, y2).\n",
    "\n",
    "                    # Append result to the lists.\n",
    "                    boxes.append(box_xyxy)\n",
    "                    labels.append(class_label)\n",
    "                    confidences.append(conf)\n",
    "                    class_scores.append(class_score)\n",
    "\n",
    "        if len(boxes) > 0:\n",
    "            boxes = torch.stack(boxes, 0) # [n_boxes, 4]\n",
    "            labels = torch.stack(labels, 0)             # [n_boxes, ]\n",
    "            confidences = torch.stack(confidences, 0)   # [n_boxes, ]\n",
    "            class_scores = torch.stack(class_scores, 0) # [n_boxes, ]\n",
    "        else:\n",
    "            # If no box found, return empty tensors.\n",
    "            boxes = FloatTensor(0, 4)\n",
    "            labels = LongTensor(0)\n",
    "            confidences = FloatTensor(0)\n",
    "            class_scores = FloatTensor(0)\n",
    "\n",
    "        return boxes, labels, confidences, class_scores\n",
    "\n",
    "def nms(boxes, scores, nms_thresh = 0.35):\n",
    "    \"\"\" Apply non maximum supression.\n",
    "    Args:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    threshold = nms_thresh\n",
    "\n",
    "\n",
    "    x1 = boxes[:, 0] # [n,]\n",
    "    y1 = boxes[:, 1] # [n,]\n",
    "    x2 = boxes[:, 2] # [n,]\n",
    "    y2 = boxes[:, 3] # [n,]\n",
    "    areas = (x2 - x1) * (y2 - y1) # [n,]\n",
    "\n",
    "    _, ids_sorted = scores.sort(0, descending=True) # [n,]\n",
    "    ids = []\n",
    "    while ids_sorted.numel() > 0:\n",
    "        # Assume `ids_sorted` size is [m,] in the beginning of this iter.\n",
    "\n",
    "        i = ids_sorted.item() if (ids_sorted.numel() == 1) else ids_sorted[0]\n",
    "        ids.append(i)\n",
    "\n",
    "        if ids_sorted.numel() == 1:\n",
    "            break # If only one box is left (i.e., no box to supress), break.\n",
    "\n",
    "        inter_x1 = x1[ids_sorted[1:]].clamp(min=x1[i].item()) # [m-1, ]\n",
    "        inter_y1 = y1[ids_sorted[1:]].clamp(min=y1[i].item()) # [m-1, ]\n",
    "        inter_x2 = x2[ids_sorted[1:]].clamp(max=x2[i].item()) # [m-1, ]\n",
    "        inter_y2 = y2[ids_sorted[1:]].clamp(max=y2[i].item()) # [m-1, ]\n",
    "        inter_w = (inter_x2 - inter_x1).clamp(min=0) # [m-1, ]\n",
    "        inter_h = (inter_y2 - inter_y1).clamp(min=0) # [m-1, ]\n",
    "\n",
    "        inters = inter_w * inter_h # intersections b/w/ box `i` and other boxes, sized [m-1, ].\n",
    "        unions = areas[i] + areas[ids_sorted[1:]] - inters # unions b/w/ box `i` and other boxes, sized [m-1, ].\n",
    "        ious = inters / unions # [m-1, ]\n",
    "\n",
    "        # Remove boxes whose IoU is higher than the threshold.\n",
    "        ids_keep = (ious <= threshold).nonzero().squeeze() # [m-1, ]. Because `nonzero()` adds extra dimension, squeeze it.\n",
    "        if ids_keep.numel() == 0:\n",
    "            break # If no box left, break.\n",
    "        ids_sorted = ids_sorted[ids_keep+1] # `+1` is needed because `ids_sorted[0] = i`.\n",
    "\n",
    "    return LongTensor(ids)\n",
    "    \n",
    "## input is the \n",
    "# want output that is\n",
    "def transform_target(in_target):\n",
    "    \n",
    "    out_target = []\n",
    "    \n",
    "    for tgt_index in range(len(in_target)):\n",
    "        \n",
    "        #how many boxes for these target\n",
    "        nbox = in_target[tgt_index]['bounding_box'].shape[0]\n",
    "        \n",
    "        # CONVERT ALL THE BOUNDING BOXES for an individual sample at once\n",
    "        \n",
    "        bbox = in_target[tgt_index]['bounding_box'].to(device)\n",
    "        translation = FloatTensor(bbox.shape[0], bbox.shape[1], bbox.shape[2])\n",
    "        translation[:, 0, :].fill_(-40)\n",
    "        translation[:, 1, :].fill_(40)\n",
    "\n",
    "        # translate to uppert left\n",
    "        box = bbox - translation\n",
    "        # reflect y\n",
    "        box[:, 1, :].mul_(-1)\n",
    "\n",
    "        x_min = box[:, 0].min(dim = 1)[0]\n",
    "        y_min = box[:, 1].min(dim = 1)[0]\n",
    "        x_max = box[:, 0].max(dim = 1)[0]\n",
    "        y_max = box[:, 1].max(dim = 1)[0]\n",
    "\n",
    "\n",
    "        x_min = x_min / WIDTH\n",
    "        y_min = y_min / HEIGHT\n",
    "        x_max = x_max / WIDTH\n",
    "        y_max = y_max / HEIGHT\n",
    "        \n",
    "\n",
    "        boxes = torch.stack([x_min, y_min, x_max, y_max], 1)\n",
    "\n",
    "        labels = IntTensor(nbox)\n",
    "        for box_index in range(nbox):\n",
    "            category = in_target[tgt_index]['category'][box_index]\n",
    "            \n",
    "            # from which sample in the batch\n",
    "            labels[box_index] = category\n",
    "            \n",
    "        individual_target = target_encode(boxes, labels)\n",
    "        out_target.append(individual_target)\n",
    "        \n",
    "    return torch.stack(out_target, dim = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load presaved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works by side effects\n",
    "def load_pretask_weight_from_model(model, presaved_encoder):\n",
    "    model.encoder.load_state_dict(presaved_encoder.state_dict())\n",
    "    \n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you want Initialize Our Model with encoder weights from an existing pretask encoder in memory\n",
    "def initialize_model_for_training(presaved_encoder):\n",
    "    model = KobeModel(num_classes = 10, encoder_features = 6, rm_dim = 800)\n",
    "    load_pretask_weight_from_model(model, presaved_encoder)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you want Initialize Our Model with encoder weights from a file\n",
    "def initialize_model_for_training_file(presaved_encoder_file):\n",
    "    presaved_encoder = PreTaskEncoder()\n",
    "    presaved_encoder.load_state_dict(torch.load(presaved_encoder_file))\n",
    "    presaved_encoder.eval()\n",
    "\n",
    "    \n",
    "    return initialize_model_for_training(presaved_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting predictions to the format for competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to calculate bounding boxes and such"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoadMapLoss(pred_rm, target_rm):\n",
    "    bce_loss = nn.BCELoss()\n",
    "\n",
    "    return bce_loss(pred_rm, target_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_joint_loss(yolo_loss, rm_loss, lambd):\n",
    "    return yolo_loss + lambd * rm_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop and test loops\n",
    "Not necessarily using data loader.\n",
    "\n",
    "Assuming targets are already pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo(data_loader, kobe_model, kobe_optimizer, lambd = 20):\n",
    "    kobe_model.train()\n",
    "    train_loss = 0 \n",
    "        \n",
    "    for i, data in enumerate(data_loader):\n",
    "        sample, target, road_image = data\n",
    "        print('Original')\n",
    "        print(target)\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        target_original = target\n",
    "        target = transform_target(target_original).to(device)\n",
    "        road_image = torch.stack(road_image).float().to(device)\n",
    "\n",
    "        kobe_optimizer.zero_grad()\n",
    "\n",
    "        output_yolo, yolo_loss, output_rm, rm_loss = kobe_model(sample, yolo_targets = target, \n",
    "                                                                rm_targets = road_image)\n",
    "\n",
    "        # SHOULD GET LOWER OVER EPOCHS\n",
    "        #print(\"PRINTING output yolo after nms\")\n",
    "        #if output_yolo[0] is not None:\n",
    "        #    print(output_yolo[0].shape)\n",
    "        #    print(output_yolo[0])\n",
    "        #else:\n",
    "        #    print(\"It was none!\")\n",
    "        #    print(output_yolo)\n",
    "        #rm_loss = RoadMapLoss(outputs_rm, batch_rms)\n",
    "\n",
    "        # loss = total_joint_loss(yolo_loss, rm_loss, lambd)\n",
    "        # print(\"yolo loss\")\n",
    "        # print(yolo_loss)\n",
    "        # print('rm loss')\n",
    "        # print(lambd * rm_loss)\n",
    "        \n",
    "        predicted_bounding_boxes = output_yolo[0].cpu()\n",
    "        ats_bounding_boxes = compute_ats_bounding_boxes(predicted_bounding_boxes, target_original[0]['bounding_box'])\n",
    "        print('ats bounding box')\n",
    "        print(ats_bounding_boxes)\n",
    "        \n",
    "        ts_road_map = compute_ts_road_map(output_rm, road_image)\n",
    "        print('ts roadmap')\n",
    "        print(ts_road_map)\n",
    "        \n",
    "        \n",
    "        total_loss = total_joint_loss(yolo_loss, rm_loss, lambd)\n",
    "        train_loss += (total_loss.item())\n",
    "        total_loss.backward()\n",
    "\n",
    "        kobe_optimizer.step()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print(\"TRAIN LOSS: {}\".format(train_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-defined anchors. Should honestly come from KMeans on detection boxes but let's see how this does before going complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our YoloLayer for task of object localization\n",
    "\n",
    "Ignoring orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_HIDDEN = int(26718 / 2)\n",
    "class PreTaskEncoder(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(PreTaskEncoder, self).__init__()\n",
    "        # number of different kernels to use\n",
    "        self.n_features = n_features\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=n_features,\n",
    "                               kernel_size=5,\n",
    "                               )\n",
    "        self.conv2 = nn.Conv2d(n_features,\n",
    "                               int(n_features/2),\n",
    "                               kernel_size=5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # return an array shape\n",
    "        x = x.view(-1, ENCODER_HIDDEN)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer2d(nn.Module):\n",
    "    def __init__(self, channels, dim):\n",
    "        super(ReshapeLayer2d, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], self.channels, self.dim, self.dim)\n",
    "    \n",
    "class ReshapeLayer1d(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ReshapeLayer1d, self).__init__()\n",
    "        self.features = features\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], self.features)\n",
    "\n",
    "class YoloDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        \n",
    "        super(YoloDecoder, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # takes in dense output from encoder or shared decoder and puts into an\n",
    "        # image of dim img_dim\n",
    "\n",
    "        self.m = nn.Sequential(\n",
    "            nn.Linear(6 * ENCODER_HIDDEN, 2 * 15 * 15),\n",
    "            nn.ReLU(),\n",
    "            ReshapeLayer2d(2, 15),\n",
    "            nn.Conv2d(2, 2, kernel_size=3, stride = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride = 1),\n",
    "            ReshapeLayer1d(288),\n",
    "            nn.Linear(288, S * S * (5 * B + self.num_classes)),\n",
    "            # Sigmoid is final layer in Yolo v1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.m(x)\n",
    "\n",
    "\n",
    "        num_samples = x.shape[0]\n",
    "\n",
    "        prediction = (\n",
    "            x.view(num_samples, S, S, 5 * B + self.num_classes)\n",
    "            .contiguous()\n",
    "        )\n",
    "                \n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/motokimura/yolo_v1_pytorch/\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, feature_size=S, num_bboxes=B, num_classes=NUM_CLASSES, \n",
    "                 lambda_coord=l_coord, lambda_noobj=l_noobj):\n",
    "        \"\"\" Constructor.\n",
    "        Args:\n",
    "            feature_size: (int) size of input feature map.\n",
    "            num_bboxes: (int) number of bboxes per each cell.\n",
    "            num_classes: (int) number of the object classes.\n",
    "            lambda_coord: (float) weight for bbox location/size losses.\n",
    "            lambda_noobj: (float) weight for no-objectness loss.\n",
    "        \"\"\"\n",
    "        super(YoloLoss, self).__init__()\n",
    "\n",
    "        self.S = feature_size\n",
    "        self.B = num_bboxes\n",
    "        self.C = num_classes\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "\n",
    "\n",
    "    def compute_iou(self, bbox1, bbox2):\n",
    "        \"\"\" Compute the IoU (Intersection over Union) of two set of bboxes, each bbox format: [x1, y1, x2, y2].\n",
    "        Args:\n",
    "            bbox1: (Tensor) bounding bboxes, sized [N, 4].\n",
    "            bbox2: (Tensor) bounding bboxes, sized [M, 4].\n",
    "        Returns:\n",
    "            (Tensor) IoU, sized [N, M].\n",
    "        \"\"\"\n",
    "        N = bbox1.size(0)\n",
    "        M = bbox2.size(0)\n",
    "\n",
    "        # Compute left-top coordinate of the intersections\n",
    "        lt = torch.max(\n",
    "            bbox1[:, :2].unsqueeze(1).expand(N, M, 2), # [N, 2] -> [N, 1, 2] -> [N, M, 2]\n",
    "            bbox2[:, :2].unsqueeze(0).expand(N, M, 2)  # [M, 2] -> [1, M, 2] -> [N, M, 2]\n",
    "        )\n",
    "        # Conpute right-bottom coordinate of the intersections\n",
    "        rb = torch.min(\n",
    "            bbox1[:, 2:].unsqueeze(1).expand(N, M, 2), # [N, 2] -> [N, 1, 2] -> [N, M, 2]\n",
    "            bbox2[:, 2:].unsqueeze(0).expand(N, M, 2)  # [M, 2] -> [1, M, 2] -> [N, M, 2]\n",
    "        )\n",
    "        # Compute area of the intersections from the coordinates\n",
    "        wh = rb - lt   # width and height of the intersection, [N, M, 2]\n",
    "        wh[wh < 0] = 0 # clip at 0\n",
    "        inter = wh[:, :, 0] * wh[:, :, 1] # [N, M]\n",
    "\n",
    "        # Compute area of the bboxes\n",
    "        area1 = (bbox1[:, 2] - bbox1[:, 0]) * (bbox1[:, 3] - bbox1[:, 1]) # [N, ]\n",
    "        area2 = (bbox2[:, 2] - bbox2[:, 0]) * (bbox2[:, 3] - bbox2[:, 1]) # [M, ]\n",
    "        area1 = area1.unsqueeze(1).expand_as(inter) # [N, ] -> [N, 1] -> [N, M]\n",
    "        area2 = area2.unsqueeze(0).expand_as(inter) # [M, ] -> [1, M] -> [N, M]\n",
    "\n",
    "        # Compute IoU from the areas\n",
    "        union = area1 + area2 - inter # [N, M, 2]\n",
    "        iou = inter / union           # [N, M, 2]\n",
    "\n",
    "        return iou\n",
    "\n",
    "    def forward(self, pred_tensor, target_tensor):\n",
    "        \"\"\" Compute loss for YOLO training.\n",
    "        Args:\n",
    "            pred_tensor: (Tensor) predictions, sized [n_batch, S, S, Bx5+C], 5=len([x, y, w, h, conf]).\n",
    "            target_tensor: (Tensor) targets, sized [n_batch, S, S, Bx5+C].\n",
    "        Returns:\n",
    "            (Tensor): loss, sized [1, ].\n",
    "        \"\"\"\n",
    "        # TODO: Romove redundant dimensions for some Tensors.\n",
    "\n",
    "        S, B, C = self.S, self.B, self.C\n",
    "        \n",
    "        N = 5 * B + C\n",
    "\n",
    "        batch_size = pred_tensor.size(0)\n",
    "        coord_mask = target_tensor[:, :, :, 4] > 0  # mask for the cells which contain objects. [n_batch, S, S]\n",
    "        noobj_mask = target_tensor[:, :, :, 4] == 0 # mask for the cells which do not contain objects. [n_batch, S, S]\n",
    "        coord_mask = coord_mask.unsqueeze(-1).expand_as(target_tensor) # [n_batch, S, S] -> [n_batch, S, S, N]\n",
    "        noobj_mask = noobj_mask.unsqueeze(-1).expand_as(target_tensor) # [n_batch, S, S] -> [n_batch, S, S, N]\n",
    "\n",
    "        coord_pred = pred_tensor[coord_mask].view(-1, N)            # pred tensor on the cells which contain objects. [n_coord, N]\n",
    "                                                                    # n_coord: number of the cells which contain objects.\n",
    "        bbox_pred = coord_pred[:, :5*B].contiguous().view(-1, 5)    # [n_coord x B, 5=len([x, y, w, h, conf])]\n",
    "        class_pred = coord_pred[:, 5*B:]                            # [n_coord, C]\n",
    "\n",
    "        coord_target = target_tensor[coord_mask].view(-1, N)        # target tensor on the cells which contain objects. [n_coord, N]\n",
    "                                                                    # n_coord: number of the cells which contain objects.\n",
    "        bbox_target = coord_target[:, :5*B].contiguous().view(-1, 5)# [n_coord x B, 5=len([x, y, w, h, conf])]\n",
    "        class_target = coord_target[:, 5*B:]                        # [n_coord, C]\n",
    "\n",
    "        # Compute loss for the cells with no object bbox.\n",
    "        noobj_pred = pred_tensor[noobj_mask].view(-1, N)        # pred tensor on the cells which do not contain objects. [n_noobj, N]\n",
    "                                                                # n_noobj: number of the cells which do not contain objects.\n",
    "        noobj_target = target_tensor[noobj_mask].view(-1, N)    # target tensor on the cells which do not contain objects. [n_noobj, N]\n",
    "                                                                # n_noobj: number of the cells which do not contain objects.\n",
    "        noobj_conf_mask = BoolTensor(noobj_pred.size()).fill_(0) # [n_noobj, N]\n",
    "        for b in range(B):\n",
    "            noobj_conf_mask[:, 4 + b*5] = 1 # noobj_conf_mask[:, 4] = 1; noobj_conf_mask[:, 9] = 1\n",
    "        noobj_pred_conf = noobj_pred[noobj_conf_mask]       # [n_noobj, 2=len([conf1, conf2])]\n",
    "        noobj_target_conf = noobj_target[noobj_conf_mask]   # [n_noobj, 2=len([conf1, conf2])]\n",
    "        loss_noobj = F.mse_loss(noobj_pred_conf, noobj_target_conf, reduction='sum')\n",
    "\n",
    "        # Compute loss for the cells with objects.\n",
    "        coord_response_mask = BoolTensor(bbox_target.size()).fill_(0)    # [n_coord x B, 5]\n",
    "        coord_not_response_mask = BoolTensor(bbox_target.size()).fill_(1)# [n_coord x B, 5]\n",
    "        bbox_target_iou = torch.zeros(bbox_target.size())                    # [n_coord x B, 5], only the last 1=(conf,) is used\n",
    "\n",
    "        # Choose the predicted bbox having the highest IoU for each target bbox.\n",
    "        for i in range(0, bbox_target.size(0), B):\n",
    "            pred = bbox_pred[i:i+B] # predicted bboxes at i-th cell, [B, 5=len([x, y, w, h, conf])]\n",
    "            pred_xyxy = Variable(torch.FloatTensor(pred.size())) # [B, 5=len([x1, y1, x2, y2, conf])]\n",
    "            # Because (center_x,center_y)=pred[:, 2] and (w,h)=pred[:,2:4] are normalized for cell-size and image-size respectively,\n",
    "            # rescale (center_x,center_y) for the image-size to compute IoU correctly.\n",
    "            pred_xyxy[:,  :2] = pred[:, 2]/float(S) - 0.5 * pred[:, 2:4]\n",
    "            pred_xyxy[:, 2:4] = pred[:, 2]/float(S) + 0.5 * pred[:, 2:4]\n",
    "\n",
    "            target = bbox_target[i] # target bbox at i-th cell. Because target boxes contained by each cell are identical in current implementation, enough to extract the first one.\n",
    "            target = bbox_target[i].view(-1, 5) # target bbox at i-th cell, [1, 5=len([x, y, w, h, conf])]\n",
    "            target_xyxy = Variable(torch.FloatTensor(target.size())) # [1, 5=len([x1, y1, x2, y2, conf])]\n",
    "            # Because (center_x,center_y)=target[:, 2] and (w,h)=target[:,2:4] are normalized for cell-size and image-size respectively,\n",
    "            # rescale (center_x,center_y) for the image-size to compute IoU correctly.\n",
    "            target_xyxy[:,  :2] = target[:, 2]/float(S) - 0.5 * target[:, 2:4]\n",
    "            target_xyxy[:, 2:4] = target[:, 2]/float(S) + 0.5 * target[:, 2:4]\n",
    "\n",
    "            iou = self.compute_iou(pred_xyxy[:, :4], target_xyxy[:, :4]) # [B, 1]\n",
    "            max_iou, max_index = iou.max(0)\n",
    "            max_index = max_index.data\n",
    "\n",
    "            coord_response_mask[i+max_index] = 1\n",
    "            coord_not_response_mask[i+max_index] = 0\n",
    "\n",
    "            # \"we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth\"\n",
    "            # from the original paper of YOLO.\n",
    "            bbox_target_iou[i+max_index, LongTensor([4])] = (max_iou).data\n",
    "        bbox_target_iou = Variable(bbox_target_iou).to(device)\n",
    "\n",
    "        # BBox location/size and objectness loss for the response bboxes.\n",
    "        bbox_pred_response = bbox_pred[coord_response_mask].view(-1, 5)      # [n_response, 5]\n",
    "        bbox_target_response = bbox_target[coord_response_mask].view(-1, 5)  # [n_response, 5], only the first 4=(x, y, w, h) are used\n",
    "        target_iou = bbox_target_iou[coord_response_mask].view(-1, 5)        # [n_response, 5], only the last 1=(conf,) is used\n",
    "        loss_xy = F.mse_loss(bbox_pred_response[:, :2], bbox_target_response[:, :2], reduction='sum')\n",
    "        loss_wh = F.mse_loss(torch.sqrt(bbox_pred_response[:, 2:4]), torch.sqrt(bbox_target_response[:, 2:4]), reduction='sum')\n",
    "        loss_obj = F.mse_loss(bbox_pred_response[:, 4], target_iou[:, 4], reduction='sum')\n",
    "\n",
    "        # Class probability loss for the cells which contain objects.\n",
    "        loss_class = F.mse_loss(class_pred, class_target, reduction='sum')\n",
    "\n",
    "        # Total loss\n",
    "        loss = self.lambda_coord * (loss_xy + loss_wh) + loss_obj + self.lambda_noobj * loss_noobj + loss_class\n",
    "        loss = loss / float(batch_size)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RmDecoder(nn.Module):\n",
    "    def __init__(self, rm_dim):\n",
    "        super(RmDecoder, self).__init__()\n",
    "        \n",
    "        self.rm_dim = 800\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(6 * ENCODER_HIDDEN, 2 * 15 * 15),\n",
    "            nn.ReLU(),\n",
    "            ReshapeLayer2d(2, 15),\n",
    "            nn.ConvTranspose2d(2, 2, kernel_size=4, stride = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(2, 2, kernel_size=10, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=4),\n",
    "            nn.ConvTranspose2d(2, 2, kernel_size=4, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 1, kernel_size = 3, stride = 1),\n",
    "            # Sigmoid is final layer in Yolo v1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.model(x)\n",
    "        x = x.squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Model does that does both tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KobeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, encoder_features, rm_dim):\n",
    "        super(KobeModel, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.encoder = PreTaskEncoder(encoder_features)\n",
    "        \n",
    "        \n",
    "        #self.shared_decoder = nn.Sequential()\n",
    "        \n",
    "        self.yolo_decoder = YoloDecoder(num_classes = num_classes)\n",
    "        \n",
    "        self.yolo_loss = YoloLoss(feature_size=S, num_bboxes=B, num_classes=num_classes, \n",
    "                                  lambda_coord=l_coord, lambda_noobj = l_noobj)\n",
    "        \n",
    "        self.rm_decoder = RmDecoder(rm_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \n",
    "        # get all the representations laid out like this\n",
    "        x = torch.cat([self.encoder(x[:, i, :]) for i in range(6)], dim = 1)\n",
    "            \n",
    "            \n",
    "        #convert from dense representation from encoder into an image\n",
    "        # x.view(...)\n",
    "        \n",
    "        #x = self.shared_decoder(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, yolo_targets = None, rm_targets = None ):\n",
    "        encoding = self.encode(x)\n",
    "        \n",
    "        output_1, yolo_loss = self.get_bounding_boxes(x, encoding = encoding, targets = yolo_targets)\n",
    "        \n",
    "        # roadmap decoder\n",
    "        output_2, rm_loss = self.get_road_map(x, encoding, targets = rm_targets)\n",
    "        \n",
    "        # output1 is not in the context of our bounding boxes\n",
    "        #return output_1, output_2, yolo_loss, rm_loss\n",
    "        return output_1, yolo_loss, output_2, rm_loss\n",
    "    \n",
    "    # for easy use for competition\n",
    "    # in competition, encoding is None\n",
    "    def get_bounding_boxes(self, x, encoding = None, targets = None):\n",
    "        if encoding is None:\n",
    "            encoding = self.encode(x)\n",
    "        \n",
    "        outputs = self.yolo_decoder(encoding)\n",
    "        \n",
    "        if targets is not None:\n",
    "            yoloLossValue = self.yolo_loss(outputs, targets)\n",
    "        else:\n",
    "            yoloLossValue = 0\n",
    "        \n",
    "        \n",
    "        boxes = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            # Get detected boxes_detected, labels, confidences, class-scores.\n",
    "            boxes_normalized_all, class_labels_all, confidences_all, class_scores_all = pred_decode(output)\n",
    "            if boxes_normalized_all.size(0) == 0:\n",
    "                return ()\n",
    "\n",
    "            # Apply non maximum supression for boxes of each class.\n",
    "            boxes_normalized, class_labels, probs = [], [], []\n",
    "\n",
    "            for class_label in range(self.num_classes):\n",
    "                mask = (class_labels_all == class_label)\n",
    "                if torch.sum(mask) == 0:\n",
    "                    continue # if no box found, skip that class.\n",
    "\n",
    "                boxes_normalized_masked = boxes_normalized_all[mask]\n",
    "                class_labels_maked = class_labels_all[mask]\n",
    "                confidences_masked = confidences_all[mask]\n",
    "                class_scores_masked = class_scores_all[mask]\n",
    "\n",
    "                ids = nms(boxes_normalized_masked, confidences_masked)\n",
    "\n",
    "                boxes_normalized.append(boxes_normalized_masked[ids])\n",
    "                class_labels.append(class_labels_maked[ids])\n",
    "                probs.append(confidences_masked[ids] * class_scores_masked[ids])\n",
    "\n",
    "            boxes_normalized = torch.cat(boxes_normalized, 0)\n",
    "            class_labels = torch.cat(class_labels, 0)\n",
    "            probs = torch.cat(probs, 0)\n",
    "        \n",
    "\n",
    "            better_coordinates = FloatTensor(boxes_normalized.shape[0], 2, 4)\n",
    "            translation = FloatTensor(boxes_normalized.shape[0], 2, 4)\n",
    "            translation[:, 0, :].fill_(-40)\n",
    "            translation[:, 1, :].fill_(40)\n",
    "\n",
    "            center_x = (boxes_normalized[:, 0] + boxes_normalized[:, 2]) / 2 * WIDTH\n",
    "            center_y = (boxes_normalized[:, 1] + boxes_normalized[:, 3]) / 2 * HEIGHT\n",
    "            width = (boxes_normalized[:, 2] - boxes_normalized[:,0]) * WIDTH\n",
    "            height = (boxes_normalized[:, 3] - boxes_normalized[:,1]) * HEIGHT\n",
    "            \n",
    "            x1 = center_x - width/2\n",
    "            x2 = center_x + width/2\n",
    "            x3 = center_x - width/2\n",
    "            x4 = center_x + width/2\n",
    "            \n",
    "            \n",
    "            y1 = center_y - height/2\n",
    "            y2 = center_y + height/2\n",
    "            y3 = center_y + height/2\n",
    "            y4 = center_y - height/2\n",
    "            \n",
    "            better_coordinates[:, 0, 0] = x1\n",
    "            better_coordinates[:, 0, 1] = x2\n",
    "            better_coordinates[:, 0, 2] = x3\n",
    "            better_coordinates[:, 0, 3] = x4\n",
    "            \n",
    "            better_coordinates[:, 1, 0] = y1\n",
    "            better_coordinates[:, 1, 1] = y2\n",
    "            better_coordinates[:, 1, 2] = y3\n",
    "            better_coordinates[:, 1, 3] = y4\n",
    "            \n",
    "            better_coordinates[:, 1, :].mul_(-1)\n",
    "            # shift back!\n",
    "            better_coordinates += translation\n",
    "            \n",
    "            boxes.append(better_coordinates)\n",
    "        \n",
    "        #print('got this incoming')\n",
    "        #print(x.shape)\n",
    "        #print('got these many boxes to look at {}'.format(len(boxes)))\n",
    "        ##print('it has this many detections {} at one site'.format(boxes[0].shape[0]))\n",
    "        #print(\"looks like this\")\n",
    "        #print(boxes[0])\n",
    "        #print(\"Bounding box outputs\")\n",
    "        #print(tuple(boxes))\n",
    "        return tuple(boxes), yoloLossValue\n",
    "    \n",
    "    def get_road_map(self, x, encoding = None, targets = None):\n",
    "        if encoding is None:\n",
    "            encoding = self.encode(x)\n",
    "        \n",
    "        outputs = self.rm_decoder(encoding)\n",
    "        bce_loss = nn.BCELoss()\n",
    "        if targets is not None:\n",
    "            loss = bce_loss(outputs, targets) / x.shape[0]\n",
    "        else:\n",
    "            loss = 0\n",
    "        #print('roadmap outputs')\n",
    "        #print(outputs)\n",
    "        return outputs, loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kobe_model = KobeModel(num_classes = 10, encoder_features = 6, rm_dim = 800)\n",
    "#kobe_model = initialize_model_for_training_file(FILE_OF_PRETRAINING_ENCODER)\n",
    "\n",
    "kobe_model.to(device)\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "\n",
    "kobe_optimizer = torch.optim.Adam(kobe_model.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'data'\n",
    "annotation_csv = 'data/annotation.csv'\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, \n",
    "                                          batch_size=1, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2, \n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for sample, target, road_image, extra in trainloader:\n",
    "    sample = torch.stack(sample).to(device)\n",
    "    target = transform_target(target).to(device)\n",
    "    road_image = torch.stack(road_image)\n",
    "    \n",
    "    output_yolo, yolo_loss = kobe_model(sample, yolo_target = target)\n",
    "    \n",
    "    # SHOULD GET LOWER OVER EPOCHS\n",
    "    print(output_yolo[0].shape)\n",
    "    yolo_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "Original\n",
      "({'bounding_box': tensor([[[ 1.6391e+01,  1.6377e+01,  1.1983e+01,  1.1969e+01],\n",
      "         [-2.5280e+00, -4.3019e+00, -2.4936e+00, -4.2674e+00]],\n",
      "\n",
      "        [[-2.8723e+01, -2.8724e+01, -3.3450e+01, -3.3452e+01],\n",
      "         [ 8.7188e+00,  6.8099e+00,  8.7228e+00,  6.8139e+00]],\n",
      "\n",
      "        [[ 1.5490e+00,  1.5841e+00,  6.1319e+00,  6.1670e+00],\n",
      "         [ 1.5576e+01,  1.7485e+01,  1.5492e+01,  1.7401e+01]],\n",
      "\n",
      "        [[ 2.5278e+00,  2.4156e+00,  7.0239e+00,  6.9117e+00],\n",
      "         [ 1.1798e+01,  1.3609e+01,  1.2077e+01,  1.3888e+01]],\n",
      "\n",
      "        [[ 3.1396e+01,  3.1414e+01,  2.5926e+01,  2.5945e+01],\n",
      "         [-2.9346e+00, -4.9084e+00, -2.9873e+00, -4.9611e+00]],\n",
      "\n",
      "        [[-2.0492e+01, -2.0513e+01, -2.5270e+01, -2.5291e+01],\n",
      "         [ 1.0609e+00, -8.1192e-01,  1.1150e+00, -7.5782e-01]],\n",
      "\n",
      "        [[ 1.4886e+01,  1.4708e+01,  1.9025e+01,  1.8847e+01],\n",
      "         [ 1.2336e+01,  1.4206e+01,  1.2730e+01,  1.4600e+01]],\n",
      "\n",
      "        [[ 8.4685e+00,  8.4858e+00,  3.6670e+00,  3.6843e+00],\n",
      "         [-2.2910e+00, -4.1048e+00, -2.3373e+00, -4.1511e+00]],\n",
      "\n",
      "        [[-1.7087e+01, -1.7093e+01, -2.1671e+01, -2.1677e+01],\n",
      "         [ 5.0717e+00,  3.0068e+00,  5.0841e+00,  3.0192e+00]],\n",
      "\n",
      "        [[ 1.3446e+01,  1.3480e+01,  1.2583e+01,  1.2617e+01],\n",
      "         [-5.7316e+00, -6.5859e+00, -5.7662e+00, -6.6205e+00]],\n",
      "\n",
      "        [[-2.3975e+00, -2.4119e+00, -7.2121e+00, -7.2265e+00],\n",
      "         [-2.1461e+00, -3.9600e+00, -2.1085e+00, -3.9223e+00]],\n",
      "\n",
      "        [[-1.0780e+01, -1.0815e+01, -1.5164e+01, -1.5199e+01],\n",
      "         [ 8.3130e+00,  6.4045e+00,  8.3932e+00,  6.4846e+00]],\n",
      "\n",
      "        [[-1.3357e+01, -1.3340e+01, -7.4191e+00, -7.4027e+00],\n",
      "         [ 1.1670e+01,  1.3745e+01,  1.1624e+01,  1.3699e+01]],\n",
      "\n",
      "        [[-2.8233e+01, -2.8250e+01, -2.3363e+01, -2.3381e+01],\n",
      "         [ 1.1730e+01,  1.3592e+01,  1.1777e+01,  1.3639e+01]],\n",
      "\n",
      "        [[ 8.2225e+00,  8.1103e+00,  1.2817e+01,  1.2705e+01],\n",
      "         [ 1.2037e+01,  1.3847e+01,  1.2322e+01,  1.4132e+01]],\n",
      "\n",
      "        [[ 8.4604e+00,  8.4395e+00,  2.9491e+00,  2.9282e+00],\n",
      "         [ 4.1706e+00,  2.2918e+00,  4.2311e+00,  2.3523e+00]],\n",
      "\n",
      "        [[-1.9984e+01, -2.0002e+01, -1.5755e+01, -1.5773e+01],\n",
      "         [ 1.1902e+01,  1.3764e+01,  1.1943e+01,  1.3805e+01]],\n",
      "\n",
      "        [[-4.8380e+00, -4.8569e+00, -1.0530e-02, -2.9441e-02],\n",
      "         [ 1.1671e+01,  1.3657e+01,  1.1718e+01,  1.3704e+01]],\n",
      "\n",
      "        [[ 2.7670e+00,  2.7843e+00, -1.6985e+00, -1.6812e+00],\n",
      "         [-2.2010e+00, -4.0148e+00, -2.2441e+00, -4.0579e+00]],\n",
      "\n",
      "        [[ 2.3655e+01,  2.3579e+01,  1.9251e+01,  1.9175e+01],\n",
      "         [-2.6689e+00, -4.4412e+00, -2.4807e+00, -4.2529e+00]],\n",
      "\n",
      "        [[-2.2139e+01, -2.2058e+01, -1.7097e+01, -1.7016e+01],\n",
      "         [ 2.6374e+01,  2.8272e+01,  2.6158e+01,  2.8057e+01]],\n",
      "\n",
      "        [[ 3.1323e-01,  2.2623e-01,  2.2629e+00,  2.1759e+00],\n",
      "         [ 2.9097e+01,  2.9770e+01,  2.9349e+01,  3.0023e+01]],\n",
      "\n",
      "        [[-3.5868e+01, -3.5885e+01, -3.1323e+01, -3.1340e+01],\n",
      "         [ 1.2063e+01,  1.3878e+01,  1.2107e+01,  1.3921e+01]],\n",
      "\n",
      "        [[ 2.2185e+01,  2.2129e+01,  1.7470e+01,  1.7415e+01],\n",
      "         [ 7.3268e+00,  5.4487e+00,  7.4651e+00,  5.5871e+00]],\n",
      "\n",
      "        [[ 3.8485e+01,  3.8470e+01,  3.4099e+01,  3.4085e+01],\n",
      "         [-2.9779e+00, -4.8297e+00, -2.9436e+00, -4.7954e+00]]],\n",
      "       dtype=torch.float64), 'category': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n",
      "        2])},)\n",
      "torch.Size([41, 2, 4])\n",
      "torch.Size([25, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2733, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[  2.9577,   3.0474,   2.0995,   2.1892],\n",
      "         [ -5.5691,  -6.4193,  -5.6597,  -6.5099]],\n",
      "\n",
      "        [[-16.8860, -16.9301, -12.0593, -12.1035],\n",
      "         [ 11.5248,  13.5102,  11.6325,  13.6178]],\n",
      "\n",
      "        [[ -3.4054,  -3.3651,  -8.2061,  -8.1658],\n",
      "         [ -2.2688,  -4.0822,  -2.3758,  -4.1892]],\n",
      "\n",
      "        [[ 26.0563,  26.0265,  21.3400,  21.3102],\n",
      "         [  7.3506,   5.4719,   7.4250,   5.5463]],\n",
      "\n",
      "        [[-19.2665, -19.2680, -24.0453, -24.0468],\n",
      "         [  0.8362,  -1.0366,   0.8396,  -1.0332]],\n",
      "\n",
      "        [[ 11.4857,  11.4887,   5.9739,   5.9769],\n",
      "         [  3.9606,   2.0818,   3.9514,   2.0725]],\n",
      "\n",
      "        [[-25.4026, -25.4124, -19.4648, -19.4746],\n",
      "         [ 11.4166,  13.4914,  11.4454,  13.5202]],\n",
      "\n",
      "        [[-13.4809, -13.4669, -18.0647, -18.0507],\n",
      "         [  4.7774,   2.7126,   4.7460,   2.6812]],\n",
      "\n",
      "        [[ -7.3058,  -7.3167, -11.6906, -11.7015],\n",
      "         [  8.1022,   6.1934,   8.1268,   6.2180]],\n",
      "\n",
      "        [[ -9.5229,  -9.6581,  -5.0305,  -5.1657],\n",
      "         [ 11.7445,  13.5533,  12.0805,  13.8893]],\n",
      "\n",
      "        [[ -9.1080,  -9.0677, -13.5728, -13.5325],\n",
      "         [ -2.2511,  -4.0645,  -2.3507,  -4.1641]],\n",
      "\n",
      "        [[  4.5195,   4.5279,   0.1116,   0.1201],\n",
      "         [ -2.4055,  -4.1794,  -2.4269,  -4.2007]],\n",
      "\n",
      "        [[ -3.8310,  -3.9661,   0.7601,   0.6250],\n",
      "         [ 12.0554,  13.8642,  12.3988,  14.2076]],\n",
      "\n",
      "        [[  2.6657,   2.4859,   6.8045,   6.6247],\n",
      "         [ 12.4746,  14.3449,  12.8729,  14.7431]],\n",
      "\n",
      "        [[ 19.5291,  19.5729,  14.0606,  14.1044],\n",
      "         [ -2.6219,  -4.5953,  -2.7439,  -4.7173]],\n",
      "\n",
      "        [[ 11.7846,  11.7312,   7.3788,   7.3253],\n",
      "         [ -2.4544,  -4.2275,  -2.3220,  -4.0950]],\n",
      "\n",
      "        [[-32.0358, -32.0772, -27.8070, -27.8483],\n",
      "         [ 11.5635,  13.4249,  11.6578,  13.5192]],\n",
      "\n",
      "        [[ 11.5894,  11.6076,  10.1442,  10.1624],\n",
      "         [ -6.4367,  -6.8002,  -6.5089,  -6.8724]],\n",
      "\n",
      "        [[ 26.6191,  26.6280,  22.2333,  22.2421],\n",
      "         [ -2.5752,  -4.4271,  -2.5965,  -4.4483]],\n",
      "\n",
      "        [[ 31.6897,  31.7308,  27.3048,  27.3460],\n",
      "         [ -2.8778,  -4.7292,  -2.9756,  -4.8270]],\n",
      "\n",
      "        [[ 37.3438,  37.3526,  32.6779,  32.6867],\n",
      "         [ -2.8586,  -4.7105,  -2.8813,  -4.7331]],\n",
      "\n",
      "        [[-14.2727, -14.2641, -19.0875, -19.0789],\n",
      "         [ -2.2614,  -4.0753,  -2.2848,  -4.0986]]], dtype=torch.float64), 'category': tensor([3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2])},)\n",
      "torch.Size([46, 2, 4])\n",
      "torch.Size([22, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2696, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[-33.3837, -33.5572, -32.6516, -32.8251],\n",
      "         [  7.1387,   7.8789,   7.3095,   8.0496]],\n",
      "\n",
      "        [[-22.3984, -22.0327, -27.1284, -26.7627],\n",
      "         [ -1.1091,  -2.9685,  -2.0339,  -3.8933]]], dtype=torch.float64), 'category': tensor([3, 2])},)\n",
      "torch.Size([49, 2, 4])\n",
      "torch.Size([2, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.1951, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[ 18.4104,  18.4687,  14.6029,  14.6611],\n",
      "         [ 11.5554,   9.9299,  11.4190,   9.7935]],\n",
      "\n",
      "        [[  1.5768,   1.5778,  -2.5812,  -2.5802],\n",
      "         [  4.8252,   3.1297,   4.8228,   3.1272]],\n",
      "\n",
      "        [[ -9.4329,  -9.5292,  -4.3541,  -4.4503],\n",
      "         [ 24.8998,  26.8569,  25.1495,  27.1066]],\n",
      "\n",
      "        [[ -2.8817,  -2.9121,   2.6828,   2.6523],\n",
      "         [ 25.4406,  27.5798,  25.5197,  27.6589]],\n",
      "\n",
      "        [[-29.1447, -29.2085, -23.5175, -23.5813],\n",
      "         [ 25.2753,  27.2888,  25.4536,  27.4670]],\n",
      "\n",
      "        [[  7.0724,   7.1080,  11.6025,  11.6380],\n",
      "         [ 25.2595,  26.9777,  25.1659,  26.8840]],\n",
      "\n",
      "        [[ -7.0725,  -7.1096,  -1.7052,  -1.7424],\n",
      "         [ 21.4701,  23.6962,  21.5597,  23.7858]],\n",
      "\n",
      "        [[-18.4698, -18.4722, -22.9198, -22.9222],\n",
      "         [  4.3018,   2.3393,   4.3072,   2.3447]],\n",
      "\n",
      "        [[-28.7058, -28.6358, -35.9095, -35.8395],\n",
      "         [  9.0050,   6.2417,   8.8225,   6.0592]],\n",
      "\n",
      "        [[-37.3234, -37.2133, -30.0183, -29.9082],\n",
      "         [ 24.9390,  27.9432,  24.6715,  27.6757]],\n",
      "\n",
      "        [[-22.5382, -22.5698, -27.2395, -27.2711],\n",
      "         [  0.8151,  -1.0931,   0.8929,  -1.0153]],\n",
      "\n",
      "        [[-22.6569, -22.7538, -17.7888, -17.8856],\n",
      "         [ 24.8989,  26.8690,  25.1382,  27.1083]],\n",
      "\n",
      "        [[ 36.7539,  36.6886,  32.2218,  32.1565],\n",
      "         [ -3.0504,  -4.8767,  -2.8885,  -4.7148]],\n",
      "\n",
      "        [[-20.6156, -18.8157, -19.6937, -17.8938],\n",
      "         [-27.1065, -26.6877, -31.0665, -30.6477]],\n",
      "\n",
      "        [[-15.3856, -15.3792, -10.9346, -10.9282],\n",
      "         [ 25.2711,  27.2306,  25.2567,  27.2162]],\n",
      "\n",
      "        [[-16.1351, -16.1921, -21.0492, -21.1063],\n",
      "         [ -2.6780,  -4.3546,  -2.5109,  -4.1875]]], dtype=torch.float64), 'category': tensor([2, 2, 2, 2, 2, 2, 2, 2, 4, 0, 2, 2, 2, 2, 2, 2])},)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 2, 4])\n",
      "torch.Size([16, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.3007, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[-32.1736, -32.2316, -27.3039, -27.3619],\n",
      "         [ 11.8002,  13.6830,  11.9503,  13.8332]],\n",
      "\n",
      "        [[-17.7522, -17.7901, -18.4653, -18.5031],\n",
      "         [ -6.3254,  -7.0463,  -6.2879,  -7.0088]],\n",
      "\n",
      "        [[ 38.2032,  38.2784,  33.8952,  33.9704],\n",
      "         [  9.6364,   7.8933,   9.4505,   7.7073]],\n",
      "\n",
      "        [[-21.3456, -21.2750, -17.7173, -17.6468],\n",
      "         [ 22.2167,  24.0381,  22.0763,  23.8977]],\n",
      "\n",
      "        [[ 19.0913,  19.0260,  23.7718,  23.7065],\n",
      "         [ 15.1087,  17.1104,  15.2617,  17.2634]],\n",
      "\n",
      "        [[ -3.1366,  -3.1143,  -7.9013,  -7.8790],\n",
      "         [  7.9434,   6.0337,   7.8875,   5.9779]],\n",
      "\n",
      "        [[ -3.6261,  -3.6603,   1.0562,   1.0220],\n",
      "         [ 14.7384,  16.7408,  14.8187,  16.8212]],\n",
      "\n",
      "        [[-12.5388, -12.4267,  -7.8266,  -7.7145],\n",
      "         [ 25.1342,  26.9615,  24.8454,  26.6727]],\n",
      "\n",
      "        [[ 15.0461,  15.1351,  10.8157,  10.9047],\n",
      "         [  8.5718,   6.6642,   8.3744,   6.4668]],\n",
      "\n",
      "        [[-22.1553, -21.7764, -21.8180, -21.4390],\n",
      "         [-15.5121, -15.1331, -15.8493, -15.4703]],\n",
      "\n",
      "        [[ 37.6233,  37.7130,  33.1268,  33.2165],\n",
      "         [  6.0686,   4.2550,   5.8461,   4.0326]],\n",
      "\n",
      "        [[ 33.2504,  33.2353,  32.5076,  32.4925],\n",
      "         [ -3.7919,  -4.5067,  -3.7763,  -4.4911]],\n",
      "\n",
      "        [[ 33.3672,  33.3931,  32.4498,  32.4757],\n",
      "         [ -4.4819,  -5.2263,  -4.5138,  -5.2582]],\n",
      "\n",
      "        [[ -9.3978,  -7.2548,  -9.7382,  -7.5952],\n",
      "         [ -4.7355,  -4.8820,  -9.7123,  -9.8587]],\n",
      "\n",
      "        [[-37.3401, -37.1363, -32.4454, -32.2417],\n",
      "         [ 23.9682,  25.9294,  23.4600,  25.4212]],\n",
      "\n",
      "        [[ -6.4941,  -6.5035,  -1.4792,  -1.4886],\n",
      "         [ 24.8353,  26.8880,  24.8584,  26.9111]],\n",
      "\n",
      "        [[-24.9881, -25.1928, -30.1522, -30.3570],\n",
      "         [  0.9947,  -1.0488,   1.5118,  -0.5317]],\n",
      "\n",
      "        [[ -4.3900,  -4.4661,  15.2146,  15.1385],\n",
      "         [ 21.4478,  24.4544,  21.9450,  24.9516]],\n",
      "\n",
      "        [[ 25.8320,  25.9072,  21.5241,  21.5992],\n",
      "         [  5.4033,   3.6601,   5.2173,   3.4741]],\n",
      "\n",
      "        [[  2.8473,   2.7968,   7.8287,   7.7783],\n",
      "         [ 18.2261,  20.2612,  18.3497,  20.3849]],\n",
      "\n",
      "        [[-11.0323, -10.9890,  -6.5195,  -6.4762],\n",
      "         [ 18.7738,  20.7121,  18.6733,  20.6116]],\n",
      "\n",
      "        [[-36.6666, -34.7474, -37.4640, -35.5447],\n",
      "         [ -3.8414,  -4.1862,  -8.2778,  -8.6226]],\n",
      "\n",
      "        [[-31.0384, -31.2500, -35.4326, -35.6442],\n",
      "         [  8.9985,   6.9658,   9.4556,   7.4228]],\n",
      "\n",
      "        [[-23.9280, -23.8502, -19.4033, -19.3254],\n",
      "         [ 15.4369,  17.2310,  15.2408,  17.0349]],\n",
      "\n",
      "        [[-13.4366, -13.3074, -17.8765, -17.7472],\n",
      "         [  6.6120,   4.7908,   6.2968,   4.4756]],\n",
      "\n",
      "        [[ 27.0883,  26.9897,  31.7534,  31.6548],\n",
      "         [ 22.9327,  24.8970,  23.1670,  25.1313]]], dtype=torch.float64), 'category': tensor([2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2])},)\n",
      "torch.Size([55, 2, 4])\n",
      "torch.Size([26, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.3332, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[-15.7636, -15.8354, -20.0146, -20.0863],\n",
      "         [  1.0814,  -0.8136,   1.2420,  -0.6530]],\n",
      "\n",
      "        [[ 10.0953,  10.0611,   5.7592,   5.7250],\n",
      "         [  7.8914,   6.2144,   7.9795,   6.3025]],\n",
      "\n",
      "        [[ 26.1117,  27.8887,  24.0491,  25.8261],\n",
      "         [-11.8605, -12.6847, -16.3038, -17.1281]],\n",
      "\n",
      "        [[ 12.1892,  12.1205,   7.7694,   7.7007],\n",
      "         [  0.4427,  -1.3714,   0.6097,  -1.2044]],\n",
      "\n",
      "        [[-34.5425, -34.6392, -24.4486, -24.5453],\n",
      "         [ 25.0331,  27.8215,  25.3837,  28.1721]],\n",
      "\n",
      "        [[ 21.1964,  19.9885,  24.9659,  23.7580],\n",
      "         [ 26.7876,  28.3369,  29.7244,  31.2737]],\n",
      "\n",
      "        [[ 13.4722,  13.4357,   9.0811,   9.0446],\n",
      "         [ 10.8240,   9.0329,  10.9132,   9.1222]],\n",
      "\n",
      "        [[-29.1436, -29.0752, -24.8886, -24.8203],\n",
      "         [ 19.4014,  21.2055,  19.2406,  21.0447]],\n",
      "\n",
      "        [[ 21.4921,  21.4235,  16.7575,  16.6889],\n",
      "         [ 11.4566,   9.6455,  11.6354,   9.8244]],\n",
      "\n",
      "        [[  5.3305,   5.2637,   0.7917,   0.7249],\n",
      "         [ 11.7630,   9.9999,  11.9345,  10.1714]],\n",
      "\n",
      "        [[  5.5918,   5.5231,   1.1110,   1.0423],\n",
      "         [  4.2159,   2.4018,   4.3851,   2.5711]],\n",
      "\n",
      "        [[ 14.1140,  14.0395,   9.3944,   9.3199],\n",
      "         [  4.0255,   2.0596,   4.2038,   2.2379]],\n",
      "\n",
      "        [[ -2.8686,  -2.9310,  -7.5092,  -7.5716],\n",
      "         [  8.2685,   6.3112,   8.4160,   6.4587]],\n",
      "\n",
      "        [[ 17.2873,  17.2171,  12.5677,  12.4975],\n",
      "         [  7.6150,   5.7630,   7.7933,   5.9413]],\n",
      "\n",
      "        [[ 35.8150,  34.0464,  36.9565,  35.1880],\n",
      "         [ 22.0484,  22.5448,  26.1118,  26.6082]],\n",
      "\n",
      "        [[ 31.9025,  31.6311,  36.6150,  36.3436],\n",
      "         [-34.6087, -32.8299, -33.8901, -32.1113]],\n",
      "\n",
      "        [[-29.9637, -30.0355, -34.2146, -34.2864],\n",
      "         [  0.9111,  -0.9839,   1.0717,  -0.8233]],\n",
      "\n",
      "        [[ 23.8028,  25.5758,  21.7852,  23.5583],\n",
      "         [-25.9629, -26.7955, -30.2562, -31.0888]],\n",
      "\n",
      "        [[ -4.5059,  -4.5429,  -8.9890,  -9.0260],\n",
      "         [  0.9967,  -0.8183,   1.0878,  -0.7272]],\n",
      "\n",
      "        [[ 23.9709,  25.7619,  22.0157,  23.8068],\n",
      "         [-31.7134, -32.5065, -36.1253, -36.9184]],\n",
      "\n",
      "        [[ -5.1651,  -5.2439,  -9.8691,  -9.9479],\n",
      "         [  4.3870,   2.4813,   4.5812,   2.6755]],\n",
      "\n",
      "        [[-22.3617, -22.4335, -26.6127, -26.6844],\n",
      "         [  1.9594,   0.0644,   2.1200,   0.2250]],\n",
      "\n",
      "        [[ 32.2227,  34.0273,  30.2063,  32.0110],\n",
      "         [ -6.8808,  -7.6425, -11.6540, -12.4157]],\n",
      "\n",
      "        [[  4.2835,   4.4935,   9.2247,   9.4348],\n",
      "         [ 21.3952,  23.3382,  20.8617,  22.8047]],\n",
      "\n",
      "        [[ 22.1057,  23.8827,  20.1761,  21.9531],\n",
      "         [-20.2468, -21.0710, -24.4036, -25.2278]],\n",
      "\n",
      "        [[ -9.3798,  -8.6582,  -9.1953,  -8.4738],\n",
      "         [ 30.3088,  30.4901,  29.5749,  29.7562]]], dtype=torch.float64), 'category': tensor([2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 3])},)\n",
      "torch.Size([57, 2, 4])\n",
      "torch.Size([26, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.3556, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[  2.2155,   2.0906,  -2.2585,  -2.3834],\n",
      "         [  8.0419,   6.1748,   8.3411,   6.4740]],\n",
      "\n",
      "        [[ 32.2124,  32.6279,  36.8178,  37.2333],\n",
      "         [ 11.7913,  13.8306,  10.8534,  12.8928]],\n",
      "\n",
      "        [[-21.7991, -21.7293, -27.0124, -26.9426],\n",
      "         [  1.6778,  -0.4853,   1.5100,  -0.6531]],\n",
      "\n",
      "        [[-17.5529, -17.5044, -22.0702, -22.0217],\n",
      "         [ -1.5374,  -3.3231,  -1.6598,  -3.4454]],\n",
      "\n",
      "        [[ 17.0929,  15.6212,  14.3264,  12.8547],\n",
      "         [  8.6479,   7.6016,  12.5361,  11.4897]],\n",
      "\n",
      "        [[ -3.7719,  -3.8353,  -8.1185,  -8.1819],\n",
      "         [  1.2608,  -0.6415,   1.4058,  -0.4965]],\n",
      "\n",
      "        [[-34.0847, -33.9979, -38.7799, -38.6931],\n",
      "         [  4.6182,   2.8740,   4.3850,   2.6408]],\n",
      "\n",
      "        [[ 28.5760,  28.9422,  34.1615,  34.5277],\n",
      "         [ 19.2338,  21.2257,  18.2076,  20.1995]],\n",
      "\n",
      "        [[ 35.6074,  35.3730,  31.2402,  31.0058],\n",
      "         [ -6.4557,  -8.2578,  -5.8879,  -7.6900]],\n",
      "\n",
      "        [[ 24.9486,  25.2324,  29.4531,  29.7370],\n",
      "         [ 12.9894,  14.5878,  12.1898,  13.7882]],\n",
      "\n",
      "        [[-10.8018, -11.2239, -15.0340, -15.4561],\n",
      "         [ -0.3828,  -2.1669,   0.6181,  -1.1659]],\n",
      "\n",
      "        [[-28.5662, -28.6339, -16.8476, -16.9153],\n",
      "         [ 18.1035,  21.4306,  18.3412,  21.6682]],\n",
      "\n",
      "        [[-14.1212, -14.0451,  -9.4873,  -9.4113],\n",
      "         [ 26.2580,  28.0497,  26.0613,  27.8530]],\n",
      "\n",
      "        [[ 32.8837,  33.3746,  28.3960,  28.8869],\n",
      "         [  0.9623,  -0.7095,  -0.3541,  -2.0260]],\n",
      "\n",
      "        [[  3.9773,   4.0651,   3.1457,   3.2335],\n",
      "         [ -5.9571,  -6.6624,  -6.0606,  -6.7658]],\n",
      "\n",
      "        [[ 20.7589,  21.0575,  25.2316,  25.5302],\n",
      "         [ 21.1034,  22.8443,  20.3365,  22.0775]]], dtype=torch.float64), 'category': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 3, 2])},)\n",
      "torch.Size([60, 2, 4])\n",
      "torch.Size([16, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.3468, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[ 33.2366,  33.3132,  37.4570,  37.5337],\n",
      "         [  4.1939,   6.0643,   4.0210,   5.8914]],\n",
      "\n",
      "        [[ -8.3093,  -8.3765,  -3.3374,  -3.4047],\n",
      "         [ 18.1877,  20.0925,  18.3635,  20.2682]],\n",
      "\n",
      "        [[ -8.2893,  -8.2354, -13.3744, -13.3205],\n",
      "         [ -1.4114,  -3.4066,  -1.5488,  -3.5440]],\n",
      "\n",
      "        [[-33.0502, -33.0323, -38.2290, -38.2111],\n",
      "         [  1.1624,  -0.8694,   1.1167,  -0.9152]]], dtype=torch.float64), 'category': tensor([2, 2, 2, 2])},)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([62, 2, 4])\n",
      "torch.Size([4, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2046, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[ 36.5343,  38.5398,  37.4398,  39.4452],\n",
      "         [-33.5191, -33.0860, -37.7153, -37.2823]],\n",
      "\n",
      "        [[-14.2955, -14.3432, -19.7116, -19.7593],\n",
      "         [  1.4202,  -0.4182,   1.5604,  -0.2780]],\n",
      "\n",
      "        [[-33.8246, -33.8629, -38.4366, -38.4748],\n",
      "         [  5.0455,   3.3430,   5.1488,   3.4462]],\n",
      "\n",
      "        [[ 35.0621,  37.0323,  36.0827,  38.0529],\n",
      "         [-26.0264, -25.6010, -30.7564, -30.3310]],\n",
      "\n",
      "        [[-19.9196, -20.1970, -20.3466, -20.6240],\n",
      "         [ 30.5238,  30.4587,  32.3453,  32.2803]],\n",
      "\n",
      "        [[-19.2811, -19.5585, -19.7243, -20.0017],\n",
      "         [ 28.9557,  28.8906,  30.8464,  30.7813]],\n",
      "\n",
      "        [[-18.5071, -18.5206, -27.1013, -27.1148],\n",
      "         [ -1.4652,  -4.2652,  -1.4244,  -4.2243]],\n",
      "\n",
      "        [[ 31.5052,  33.5864,  32.6843,  34.7655],\n",
      "         [ -9.0392,  -8.5132, -13.7074, -13.1814]],\n",
      "\n",
      "        [[ 14.6786,  16.3876,  17.0485,  18.7575],\n",
      "         [ 19.6973,  20.7261,  15.7588,  16.7876]],\n",
      "\n",
      "        [[-20.8474, -21.1248, -21.2743, -21.5518],\n",
      "         [ 30.3405,  30.2754,  32.1620,  32.0969]],\n",
      "\n",
      "        [[-19.6825, -19.9599, -20.1095, -20.3869],\n",
      "         [ 28.9350,  28.8700,  30.7566,  30.6915]],\n",
      "\n",
      "        [[-12.6313, -12.5898,  -7.4996,  -7.4581],\n",
      "         [ 35.1046,  36.9501,  34.9895,  36.8350]],\n",
      "\n",
      "        [[ -5.4346,  -5.3905,  -0.5018,  -0.4577],\n",
      "         [ 34.9102,  36.8697,  34.7995,  36.7590]],\n",
      "\n",
      "        [[-21.2360, -21.5134, -21.6630, -21.9404],\n",
      "         [ 30.2340,  30.1690,  32.0556,  31.9905]],\n",
      "\n",
      "        [[ 36.8277,  38.7931,  36.3511,  38.3165],\n",
      "         [ -7.6216,  -7.8218, -12.2973, -12.4976]],\n",
      "\n",
      "        [[ 31.4025,  31.3617,  27.9130,  27.8722],\n",
      "         [  3.6406,   1.8250,   3.7187,   1.9032]],\n",
      "\n",
      "        [[-34.1329, -34.0367, -28.6274, -28.5312],\n",
      "         [ 25.8630,  28.2701,  25.6433,  28.0504]],\n",
      "\n",
      "        [[ 32.9592,  35.1279,  34.2211,  36.3898],\n",
      "         [-17.0818, -16.6136, -22.9300, -22.4618]],\n",
      "\n",
      "        [[ 20.4549,  18.1795,  19.3549,  17.0795],\n",
      "         [ 31.2515,  30.7382,  36.1309,  35.6175]],\n",
      "\n",
      "        [[ 21.6822,  21.6268,  16.9513,  16.8960],\n",
      "         [  7.3890,   5.5838,   7.5337,   5.7285]],\n",
      "\n",
      "        [[-31.9642, -32.0293, -30.0738, -30.1389],\n",
      "         [ 30.9685,  31.2459,  31.4119,  31.6893]],\n",
      "\n",
      "        [[-19.5845, -19.8620, -20.0115, -20.2889],\n",
      "         [ 30.6193,  30.5542,  32.4409,  32.3758]],\n",
      "\n",
      "        [[-13.4492, -13.4764, -17.8028, -17.8300],\n",
      "         [  4.5474,   2.7896,   4.6145,   2.8567]],\n",
      "\n",
      "        [[ 33.1377,  32.9370,  33.9512,  33.7505],\n",
      "         [ 25.7902,  26.4451,  26.0396,  26.6945]],\n",
      "\n",
      "        [[ 36.0383,  38.2549,  37.2739,  39.4904],\n",
      "         [-15.0855, -14.6068, -20.8115, -20.3329]]], dtype=torch.float64), 'category': tensor([2, 2, 2, 2, 1, 1, 4, 2, 2, 1, 1, 2, 2, 1, 2, 2, 4, 2, 2, 2, 1, 1, 2, 3,\n",
      "        4])},)\n",
      "torch.Size([62, 2, 4])\n",
      "torch.Size([25, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.3333, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[ 22.3649,  22.3173,  17.9824,  17.9348],\n",
      "         [  7.1221,   5.2848,   7.2356,   5.3983]],\n",
      "\n",
      "        [[  6.7117,   6.5746,  11.4752,  11.3382],\n",
      "         [-25.8450, -23.8668, -25.5150, -23.5368]],\n",
      "\n",
      "        [[-28.3820, -28.3974, -32.7779, -32.7932],\n",
      "         [  4.9844,   3.1686,   5.0216,   3.2057]],\n",
      "\n",
      "        [[ -6.2673,  -4.5597,  -8.3670,  -6.6593],\n",
      "         [-17.9165, -18.7240, -22.3558, -23.1634]],\n",
      "\n",
      "        [[ 30.3901,  30.3390,  25.7347,  25.6836],\n",
      "         [  0.0898,  -1.8824,   0.2104,  -1.7618]],\n",
      "\n",
      "        [[  4.2229,   4.1832,  -3.1167,  -3.1564],\n",
      "         [  4.3304,   2.2808,   4.4723,   2.4228]],\n",
      "\n",
      "        [[ -9.4934,  -7.6952, -11.3838,  -9.5856],\n",
      "         [-25.3127, -26.0874, -29.6995, -30.4743]],\n",
      "\n",
      "        [[ 24.7453,  26.5003,  24.7424,  26.4974],\n",
      "         [-10.7608, -10.7619, -15.1095, -15.1107]],\n",
      "\n",
      "        [[ 27.3445,  28.8775,  27.3419,  28.8749],\n",
      "         [-11.0744, -11.0753, -15.1541, -15.1551]],\n",
      "\n",
      "        [[  8.7154,   8.6399,  14.3760,  14.3006],\n",
      "         [-22.9773, -20.7757, -22.7833, -20.5817]],\n",
      "\n",
      "        [[ 11.9166,  11.5350,  11.2523,  10.8706],\n",
      "         [-17.0752, -17.6869, -16.6607, -17.2724]],\n",
      "\n",
      "        [[-29.3776, -29.4273, -33.7721, -33.8218],\n",
      "         [  1.0685,  -0.8488,   1.1823,  -0.7350]],\n",
      "\n",
      "        [[ -2.9558,  -1.2014,  -4.3349,  -2.5804],\n",
      "         [-11.0711, -11.6858, -15.0063, -15.6210]],\n",
      "\n",
      "        [[  4.8736,   4.7224,   8.8586,   8.7074],\n",
      "         [-28.4847, -26.7433, -28.1387, -26.3973]],\n",
      "\n",
      "        [[  9.4329,   8.9490,   8.8525,   8.3686],\n",
      "         [-16.5038, -17.0383, -15.9783, -16.5128]],\n",
      "\n",
      "        [[-36.5297, -36.4953, -31.8226, -31.7882],\n",
      "         [ 19.0607,  20.8773,  18.9717,  20.7883]],\n",
      "\n",
      "        [[ 10.7104,  10.3569,  10.0279,   9.6744],\n",
      "         [-17.1644, -17.7927, -16.7805, -17.4089]],\n",
      "\n",
      "        [[  3.9590,   3.8125,   8.2648,   8.1183],\n",
      "         [-31.0913, -29.4038, -30.7174, -29.0299]],\n",
      "\n",
      "        [[-17.0920, -17.1451, -21.7364, -21.7895],\n",
      "         [ 11.8254,   9.7772,  11.9457,   9.8975]],\n",
      "\n",
      "        [[ 12.1576,  11.7654,  11.4503,  11.0580],\n",
      "         [-16.2508, -16.8558, -15.7923, -16.3972]],\n",
      "\n",
      "        [[ 12.2202,  11.9136,  11.5114,  11.2049],\n",
      "         [-18.3377, -18.9903, -18.0049, -18.6574]],\n",
      "\n",
      "        [[  2.4302,   2.4623,   7.1605,   7.1925],\n",
      "         [-33.8018, -32.0332, -33.8874, -32.1188]],\n",
      "\n",
      "        [[ 21.8411,  23.6458,  21.9154,  23.7201],\n",
      "         [-10.7783, -10.7480, -15.2025, -15.1721]],\n",
      "\n",
      "        [[ 32.5881,  34.4061,  32.2197,  34.0376],\n",
      "         [-11.2908, -11.4512, -15.4643, -15.6248]],\n",
      "\n",
      "        [[-25.2761, -25.3809, -24.5480, -24.6529],\n",
      "         [ -6.4435,  -5.6252,  -6.3502,  -5.5319]],\n",
      "\n",
      "        [[-12.4534, -12.5002, -16.8819, -16.9287],\n",
      "         [  0.6645,  -1.1438,   0.7792,  -1.0291]],\n",
      "\n",
      "        [[  4.3897,   6.0290,   2.5718,   4.2111],\n",
      "         [ -9.7044, -10.4857, -13.5182, -14.2995]],\n",
      "\n",
      "        [[-21.2445, -21.2963, -26.0428, -26.0947],\n",
      "         [  8.6138,   6.6136,   8.7381,   6.7378]],\n",
      "\n",
      "        [[-12.4410, -12.4567, -16.8368, -16.8526],\n",
      "         [  4.6545,   2.7937,   4.6916,   2.8308]],\n",
      "\n",
      "        [[ 37.1737,  35.0390,  37.0965,  34.9618],\n",
      "         [-15.0865, -15.1224, -10.4894, -10.5253]],\n",
      "\n",
      "        [[ 23.0494,  23.0669,  29.6892,  29.7067],\n",
      "         [ 10.7553,  12.8191,  10.6992,  12.7630]],\n",
      "\n",
      "        [[ -3.7786,  -2.0867,  -5.7257,  -4.0338],\n",
      "         [-27.9601, -28.7957, -31.9023, -32.7379]]], dtype=torch.float64), 'category': tensor([2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2,\n",
      "        3, 2, 2, 2, 2, 2, 2, 2])},)\n",
      "torch.Size([63, 2, 4])\n",
      "torch.Size([32, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.3822, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[-12.7650, -12.8230, -12.0153, -12.0734],\n",
      "         [ 10.2031,  10.9612,  10.2598,  11.0179]],\n",
      "\n",
      "        [[-21.6597, -21.3072, -26.3958, -26.0433],\n",
      "         [ -1.5932,  -3.4555,  -2.4842,  -4.3465]]], dtype=torch.float64), 'category': tensor([3, 2])},)\n",
      "torch.Size([62, 2, 4])\n",
      "torch.Size([2, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.1967, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[-31.0637, -31.0876, -35.4593, -35.4832],\n",
      "         [  1.2918,  -0.6254,   1.3464,  -0.5709]],\n",
      "\n",
      "        [[ 27.0188,  26.9932,  22.3731,  22.3476],\n",
      "         [ 11.6438,   9.5956,  11.7015,   9.6532]],\n",
      "\n",
      "        [[-17.0498, -17.0273, -12.5032, -12.4807],\n",
      "         [ 18.9664,  20.7717,  18.9099,  20.7153]],\n",
      "\n",
      "        [[ 29.9926,  29.9455,  30.7254,  30.6783],\n",
      "         [ -6.4023,  -5.5789,  -6.3604,  -5.5369]],\n",
      "\n",
      "        [[ 35.1161,  36.9245,  33.2849,  35.0933],\n",
      "         [-25.3863, -26.1366, -29.7972, -30.5474]],\n",
      "\n",
      "        [[ 33.2030,  31.4094,  34.9728,  33.1792],\n",
      "         [-24.6812, -23.9012, -20.6147, -19.8347]],\n",
      "\n",
      "        [[  2.4568,   2.4339,  -1.9269,  -1.9498],\n",
      "         [  8.3111,   6.4738,   8.3655,   6.5282]],\n",
      "\n",
      "        [[-15.7616, -15.7848, -20.1573, -20.1804],\n",
      "         [  4.7903,   2.9300,   4.8449,   2.9846]],\n",
      "\n",
      "        [[ -1.4682,  -1.4946,  -8.8085,  -8.8350],\n",
      "         [  4.6047,   2.5554,   4.6989,   2.6497]],\n",
      "\n",
      "        [[-34.2398, -34.2298, -39.0397, -39.0297],\n",
      "         [  8.4043,   6.4039,   8.3801,   6.3798]],\n",
      "\n",
      "        [[ 34.9954,  35.0337,  30.9094,  30.9478],\n",
      "         [ 11.5717,   9.8636,  11.4798,   9.7717]],\n",
      "\n",
      "        [[ 25.3516,  25.3620,  30.0565,  30.0669],\n",
      "         [ 22.0924,  23.9898,  22.0668,  23.9642]],\n",
      "\n",
      "        [[ 37.3469,  37.3202,  32.6463,  32.6196],\n",
      "         [  8.0430,   5.9008,   8.1013,   5.9591]],\n",
      "\n",
      "        [[ 38.2424,  39.9607,  36.2027,  37.9210],\n",
      "         [-17.9523, -18.7366, -22.4185, -23.2027]],\n",
      "\n",
      "        [[  3.3475,   3.3910,   8.0632,   8.1067],\n",
      "         [ 21.9891,  23.8860,  21.8812,  23.7781]],\n",
      "\n",
      "        [[ -9.5914,  -9.6139, -14.0211, -14.0436],\n",
      "         [  1.0072,  -0.8011,   1.0622,  -0.7462]],\n",
      "\n",
      "        [[ 19.1130,  19.0906,  14.8863,  14.8639],\n",
      "         [  8.2521,   6.4528,   8.3046,   6.5053]],\n",
      "\n",
      "        [[-29.9632, -29.9858, -34.3588, -34.3814],\n",
      "         [  4.8942,   3.0789,   4.9488,   3.1335]],\n",
      "\n",
      "        [[ 22.5585,  22.5339,  17.9018,  17.8773],\n",
      "         [  0.9987,  -0.9735,   1.0565,  -0.9157]]], dtype=torch.float64), 'category': tensor([2, 2, 2, 3, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])},)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([61, 2, 4])\n",
      "torch.Size([19, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.3287, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7132054b3847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPOCH: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkobe_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkobe_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-e1de6089471c>\u001b[0m in \u001b[0;36mtrain_yolo\u001b[0;34m(data_loader, kobe_model, kobe_optimizer, lambd)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mkobe_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(\"EPOCH: {}\".format(epoch))\n",
    "    train_yolo(trainloader, kobe_model, kobe_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to debug architecture\n",
    "z = torch.rand(10 , 5 * 15 * 15)\n",
    "z = ReshapeLayer2d(5, 15)(z)\n",
    "z = nn.Conv2d(5, 5, kernel_size=3, stride = 1)(z)\n",
    "\n",
    "z = nn.MaxPool2d(kernel_size=2, stride = 1)(z)\n",
    "z = nn.Conv2d(5, 5, kernel_size=3, stride = 1)(z)\n",
    "\n",
    "z = nn.MaxPool2d(kernel_size = 2, stride = 1)(z)\n",
    "z = ReshapeLayer1d(405)(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Model Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 2, 4])\n",
      "torch.Size([21, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2480)\n",
      "torch.Size([1, 21, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([24, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2480)\n",
      "torch.Size([1, 24, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([25, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2480)\n",
      "torch.Size([1, 25, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([25, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2481)\n",
      "torch.Size([1, 25, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([23, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2481)\n",
      "torch.Size([1, 23, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([25, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2482)\n",
      "torch.Size([1, 25, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([23, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2482)\n",
      "torch.Size([1, 23, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([23, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2482)\n",
      "torch.Size([1, 23, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([22, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2482)\n",
      "torch.Size([1, 22, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([21, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2483)\n",
      "torch.Size([1, 21, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([21, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2483)\n",
      "torch.Size([1, 21, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([19, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2484)\n",
      "torch.Size([1, 19, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([18, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2485)\n",
      "torch.Size([1, 18, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([19, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2486)\n",
      "torch.Size([1, 19, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([19, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2487)\n",
      "torch.Size([1, 19, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([19, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2489)\n",
      "torch.Size([1, 19, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([19, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2490)\n",
      "torch.Size([1, 19, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([21, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2491)\n",
      "torch.Size([1, 21, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([19, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2492)\n",
      "torch.Size([1, 19, 2, 4])\n",
      "torch.Size([40, 2, 4])\n",
      "torch.Size([20, 2, 4])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/shapely/speedups/_speedups.pyx\u001b[0m in \u001b[0;36mshapely.speedups._speedups.geos_linearring_from_py\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute '__array_interface__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-46fd19bfa03f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpredicted_bounding_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_yolo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mats_bounding_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_ats_bounding_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_bounding_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bounding_box'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ats bounding box'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mats_bounding_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/DeepLearning/hw/final_proj/deep-learning-project/helper.py\u001b[0m in \u001b[0;36mcompute_ats_bounding_boxes\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_boxes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcondition_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0miou_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_iou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0miou_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miou_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/DeepLearning/hw/final_proj/deep-learning-project/helper.py\u001b[0m in \u001b[0;36mcompute_iou\u001b[0;34m(box1, box2)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_iou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvex_hull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvex_hull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marea\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marea\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/shapely/geometry/polygon.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shell, holes)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshell\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeos_polygon_from_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_geom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/shapely/geometry/polygon.py\u001b[0m in \u001b[0;36mgeos_polygon_from_py\u001b[0;34m(shell, holes)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshell\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeos_linearring_from_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/shapely/speedups/_speedups.pyx\u001b[0m in \u001b[0;36mshapely.speedups._speedups.geos_linearring_from_py\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_transform_task1(): \n",
    "    return torchvision.transforms.ToTensor()\n",
    "# For road map task\n",
    "def get_transform_task2(): \n",
    "    return torchvision.transforms.ToTensor()\n",
    "\n",
    "labeled_trainset_task1 = LabeledDataset(\n",
    "    image_folder=image_folder,\n",
    "    annotation_file=annotation_csv,\n",
    "    scene_index=labeled_scene_index,\n",
    "    transform=get_transform_task1(),\n",
    "    extra_info=False\n",
    "    )\n",
    "dataloader_task1 = torch.utils.data.DataLoader(\n",
    "    labeled_trainset_task1,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    "    )\n",
    "\n",
    "\n",
    "total = 0\n",
    "total_ats_bounding_boxes = 0\n",
    "total_ts_road_map = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataloader_task1):\n",
    "        total += 1\n",
    "        sample, target, road_image = data\n",
    "        output_yolo, _, output_rm, _ = kobe_model(sample)\n",
    "        \n",
    "        predicted_bounding_boxes = output_yolo[0].cpu()\n",
    "        ats_bounding_boxes = compute_ats_bounding_boxes(predicted_bounding_boxes, target['bounding_box'][0])\n",
    "        print('ats bounding box')\n",
    "        print(ats_bounding_boxes)\n",
    "        \n",
    "        ts_road_map = compute_ts_road_map(output_rm, road_image)\n",
    "        print('ts roadmap')\n",
    "        print(ts_road_map)\n",
    "        \n",
    "        \n",
    "        print(target['bounding_box'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
