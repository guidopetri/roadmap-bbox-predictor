{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to keep the code here. And then export into a .py file.\n",
    "\n",
    "Do not try to modify the .py file directly until this is notebook is gone from each branch.\n",
    "\n",
    "Remember to clear output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils and calculating loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming Coordinates\n",
    "\n",
    "Define the given coordinates as world coordinates\n",
    "\n",
    "Define normalized from upper left bound of world coordinates (translate to there, rotate, and normalize) as our normalized image coordinates (or image coordinates for short).\n",
    "\n",
    "Always facing right in world coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 40\n",
    "WIDTH = 2 * 40\n",
    "HEIGHT = 2 * 40\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "#cuda = torch.cuda.is_available()\n",
    "cuda = False\n",
    "\n",
    "device = 'cuda:0' if cuda else 'cpu'\n",
    "FloatTensor = FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "## input is the \n",
    "# want output that is\n",
    "# [batch_index, class_index, x_center, y_center, width, height, tx1, tx2, tx3, tx4, ty1, ty2, ty3, ty4]\n",
    "# where the tXN ranges from -1 to 1 and is the location of that coordinate in terms of +- w/2 or h/2\n",
    "def transform_target(in_target):\n",
    "    \n",
    "    out_target = []\n",
    "    \n",
    "    for tgt_index in range(len(in_target)):\n",
    "        \n",
    "        #how many boxes for these target\n",
    "        nbox = in_target[tgt_index]['bounding_box'].shape[0]\n",
    "        individual_target = FloatTensor(nbox, 14).fill_(0)\n",
    "        \n",
    "        # CONVERT ALL THE BOUNDING BOXES for an individual sample at once\n",
    "        \n",
    "        bbox = in_target[tgt_index]['bounding_box'].to(device)\n",
    "        translation = FloatTensor(bbox.shape[0], bbox.shape[1], bbox.shape[2])\n",
    "        translation[:, 0, :].fill_(-40)\n",
    "        translation[:, 1, :].fill_(40)\n",
    "\n",
    "        # translate to uppert left\n",
    "        box = bbox - translation\n",
    "        # reflect y\n",
    "        box[:, 1, :].mul_(-1)\n",
    "\n",
    "        x_min = box[:, 0].min(dim = 1)[0]\n",
    "        y_min = box[:, 1].min(dim = 1)[0]\n",
    "        x_max = box[:, 0].max(dim = 1)[0]\n",
    "        y_max = box[:, 1].max(dim = 1)[0]\n",
    "\n",
    "        x_center = ((x_min + x_max) / 2)\n",
    "        y_center = ((y_min + y_max) / 2)\n",
    "        width = (x_max - x_min)\n",
    "        height = (y_max - y_min)\n",
    "\n",
    "        # already normalized\n",
    "        tx = (box [:, 0, :] - x_center.view(-1, 1)) / (width.view(-1, 1) / 2)\n",
    "        ty = (box [:, 1, :] - y_center.view(-1, 1)) / (height.view(-1, 1) / 2)\n",
    "\n",
    "        x_center_n = x_center / WIDTH\n",
    "        y_center_n = y_center / HEIGHT\n",
    "        width_n = width / WIDTH\n",
    "        height_n = height / HEIGHT\n",
    "\n",
    "        individual_target[:, 2] = x_center_n\n",
    "        individual_target[:, 3] = y_center_n\n",
    "        individual_target[:, 4] = width_n\n",
    "        individual_target[:, 5] = height_n\n",
    "        \n",
    "        individual_target[:, 6:10] = tx\n",
    "        individual_target[:, 10:14] = ty\n",
    "        for box_index in range(nbox):\n",
    "            \n",
    "            \n",
    "            category = in_target[tgt_index]['category'][box_index]\n",
    "            \n",
    "            # from which sample in the batch\n",
    "            individual_target[box_index, 0] = tgt_index\n",
    "            # class\n",
    "            individual_target[box_index, 1] = category\n",
    "            \n",
    "        \n",
    "        out_target.append(individual_target)\n",
    "        \n",
    "    return torch.cat(out_target, dim = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load presaved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works by side effects\n",
    "def load_pretask_weight_from_model(model, presaved_encoder):\n",
    "    model.encoder.load_state_dict(presaved_encoder.state_dict())\n",
    "    \n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you want Initialize Our Model with encoder weights from an existing pretask encoder in memory\n",
    "def initialize_model_for_training(presaved_encoder):\n",
    "    model = KobeModel()\n",
    "    load_pretask_weight_from_model(model, presaved_encoder)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you want Initialize Our Model with encoder weights from a file\n",
    "def initialize_model_for_training_file(presaved_encoder_file):\n",
    "    presaved_encoder = PreTaskEncoder()\n",
    "    presaved_encoder.load_state_dict(torch.load(presaved_encoder_file))\n",
    "    presaved_encoder.eval()\n",
    "\n",
    "    \n",
    "    return initialize_model_for_training(presaved_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting predictions to the format for competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to calculate bounding boxes and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/master/utils/utils.py\n",
    "\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    y = x.new(x.shape)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
    "    return y\n",
    "\n",
    "\n",
    "def bbox_wh_iou(wh1, wh2):\n",
    "    wh2 = wh2.t()\n",
    "    w1, h1 = wh1[0], wh1[1]\n",
    "    w2, h2 = wh2[0], wh2[1]\n",
    "    inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n",
    "    union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n",
    "    return inter_area / union_area\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes\n",
    "    \"\"\"\n",
    "    if not x1y1x2y2:\n",
    "        # Transform from center and width to exact coordinates\n",
    "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "    else:\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "\n",
    "    # get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "    # Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n",
    "        inter_rect_y2 - inter_rect_y1 + 1, min=0\n",
    "    )\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4):\n",
    "    \"\"\"\n",
    "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
    "    Non-Maximum Suppression to further filter detections.\n",
    "    Returns detections with shape:\n",
    "        (x1, y1, x2, y2, x1, x2, x3, x4, y1, y2, y3, y4, object_conf, class_score, class_pred)\n",
    "\n",
    "        \n",
    "        # where the x1, ..., x4 and y1, ... y4 are stll from -1 to 1\n",
    "        # first x1, y1, x2, y2 are in the grid coordinates and need to be converted back\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "    prediction[..., :4] = xywh2xyxy(prediction[..., :4])\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for image_i, image_pred in enumerate(prediction):\n",
    "        # Filter out confidence scores below threshold\n",
    "        image_pred = image_pred[image_pred[:, 12] >= conf_thres]\n",
    "        # If none are remaining => process next image\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        # Object confidence times class confidence\n",
    "        score = image_pred[:, 12] * image_pred[:, 13:].max(1)[0]\n",
    "        # Sort by it\n",
    "        image_pred = image_pred[(-score).argsort()]\n",
    "        class_confs, class_preds = image_pred[:, 13:].max(1, keepdim=True)\n",
    "        detections = torch.cat((image_pred[:, :13], class_confs.float(), class_preds.float()), 1)\n",
    "        # Perform non-maximum suppression\n",
    "        keep_boxes = []\n",
    "        #print(\"DETECTIONS BEFORE NMS\")\n",
    "        #print(detections.shape)\n",
    "        while detections.size(0):\n",
    "            large_overlap = bbox_iou(detections[0, :4].unsqueeze(0), detections[:, :4]) > nms_thres\n",
    "            label_match = detections[0, -1] == detections[:, -1]\n",
    "            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n",
    "            invalid = large_overlap & label_match\n",
    "            weights = detections[invalid, 12:13]\n",
    "            # Merge overlapping bboxes by order of confidence\n",
    "            detections[0, :12] = (weights * detections[invalid, :12]).sum(0) / weights.sum()\n",
    "            keep_boxes += [detections[0]]\n",
    "            detections = detections[~invalid]\n",
    "        if keep_boxes:\n",
    "            output[image_i] = torch.stack(keep_boxes)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n",
    "\n",
    "    BoolTensor = torch.cuda.BoolTensor if pred_boxes.is_cuda else torch.BoolTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n",
    "\n",
    "    nB = pred_boxes.size(0)\n",
    "    nA = pred_boxes.size(1)\n",
    "    nC = pred_cls.size(-1)\n",
    "    nG = pred_boxes.size(2)\n",
    "\n",
    "    # Output tensors\n",
    "    obj_mask = BoolTensor(nB, nA, nG, nG).fill_(0)\n",
    "    noobj_mask = BoolTensor(nB, nA, nG, nG).fill_(1)\n",
    "    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    \n",
    "    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n",
    "    \n",
    "    ### predict additional coordinates for the center within the anchor box\n",
    "    ##### THIS IS OUR ADDITION\n",
    "    ##### RANGE OF THESE VALUES TO BE DETERMINED\n",
    "    tx1 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx2 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx3 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx4 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty1 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty2 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty3 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty4 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "\n",
    "    # Convert to position relative to box\n",
    "    target_boxes = target[:, 2:6] * nG\n",
    "    gxy = target_boxes[:, :2]\n",
    "    gwh = target_boxes[:, 2:]\n",
    "\n",
    "    \n",
    "    \n",
    "    # Get anchors with best iou\n",
    "    ious = torch.stack([bbox_wh_iou(anchor, gwh) for anchor in anchors])\n",
    "    best_ious, best_n = ious.max(0)\n",
    "    # Separate target values\n",
    "    b, target_labels = target[:, :2].long().t()\n",
    "    gx, gy = gxy.t()\n",
    "    gw, gh = gwh.t()\n",
    "    gi, gj = gxy.long().t()\n",
    "    # Set masks\n",
    "    obj_mask[b, best_n, gj, gi] = 1\n",
    "    noobj_mask[b, best_n, gj, gi] = 0\n",
    "\n",
    "    # Set noobj mask to zero where iou exceeds ignore threshold\n",
    "    for i, anchor_ious in enumerate(ious.t()):\n",
    "        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n",
    "\n",
    "    # Coordinates\n",
    "    tx[b, best_n, gj, gi] = gx - gx.floor()\n",
    "    ty[b, best_n, gj, gi] = gy - gy.floor()\n",
    "    \n",
    "    \n",
    "    tx1[b, best_n, gj, gi] = target[:, 6]\n",
    "    tx2[b, best_n, gj, gi] = target[:, 7]\n",
    "    tx3[b, best_n, gj, gi] = target[:, 8]\n",
    "    tx4[b, best_n, gj, gi] = target[:, 9]\n",
    "    \n",
    "    ty1[b, best_n, gj, gi] = target[:, 10]\n",
    "    ty2[b, best_n, gj, gi] = target[:, 11]\n",
    "    ty3[b, best_n, gj, gi] = target[:, 12]\n",
    "    ty4[b, best_n, gj, gi] = target[:, 13]\n",
    "    \n",
    "    # Width and height\n",
    "    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n",
    "    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1] + 1e-16)\n",
    "    # One-hot encoding of label\n",
    "    tcls[b, best_n, gj, gi, target_labels] = 1\n",
    "    # Compute label correctness and iou at best anchor\n",
    "    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n",
    "    # iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=False)\n",
    "\n",
    "    tconf = obj_mask.float()\n",
    "    return (class_mask, obj_mask, noobj_mask, \n",
    "            tx, ty, tw, th, \n",
    "            tx1, tx2, tx3, tx4, \n",
    "            ty1, ty2, ty3, ty4, \n",
    "            tcls, tconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoadMapLoss(pred_rm, target_rm):\n",
    "    bce_loss = nn.BCELoss()\n",
    "\n",
    "    return bce_loss(pred_rm, target_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_joint_loss(yolo_loss, rm_loss, lambd):\n",
    "    return yolo_loss + lambd * rm_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop and test loops\n",
    "Not necessarily using data loader.\n",
    "\n",
    "Assuming targets are already pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo(data_loader, kobe_model, kobe_optimizer, lambd = 0.5):\n",
    "    kobe_model.train()\n",
    "    train_loss = 0 \n",
    "        \n",
    "    for sample, target, road_image, extra in trainloader:\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        target = transform_target(target).to(device)\n",
    "        road_image = torch.stack(road_image)\n",
    "\n",
    "        kobe_optimizer.zero_grad()\n",
    "\n",
    "        output_yolo, yolo_loss = kobe_model(sample, yolo_target = target)\n",
    "\n",
    "        # SHOULD GET LOWER OVER EPOCHS\n",
    "        #print(\"PRINTING output yolo after nms\")\n",
    "        #if output_yolo[0] is not None:\n",
    "        #    print(output_yolo[0].shape)\n",
    "        #else:\n",
    "        #    print(\"It was none!\")\n",
    "        #    print(output_yolo)\n",
    "        #rm_loss = RoadMapLoss(outputs_rm, batch_rms)\n",
    "\n",
    "        #loss = total_joint_loss(yolo_loss, rm_loss, lambd)\n",
    "\n",
    "        train_loss += yolo_loss.item()\n",
    "        yolo_loss.backward()\n",
    "\n",
    "        kobe_optimizer.step()\n",
    "        \n",
    "    print(\"TRAIN LOSS: {}\".format(train_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-defined anchors. Should honestly come from KMeans on detection boxes but let's see how this does before going complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the code uses only the last 3 anchors so let ssee what this does\n",
    "# width, height\n",
    "\n",
    "#### anchors are supposed to be in terms of number of grid points it would take\n",
    "#### in a 416x416 image (assuming using default of YOLO)\n",
    "#### we are given 80x80\n",
    "### we match in the 416x416 space though\n",
    "### so scale what esteban gave by 5 (5.2 actualy but wtv)\n",
    "anchors = [(5,5), (25, 12), (12, 25), (100, 25), (50, 12), (40, 60)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our YoloLayer for task of object localization\n",
    "\n",
    "Ignoring orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_HIDDEN = 26718\n",
    "class PreTaskEncoder(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(PreTaskEncoder, self).__init__()\n",
    "        # number of different kernels to use\n",
    "        self.n_features = n_features\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=n_features,\n",
    "                               kernel_size=5,\n",
    "                               )\n",
    "        self.conv2 = nn.Conv2d(n_features,\n",
    "                               n_features,\n",
    "                               kernel_size=5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # return an array shape\n",
    "        x = x.view(-1, ENCODER_HIDDEN)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer2d(nn.Module):\n",
    "    def __init__(self, channels, dim):\n",
    "        super(ReshapeLayer2d, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], self.channels, self.dim, self.dim)\n",
    "    \n",
    "class ReshapeLayer1d(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ReshapeLayer1d, self).__init__()\n",
    "        self.features = features\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], self.features)\n",
    "\n",
    "class YoloDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, anchors, num_classes, img_dim=416):\n",
    "        \n",
    "        super(YoloDecoder, self).__init__()\n",
    "        \n",
    "        self.anchors = anchors\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_thres = 0.5\n",
    "        \n",
    "        self.obj_scale = 1\n",
    "        # 100 originally here\n",
    "        # 5 gives about 60 objects detected per image\n",
    "        # 20 gives about 10 per image\n",
    "        # 10 gives about 45\n",
    "        self.noobj_scale = 12\n",
    "        \n",
    "        self.img_dim = img_dim\n",
    "        self.grid_size = 8\n",
    "        \n",
    "        # takes in dense output from encoder or shared decoder and puts into an\n",
    "        # image of dim img_dim\n",
    "\n",
    "        self.m = nn.Sequential(\n",
    "            nn.Linear(6 * ENCODER_HIDDEN, 5 * 15 * 15),\n",
    "            nn.ReLU(),\n",
    "            ReshapeLayer2d(5, 15),\n",
    "            nn.Conv2d(5, 5, kernel_size=3, stride = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride = 1),\n",
    "            nn.Conv2d(5, 5, kernel_size=3, stride = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 1), \n",
    "            ReshapeLayer1d(405),\n",
    "            nn.Linear(405, self.num_anchors * (self.num_classes + 5 + 8) * self.grid_size * self.grid_size)\n",
    "        )\n",
    "        \n",
    "        self.compute_grid_offsets(self.grid_size, cuda)\n",
    "        \n",
    "    def compute_grid_offsets(self, grid_size, cuda=True):\n",
    "        self.grid_size = grid_size\n",
    "        g = self.grid_size\n",
    "        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "        self.stride = self.img_dim / self.grid_size\n",
    "        # Calculate offsets for each grid\n",
    "        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n",
    "        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n",
    "        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])\n",
    "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
    "        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
    "        \n",
    "    def forward(self, x, targets = None):\n",
    "        # Tensors for cuda support\n",
    "        # Tensors for cuda support\n",
    "        x = self.m(x)\n",
    "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
    "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
    "        BoolTensor = torch.cuda.BoolTensor if x.is_cuda else torch.BoolTensor\n",
    "\n",
    "        num_samples = x.shape[0]\n",
    "\n",
    "        prediction = (\n",
    "            x.view(num_samples, self.num_anchors, self.num_classes + 5 + 8, self.grid_size, self.grid_size)\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "            .contiguous()\n",
    "        )\n",
    "\n",
    "        # Get outputs\n",
    "        xc = torch.sigmoid(prediction[..., 0])  # Center x\n",
    "        yc = torch.sigmoid(prediction[..., 1])  # Center y\n",
    "        w = prediction[..., 2]  # Width\n",
    "        h = prediction[..., 3]  # Height\n",
    "        \n",
    "        #### get x1, x2, x3, x4, y1, y2, y3, y4\n",
    "        \n",
    "        x1 = torch.tanh(prediction[..., 4])\n",
    "        x2 = torch.tanh(prediction[..., 5])\n",
    "        x3 = torch.tanh(prediction[..., 6])\n",
    "        x4 = torch.tanh(prediction[..., 7])\n",
    "        y1 = torch.tanh(prediction[..., 8])\n",
    "        y2 = torch.tanh(prediction[..., 9])\n",
    "        y3 = torch.tanh(prediction[..., 10])\n",
    "        y4 = torch.tanh(prediction[..., 11])\n",
    "        \n",
    "        pred_conf = torch.sigmoid(prediction[..., 12])  # Conf\n",
    "        pred_cls = torch.sigmoid(prediction[..., 13:])  # Cls pred.\n",
    "\n",
    "        # Add offset and scale with anchors\n",
    "        # mulitply by stride to convert from grid to image coordinates (of YOLO img of 416)\n",
    "        pred_boxes = FloatTensor(prediction[..., :12].shape)\n",
    "        \n",
    "        pred_boxes[..., 0] = (xc.data + self.grid_x) * self.stride\n",
    "        pred_boxes[..., 1] = (yc.data + self.grid_y) * self.stride\n",
    "        \n",
    "        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w * self.stride\n",
    "        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h * self.stride\n",
    "        \n",
    "        pred_boxes[..., 4] = x1\n",
    "        pred_boxes[..., 5] = x2\n",
    "        pred_boxes[..., 6] = x3\n",
    "        pred_boxes[..., 7] = x4\n",
    "        \n",
    "        pred_boxes[..., 8] = y1\n",
    "        pred_boxes[..., 9] = y2\n",
    "        pred_boxes[..., 10] = y3\n",
    "        pred_boxes[..., 11] = y4\n",
    "        ### need to figure out what to do with all the x1, x2, x3, x4 etc\n",
    "\n",
    "        # ORIGINAL OUTPUTS IN TERMS OF GRID SIZES, DO NOT FORGET TO CONVERT BACK\n",
    "        output = torch.cat(\n",
    "            (\n",
    "                pred_boxes.view(num_samples, -1, 12),\n",
    "                pred_conf.view(num_samples, -1, 1),\n",
    "                pred_cls.view(num_samples, -1, self.num_classes),\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "        if targets is None:\n",
    "            return output, 0\n",
    "        else:\n",
    "            mse_loss = nn.MSELoss()\n",
    "            bce_loss = nn.BCELoss()\n",
    "    \n",
    "            class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tx1, tx2, tx3, tx4, ty1, ty2, ty3, ty4, tcls, tconf = build_targets(\n",
    "                        pred_boxes=pred_boxes,\n",
    "                        pred_cls=pred_cls,\n",
    "                        target=targets,\n",
    "                        anchors=self.scaled_anchors,\n",
    "                        ignore_thres=self.ignore_thres,\n",
    "                    )\n",
    "\n",
    "            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n",
    "            loss_xc = mse_loss(xc[obj_mask], tx[obj_mask])\n",
    "            loss_yc = mse_loss(yc[obj_mask], ty[obj_mask])\n",
    "        \n",
    "\n",
    "            loss_x1 = mse_loss(x1[obj_mask], tx1[obj_mask])\n",
    "            loss_x2 = mse_loss(x2[obj_mask], tx2[obj_mask])\n",
    "            loss_x3 = mse_loss(x3[obj_mask], tx3[obj_mask])\n",
    "            loss_x4 = mse_loss(x4[obj_mask], tx4[obj_mask])\n",
    "            \n",
    "            loss_y1 = mse_loss(y1[obj_mask], ty1[obj_mask])\n",
    "            loss_y2 = mse_loss(y2[obj_mask], ty2[obj_mask])\n",
    "            loss_y3 = mse_loss(y3[obj_mask], ty3[obj_mask])\n",
    "            loss_y4 = mse_loss(y4[obj_mask], ty4[obj_mask])\n",
    "            \n",
    "            loss_w = mse_loss(w[obj_mask], tw[obj_mask])\n",
    "            loss_h = mse_loss(h[obj_mask], th[obj_mask])\n",
    "\n",
    "            loss_conf_obj = bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n",
    "            loss_conf_noobj = bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n",
    "            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n",
    "            loss_cls = bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n",
    "            \n",
    "            #print(\"Losses xc {}, yc {}, x1 {}, x2 {}, x3 {}, x4 {}, y1 {}, y2 {}, y3 {}, y4 {}, \\\n",
    "            #       w {}, h {}, conf {}, cls {}\".format(loss_xc.item(), loss_yc.item(), loss_x1.item(),\n",
    "            #                                          loss_x2.item(), loss_x3.item(), loss_x4.item(),\n",
    "            #                                          loss_y1.item(), loss_y2.item(), loss_y3.item(),\n",
    "            #                                          loss_y4.item(), loss_w.item(), loss_h.item(),\n",
    "            #                                          loss_conf.item(), loss_cls.item()))\n",
    "            total_loss = 10*(loss_xc + loss_yc + \\\n",
    "                         loss_x1 + loss_x2 + loss_x3 + loss_x4 + \\\n",
    "                         loss_y1 + loss_y2 + loss_y3 + loss_y4 + \\\n",
    "                         loss_w + loss_h) + loss_conf + 10*loss_cls\n",
    "\n",
    "            return output, total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Model does that does both tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KobeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, encoder_features, yolo_dim, rm_dim):\n",
    "        super(KobeModel, self).__init__()\n",
    "        \n",
    "        self.yolo_dim = yolo_dim\n",
    "        \n",
    "        self.encoder = PreTaskEncoder(encoder_features)\n",
    "        \n",
    "        \n",
    "        #self.shared_decoder = nn.Sequential()\n",
    "        \n",
    "        self.yolo_decoder = YoloDecoder(anchors, num_classes, img_dim=yolo_dim)\n",
    "        \n",
    "        #self.rm_decoder = RmDecoder(rm_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \n",
    "        # get all the representations laid out like this\n",
    "        x = torch.cat([self.encoder(x[:, i, :]) for i in range(6)], dim = 1)\n",
    "            \n",
    "            \n",
    "        #convert from dense representation from encoder into an image\n",
    "        # x.view(...)\n",
    "        \n",
    "        #x = self.shared_decoder(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, yolo_target = None, rm_target = None ):\n",
    "        encoding = self.encode(x)\n",
    "        \n",
    "        # output_1 first 12 corresponds to the bounding boxes\n",
    "        # output_1 13 corresponds to confidences\n",
    "        # output_1's and the rest corresponds to the class labels\n",
    "        # oputput_1[3] is them spread out\n",
    "        output_1, yolo_loss = self.get_bounding_boxes(x, encoding = encoding, target = yolo_target)\n",
    "        # roadmap decoder\n",
    "        #output_2, rm_loss = self.rm_decoder(x, encoding, target = rm_target)\n",
    "        \n",
    "        # output1 is not in the context of our bounding boxes\n",
    "        #return output_1, output_2, yolo_loss, rm_loss\n",
    "        return output_1, yolo_loss\n",
    "    \n",
    "    # for easy use for competition\n",
    "    # in competition, encoding is None\n",
    "    def get_bounding_boxes(self, x, encoding = None, target = None):\n",
    "        if encoding is None:\n",
    "            encoding = self.encode(x)\n",
    "        \n",
    "        outputs, yolo_loss = self.yolo_decoder(encoding, targets=target)\n",
    "        \n",
    "        outputs = non_max_suppression(outputs)\n",
    "        \n",
    "        boxes = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            # let's convert it back to center_x, center_y, width and height\n",
    "            if output is None:\n",
    "                boxes.append(None)\n",
    "                continue\n",
    "            \n",
    "            better_coordinates = FloatTensor(len(output), 2, 4)\n",
    "            translation = FloatTensor(len(output), 2, 4)\n",
    "            translation[:, 0, :].fill_(-40)\n",
    "            translation[:, 1, :].fill_(40)\n",
    "\n",
    "            center_x = (output[:, 0] + output[:, 2]) / 2 / 416 * 80\n",
    "            center_y = (output[:, 1] + output[:, 3]) / 2 / 416 * 80\n",
    "            width = output[:, 2] - output[:,0] / 416 * 80\n",
    "            height = output[:, 3] - output[:,1] / 416 * 80\n",
    "            \n",
    "            x1 = center_x + output[:, 4] * width/2\n",
    "            x2 = center_x + output[:, 5] * width/2\n",
    "            x3 = center_x + output[:, 6] * width/2\n",
    "            x4 = center_x + output[:, 7] * width/2\n",
    "            y1 = center_y + output[:, 8] * height/2\n",
    "            y2 = center_y + output[:, 9] * height/2\n",
    "            y3 = center_y + output[:, 10] * height/2\n",
    "            y4 = center_y + output[:, 11] * height/2  \n",
    "            \n",
    "            better_coordinates[:, 0, 0] = x1\n",
    "            better_coordinates[:, 0, 1] = x2\n",
    "            better_coordinates[:, 0, 2] = x3\n",
    "            better_coordinates[:, 0, 3] = x4\n",
    "            \n",
    "            better_coordinates[:, 1, 0] = y1\n",
    "            better_coordinates[:, 1, 1] = y2\n",
    "            better_coordinates[:, 1, 2] = y3\n",
    "            better_coordinates[:, 1, 3] = y4\n",
    "            \n",
    "            better_coordinates[:, 1, :].mul_(-1)\n",
    "            # shift back!\n",
    "            better_coordinates += translation\n",
    "            \n",
    "            boxes.append(better_coordinates)\n",
    "        return tuple(boxes), yolo_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kobe_model = KobeModel(10, 6, 416, 800)\n",
    "kobe_model.to(device)\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "\n",
    "kobe_optimizer = torch.optim.Adam(kobe_model.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'data'\n",
    "annotation_csv = 'data/annotation.csv'\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for sample, target, road_image, extra in trainloader:\n",
    "    sample = torch.stack(sample).to(device)\n",
    "    target = transform_target(target).to(device)\n",
    "    road_image = torch.stack(road_image)\n",
    "    \n",
    "    output_yolo, yolo_loss = kobe_model(sample, yolo_target = target)\n",
    "    \n",
    "    # SHOULD GET LOWER OVER EPOCHS\n",
    "    print(output_yolo[0].shape)\n",
    "    yolo_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "DETECTIONS BEFORE NMS\n",
      "torch.Size([796, 15])\n",
      "DETECTIONS BEFORE NMS\n",
      "torch.Size([796, 15])\n",
      "PRINTING output yolo after nms\n",
      "torch.Size([780, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(\"EPOCH: {}\".format(epoch))\n",
    "    train_yolo(trainloader, kobe_model, kobe_optimizer, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to debug architecture\n",
    "z = torch.rand(10 , 5 * 15 * 15)\n",
    "z = ReshapeLayer2d(5, 15)(z)\n",
    "z = nn.Conv2d(5, 5, kernel_size=3, stride = 1)(z)\n",
    "\n",
    "z = nn.MaxPool2d(kernel_size=2, stride = 1)(z)\n",
    "z = nn.Conv2d(5, 5, kernel_size=3, stride = 1)(z)\n",
    "\n",
    "z = nn.MaxPool2d(kernel_size = 2, stride = 1)(z)\n",
    "z = ReshapeLayer1d(405)(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
