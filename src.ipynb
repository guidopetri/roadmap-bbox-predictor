{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to keep the code here. And then export into a .py file.\n",
    "\n",
    "Do not try to modify the .py file directly until this is notebook is gone from each branch.\n",
    "\n",
    "Remember to clear output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for formulating final predictions\n",
    "\n",
    "from torchvision.ops import nms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils and calculating loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load presaved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works by side effects\n",
    "def load_pretask_weight_from_model(model, presaved_encoder):\n",
    "    model.encoder.load_state_dict(presaved_encoder.state_dict())\n",
    "    \n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you want Initialize Our Model with encoder weights from an existing pretask encoder in memory\n",
    "def initialize_model_for_training(presaved_encoder):\n",
    "    model = KobeModel()\n",
    "    load_pretask_weight_from_model(model, presaved_encoder)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you want Initialize Our Model with encoder weights from a file\n",
    "def initialize_model_for_training_file(presaved_encoder_file):\n",
    "    presaved_encoder = PreTaskEncoder()\n",
    "    presaved_encoder.load_state_dict(torch.load(presaved_encoder_file))\n",
    "    presaved_encoder.eval()\n",
    "\n",
    "    \n",
    "    return initialize_model_for_training(presaved_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting predictions to the format for competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to calculate bounding boxes and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/master/utils/utils.py\n",
    "\n",
    "def bbox_wh_iou(wh1, wh2):\n",
    "    wh2 = wh2.t()\n",
    "    w1, h1 = wh1[0], wh1[1]\n",
    "    w2, h2 = wh2[0], wh2[1]\n",
    "    inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n",
    "    union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n",
    "    return inter_area / union_area\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes\n",
    "    \"\"\"\n",
    "    if not x1y1x2y2:\n",
    "        # Transform from center and width to exact coordinates\n",
    "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "    else:\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "\n",
    "    # get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "    # Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n",
    "        inter_rect_y2 - inter_rect_y1 + 1, min=0\n",
    "    )\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4):\n",
    "    \"\"\"\n",
    "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
    "    Non-Maximum Suppression to further filter detections.\n",
    "    Returns detections with shape:\n",
    "        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n",
    "    \"\"\"\n",
    "\n",
    "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "    prediction[..., :4] = xywh2xyxy(prediction[..., :4])\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for image_i, image_pred in enumerate(prediction):\n",
    "        # Filter out confidence scores below threshold\n",
    "        image_pred = image_pred[image_pred[:, 4] >= conf_thres]\n",
    "        # If none are remaining => process next image\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        # Object confidence times class confidence\n",
    "        score = image_pred[:, 4] * image_pred[:, 5:].max(1)[0]\n",
    "        # Sort by it\n",
    "        image_pred = image_pred[(-score).argsort()]\n",
    "        class_confs, class_preds = image_pred[:, 5:].max(1, keepdim=True)\n",
    "        detections = torch.cat((image_pred[:, :5], class_confs.float(), class_preds.float()), 1)\n",
    "        # Perform non-maximum suppression\n",
    "        keep_boxes = []\n",
    "        while detections.size(0):\n",
    "            large_overlap = bbox_iou(detections[0, :4].unsqueeze(0), detections[:, :4]) > nms_thres\n",
    "            label_match = detections[0, -1] == detections[:, -1]\n",
    "            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n",
    "            invalid = large_overlap & label_match\n",
    "            weights = detections[invalid, 4:5]\n",
    "            # Merge overlapping bboxes by order of confidence\n",
    "            detections[0, :4] = (weights * detections[invalid, :4]).sum(0) / weights.sum()\n",
    "            keep_boxes += [detections[0]]\n",
    "            detections = detections[~invalid]\n",
    "        if keep_boxes:\n",
    "            output[image_i] = torch.stack(keep_boxes)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n",
    "\n",
    "    ByteTensor = torch.cuda.ByteTensor if pred_boxes.is_cuda else torch.ByteTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n",
    "\n",
    "    nB = pred_boxes.size(0)\n",
    "    nA = pred_boxes.size(1)\n",
    "    nC = pred_cls.size(-1)\n",
    "    nG = pred_boxes.size(2)\n",
    "\n",
    "    # Output tensors\n",
    "    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)\n",
    "    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)\n",
    "    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    \n",
    "    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n",
    "    \n",
    "    ### predict additional coordinates for the center within the anchor box\n",
    "    ##### THIS IS OUR ADDITION\n",
    "    ##### RANGE OF THESE VALUES TO BE DETERMINED\n",
    "    tx1 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx2 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx3 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx4 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty1 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty2 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty3 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty4 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "\n",
    "    # Convert to position relative to box\n",
    "    target_boxes = target[:, 2:6] * nG\n",
    "    gxy = target_boxes[:, :2]\n",
    "    gwh = target_boxes[:, 2:]\n",
    "\n",
    "    \n",
    "    \n",
    "    # Get anchors with best iou\n",
    "    ious = torch.stack([bbox_wh_iou(anchor, gwh) for anchor in anchors])\n",
    "    best_ious, best_n = ious.max(0)\n",
    "    # Separate target values\n",
    "    b, target_labels = target[:, :2].long().t()\n",
    "    gx, gy = gxy.t()\n",
    "    gw, gh = gwh.t()\n",
    "    gi, gj = gxy.long().t()\n",
    "    # Set masks\n",
    "    obj_mask[b, best_n, gj, gi] = 1\n",
    "    noobj_mask[b, best_n, gj, gi] = 0\n",
    "\n",
    "    # Set noobj mask to zero where iou exceeds ignore threshold\n",
    "    for i, anchor_ious in enumerate(ious.t()):\n",
    "        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n",
    "\n",
    "    # Coordinates\n",
    "    tx[b, best_n, gj, gi] = gx - gx.floor()\n",
    "    ty[b, best_n, gj, gi] = gy - gy.floor()\n",
    "    # Width and height\n",
    "    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n",
    "    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1] + 1e-16)\n",
    "    # One-hot encoding of label\n",
    "    tcls[b, best_n, gj, gi, target_labels] = 1\n",
    "    # Compute label correctness and iou at best anchor\n",
    "    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n",
    "    iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=False)\n",
    "\n",
    "    tconf = obj_mask.float()\n",
    "    return (iou_scores, class_mask, obj_mask, noobj_mask, \n",
    "            tx, ty, tw, th, \n",
    "            tx1, tx2, tx3, tx4, \n",
    "            ty1, ty2, ty3, ty4, \n",
    "            tcls, tconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YoloLoss(pred_boxes, pred_conf, pred_cls, targets, scaled_anchors, ignore_thres):\n",
    "    \n",
    "    mse_loss = nn.MSELoss()\n",
    "    bce_loss = nn.BCELoss()\n",
    "    \n",
    "    iou_scores, class_mask, obj_mask, noobj_mask, \n",
    "            tx, ty, tw, th, \n",
    "            tx1, tx2, tx3, tx4, \n",
    "            ty1, ty2, ty3, ty4, \n",
    "            tcls, tconf = build_targets(\n",
    "                pred_boxes=pred_boxes,\n",
    "                pred_cls=pred_cls,\n",
    "                target=targets,\n",
    "                anchors=scaled_anchors,\n",
    "                ignore_thres=ignore_thres,\n",
    "            )\n",
    "    \n",
    "    loss_x = self.mse_loss(xc[obj_mask], tx[obj_mask])\n",
    "    loss_y = self.mse_loss(yc[obj_mask], ty[obj_mask])\n",
    "    loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n",
    "    loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n",
    "    \n",
    "    loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n",
    "    loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n",
    "    loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n",
    "    loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n",
    "    total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoadMapLoss(pred_rm, target_rm):\n",
    "    bce_loss = nn.BCELoss()\n",
    "\n",
    "    return bce_loss(pred_rm, target_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_joint_loss(yolo_loss, rm_loss, lambd):\n",
    "    return yolo_loss + lambd * rm_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop and test loops\n",
    "Not necessarily using data loader.\n",
    "\n",
    "Assuming targets are already pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, targets_bb, targets_rm, kobe_model, kobe_optimizer, n_epochs, lambd = 0.5):\n",
    "    for epoch in range(n_epochs):\n",
    "        kobe_model.train()\n",
    "        start_time = time.time()\n",
    "        permutations = torch.randperm(df.shape[0])\n",
    "        for i in range(math.ceil(len(df)/batch_size)):\n",
    "            batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "            batch_images = data[batch_ind, :]\n",
    "            batch_targets_bb = targets_bb[batch_ind, :]\n",
    "            batch_rms = targets_rm[batch_ind, :]\n",
    "            \n",
    "            kobe_optimizer.zero_grad()\n",
    "\n",
    "            imgs = Variable(batch_images.to(device))\n",
    "            targets = Variable(batch_targets.to(device), requires_grad=False)\n",
    "\n",
    "            outputs_yolo, outputs_rm = kobe_model(imgs)\n",
    "            \n",
    "            yolo_loss = YoloLoss(outputs_yolo[0], outputs_yolo[1], outputs_yolo[2], targets, \n",
    "                                 kobe_model.yolo_decoder.scaled_anchors, kobe_model.yolo_decoder.ignore_thres) \n",
    "            \n",
    "            rm_loss = RoadMapLoss(outputs_rm, batch_rms)\n",
    "            \n",
    "            loss = total_joint_loss(yolo_loss, rm_loss, lambd)\n",
    "            loss.backward()\n",
    "            \n",
    "            kobe_optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-defined anchors. Should honestly come from KMeans on detection boxes but let's see how this does before going complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the code uses only the last 3 anchors so let ssee what this does\n",
    "# width, height\n",
    "anchors = [(10,13),  (16,30),  (33,23),  (30,61),  (62,45),  (59,119),  (116,90),  (156,198),  (373,326)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our YoloLayer for task of object localization\n",
    "\n",
    "Ignoring orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTaskEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PreTaskEncoder, self).__init__()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, anchors, num_classes, img_dim=416):\n",
    "        \n",
    "        super(YoloDecoder, self).__init__()\n",
    "        \n",
    "        self.anchors = anchors\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_thres = 0.5\n",
    "        \n",
    "        self.obj_scale = 1\n",
    "        self.noobj_scale = 100\n",
    "        self.img_dim = img_dim\n",
    "        self.grid_size = 16\n",
    "        \n",
    "        # takes in dense output from encoder or shared decoder and puts into an\n",
    "        # image of dim img_dim\n",
    "        self.m = nn.Sequential(\n",
    "        \n",
    "        )\n",
    "        \n",
    "    def compute_grid_offsets(self, grid_size, cuda=True):\n",
    "        self.grid_size = grid_size\n",
    "        g = self.grid_size\n",
    "        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "        self.stride = self.img_dim / self.grid_size\n",
    "        # Calculate offsets for each grid\n",
    "        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n",
    "        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n",
    "        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])\n",
    "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
    "        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Tensors for cuda support\n",
    "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
    "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
    "        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n",
    "\n",
    "        self.img_dim = img_dim\n",
    "        \n",
    "        num_samples = x.size(0)\n",
    "        grid_size = x.size(2)\n",
    "\n",
    "        prediction = (\n",
    "            # 4 from x, y, w, h\n",
    "            # 8 from x1, ..., x4, y1, ..., y4 (OUR ADDITION)\n",
    "            # 1 from confidence of if here or not\n",
    "            # 13 total\n",
    "            # was originally just five\n",
    "            x.view(num_samples, self.num_anchors, self.num_classes + 13, grid_size, grid_size)\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "            .contiguous()\n",
    "        )\n",
    "        \n",
    "        # Get outputs\n",
    "        x = torch.sigmoid(prediction[..., 0])  # Center x\n",
    "        y = torch.sigmoid(prediction[..., 1])  # Center y\n",
    "        w = prediction[..., 2]  # Width\n",
    "        h = prediction[..., 3]  # Height\n",
    "        pred_conf = torch.sigmoid(prediction[..., 4])  # Conf\n",
    "        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n",
    "\n",
    "        # If grid size does not match current we compute new offsets\n",
    "        if grid_size != self.grid_size:\n",
    "            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)\n",
    "\n",
    "        # Add offset and scale with anchors\n",
    "        pred_boxes = FloatTensor(prediction[..., :4].shape)\n",
    "        pred_boxes[..., 0] = x.data + self.grid_x\n",
    "        pred_boxes[..., 1] = y.data + self.grid_y\n",
    "        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w\n",
    "        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h\n",
    "\n",
    "        output = torch.cat(\n",
    "            (\n",
    "                pred_boxes.view(num_samples, -1, 4) * self.stride,\n",
    "                pred_conf.view(num_samples, -1, 1),\n",
    "                pred_cls.view(num_samples, -1, self.num_classes),\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "        \n",
    "        return pred_boxes, pred_conf, pred_cls, output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Model does that does both tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KobeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, yolo_dim, rm_dim):\n",
    "        super(KobeModel, self).__init__()\n",
    "        \n",
    "        # \n",
    "        \n",
    "        self.encoder = PreTaskEncoder()\n",
    "        \n",
    "        \n",
    "        self.shared_decoder = nn.Sequential(\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.yolo_decoder = YoloDecoder(anchors, num_classes, img_dim=yolo_dim)\n",
    "        \n",
    "        self.rm_decoder = RmDecoder(rm_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        #convert from dense representation from encoder into an image\n",
    "        x.view(...)\n",
    "        \n",
    "        x = self.shared_decoder(x)\n",
    "        \n",
    "        # output_1[0] corresponds to the bounding boxes\n",
    "        # output_1[1] corresponds to confidences\n",
    "        # output_1[2] corresponds to the class labels\n",
    "        # oputput_1[3] is them spread out\n",
    "        output_1 = self.yolo_decoder(x)\n",
    "        # roadmap decoder\n",
    "        output_2 = self.rm_decoder(x)\n",
    "        \n",
    "        return output_1, output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
