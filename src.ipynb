{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to keep the code here. And then export into a .py file.\n",
    "\n",
    "Do not try to modify the .py file directly until this is notebook is gone from each branch.\n",
    "\n",
    "Remember to clear output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils and calculating loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming Coordinates\n",
    "\n",
    "Define the given coordinates as world coordinates\n",
    "\n",
    "Define normalized from upper left bound of world coordinates (translate to there, rotate, and normalize) as our normalized image coordinates (or image coordinates for short).\n",
    "\n",
    "Always facing right in world coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 40\n",
    "WIDTH = 2 * 40\n",
    "HEIGHT = 2 * 40\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "S = 7\n",
    "B = 2\n",
    "l_coord = 5\n",
    "l_noobj = 0.5\n",
    "\n",
    "#cuda = torch.cuda.is_available()\n",
    "cuda = False\n",
    "\n",
    "device = 'cuda:0' if cuda else 'cpu'\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "IntTensor = torch.cuda.IntTensor if cuda else torch.IntTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "BoolTensor = torch.cuda.BoolTensor if cuda else torch.BoolTensor\n",
    "\n",
    "def target_encode(boxes, labels):\n",
    "        \"\"\" Encode box coordinates and class labels as one target tensor.\n",
    "        Args:\n",
    "            boxes: (tensor) [[x1, y1, x2, y2]_obj1, ...], normalized from 0.0 to 1.0 w.r.t. image width/height.\n",
    "            labels: (tensor) [c_obj1, c_obj2, ...]\n",
    "        Returns:\n",
    "            An encoded tensor sized [S, S, 5 x B + C], 5=(x, y, w, h, conf)\n",
    "        \"\"\"\n",
    "\n",
    "        C = NUM_CLASSES\n",
    "        N = 5 * B + C\n",
    "\n",
    "        target = torch.zeros(S, S, N)\n",
    "        cell_size = 1.0 / float(S)\n",
    "        boxes_wh = boxes[:, 2:] - boxes[:, :2] # width and height for each box, [n, 2]\n",
    "        boxes_xy = (boxes[:, 2:] + boxes[:, :2]) / 2.0 # center x & y for each box, [n, 2]\n",
    "        for b in range(boxes.size(0)):\n",
    "            xy, wh, label = boxes_xy[b], boxes_wh[b], int(labels[b])\n",
    "\n",
    "            ij = (xy / cell_size).ceil() - 1.0\n",
    "            i, j = int(ij[0]), int(ij[1]) # y & x index which represents its location on the grid.\n",
    "            x0y0 = ij * cell_size # x & y of the cell left-top corner.\n",
    "            xy_normalized = (xy - x0y0) / cell_size # x & y of the box on the cell, normalized from 0.0 to 1.0.\n",
    "\n",
    "            # TBM, remove redundant dimensions from target tensor.\n",
    "            # To remove these, loss implementation also has to be modified.\n",
    "            for k in range(B):\n",
    "                s = 5 * k\n",
    "                target[j, i, s  :s+2] = xy_normalized\n",
    "                target[j, i, s+2:s+4] = wh\n",
    "                target[j, i, s+4    ] = 1.0\n",
    "            target[j, i, 5*B + label] = 1.0\n",
    "\n",
    "        return target\n",
    "    \n",
    "def pred_decode(pred_tensor, conf_thresh=0.1, prob_thresh=0.1):\n",
    "        \"\"\" Decode tensor into box coordinates, class labels, and probs_detected.\n",
    "        Args:\n",
    "            pred_tensor: (tensor) tensor to decode sized [S, S, 5 x B + C], 5=(x, y, w, h, conf)\n",
    "        Returns:\n",
    "            boxes: (tensor) [[x1, y1, x2, y2]_obj1, ...]. Normalized from 0.0 to 1.0 w.r.t. image width/height, sized [n_boxes, 4].\n",
    "            labels: (tensor) class labels for each detected boxe, sized [n_boxes,].\n",
    "            confidences: (tensor) objectness confidences for each detected box, sized [n_boxes,].\n",
    "            class_scores: (tensor) scores for most likely class for each detected box, sized [n_boxes,].\n",
    "        \"\"\"\n",
    "        C = NUM_CLASSES\n",
    "        boxes, labels, confidences, class_scores = [], [], [], []\n",
    "\n",
    "        cell_size = 1.0 / float(S)\n",
    "\n",
    "        conf = pred_tensor[:, :, 4].unsqueeze(2) # [S, S, 1]\n",
    "        for b in range(1, B):\n",
    "            conf = torch.cat((conf, pred_tensor[:, :, 5*b + 4].unsqueeze(2)), 2)\n",
    "        conf_mask = conf > conf_thresh # [S, S, B]\n",
    "\n",
    "        # TBM, further optimization may be possible by replacing the following for-loops with tensor operations.\n",
    "        for i in range(S): # for x-dimension.\n",
    "            for j in range(S): # for y-dimension.\n",
    "                class_score, class_label = torch.max(pred_tensor[j, i, 5*B:], 0)\n",
    "\n",
    "                for b in range(B):\n",
    "                    conf = pred_tensor[j, i, 5*b + 4]\n",
    "                    prob = conf * class_score\n",
    "                    if float(prob) < prob_thresh:\n",
    "                        continue\n",
    "\n",
    "                    # Compute box corner (x1, y1, x2, y2) from tensor.\n",
    "                    box = pred_tensor[j, i, 5*b : 5*b + 4]\n",
    "                    x0y0_normalized = FloatTensor([i, j]) * cell_size # cell left-top corner. Normalized from 0.0 to 1.0 w.r.t. image width/height.\n",
    "                    xy_normalized = box[:2] * cell_size + x0y0_normalized   # box center. Normalized from 0.0 to 1.0 w.r.t. image width/height.\n",
    "                    wh_normalized = box[2:] # Box width and height. Normalized from 0.0 to 1.0 w.r.t. image width/height.\n",
    "                    box_xyxy = FloatTensor(4) # [4,]\n",
    "                    box_xyxy[:2] = xy_normalized - 0.5 * wh_normalized # left-top corner (x1, y1).\n",
    "                    box_xyxy[2:] = xy_normalized + 0.5 * wh_normalized # right-bottom corner (x2, y2).\n",
    "\n",
    "                    # Append result to the lists.\n",
    "                    boxes.append(box_xyxy)\n",
    "                    labels.append(class_label)\n",
    "                    confidences.append(conf)\n",
    "                    class_scores.append(class_score)\n",
    "\n",
    "        if len(boxes) > 0:\n",
    "            boxes = torch.stack(boxes, 0) # [n_boxes, 4]\n",
    "            labels = torch.stack(labels, 0)             # [n_boxes, ]\n",
    "            confidences = torch.stack(confidences, 0)   # [n_boxes, ]\n",
    "            class_scores = torch.stack(class_scores, 0) # [n_boxes, ]\n",
    "        else:\n",
    "            # If no box found, return empty tensors.\n",
    "            boxes = FloatTensor(0, 4)\n",
    "            labels = LongTensor(0)\n",
    "            confidences = FloatTensor(0)\n",
    "            class_scores = FloatTensor(0)\n",
    "\n",
    "        return boxes, labels, confidences, class_scores\n",
    "\n",
    "def nms(boxes, scores, nms_thresh = 0.35):\n",
    "    \"\"\" Apply non maximum supression.\n",
    "    Args:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    threshold = nms_thresh\n",
    "\n",
    "\n",
    "    x1 = boxes[:, 0] # [n,]\n",
    "    y1 = boxes[:, 1] # [n,]\n",
    "    x2 = boxes[:, 2] # [n,]\n",
    "    y2 = boxes[:, 3] # [n,]\n",
    "    areas = (x2 - x1) * (y2 - y1) # [n,]\n",
    "\n",
    "    _, ids_sorted = scores.sort(0, descending=True) # [n,]\n",
    "    ids = []\n",
    "    while ids_sorted.numel() > 0:\n",
    "        # Assume `ids_sorted` size is [m,] in the beginning of this iter.\n",
    "\n",
    "        i = ids_sorted.item() if (ids_sorted.numel() == 1) else ids_sorted[0]\n",
    "        ids.append(i)\n",
    "\n",
    "        if ids_sorted.numel() == 1:\n",
    "            break # If only one box is left (i.e., no box to supress), break.\n",
    "\n",
    "        inter_x1 = x1[ids_sorted[1:]].clamp(min=x1[i].item()) # [m-1, ]\n",
    "        inter_y1 = y1[ids_sorted[1:]].clamp(min=y1[i].item()) # [m-1, ]\n",
    "        inter_x2 = x2[ids_sorted[1:]].clamp(max=x2[i].item()) # [m-1, ]\n",
    "        inter_y2 = y2[ids_sorted[1:]].clamp(max=y2[i].item()) # [m-1, ]\n",
    "        inter_w = (inter_x2 - inter_x1).clamp(min=0) # [m-1, ]\n",
    "        inter_h = (inter_y2 - inter_y1).clamp(min=0) # [m-1, ]\n",
    "\n",
    "        inters = inter_w * inter_h # intersections b/w/ box `i` and other boxes, sized [m-1, ].\n",
    "        unions = areas[i] + areas[ids_sorted[1:]] - inters # unions b/w/ box `i` and other boxes, sized [m-1, ].\n",
    "        ious = inters / unions # [m-1, ]\n",
    "\n",
    "        # Remove boxes whose IoU is higher than the threshold.\n",
    "        ids_keep = (ious <= threshold).nonzero().squeeze() # [m-1, ]. Because `nonzero()` adds extra dimension, squeeze it.\n",
    "        if ids_keep.numel() == 0:\n",
    "            break # If no box left, break.\n",
    "        ids_sorted = ids_sorted[ids_keep+1] # `+1` is needed because `ids_sorted[0] = i`.\n",
    "\n",
    "    return LongTensor(ids)\n",
    "    \n",
    "## input is the \n",
    "# want output that is\n",
    "def transform_target(in_target):\n",
    "    \n",
    "    out_target = []\n",
    "    \n",
    "    for tgt_index in range(len(in_target)):\n",
    "        \n",
    "        #how many boxes for these target\n",
    "        nbox = in_target[tgt_index]['bounding_box'].shape[0]\n",
    "        \n",
    "        # CONVERT ALL THE BOUNDING BOXES for an individual sample at once\n",
    "        \n",
    "        bbox = in_target[tgt_index]['bounding_box'].to(device)\n",
    "        translation = FloatTensor(bbox.shape[0], bbox.shape[1], bbox.shape[2])\n",
    "        translation[:, 0, :].fill_(-40)\n",
    "        translation[:, 1, :].fill_(40)\n",
    "\n",
    "        # translate to uppert left\n",
    "        box = bbox - translation\n",
    "        # reflect y\n",
    "        box[:, 1, :].mul_(-1)\n",
    "\n",
    "        x_min = box[:, 0].min(dim = 1)[0]\n",
    "        y_min = box[:, 1].min(dim = 1)[0]\n",
    "        x_max = box[:, 0].max(dim = 1)[0]\n",
    "        y_max = box[:, 1].max(dim = 1)[0]\n",
    "\n",
    "\n",
    "        x_min = x_min / WIDTH\n",
    "        y_min = y_min / HEIGHT\n",
    "        x_max = x_max / WIDTH\n",
    "        y_max = y_max / HEIGHT\n",
    "        \n",
    "\n",
    "        boxes = torch.stack([x_min, y_min, x_max, y_max], 1)\n",
    "\n",
    "        labels = IntTensor(nbox)\n",
    "        for box_index in range(nbox):\n",
    "            category = in_target[tgt_index]['category'][box_index]\n",
    "            \n",
    "            # from which sample in the batch\n",
    "            labels[box_index] = category\n",
    "            \n",
    "        individual_target = target_encode(boxes, labels)\n",
    "        out_target.append(individual_target)\n",
    "        \n",
    "    return torch.stack(out_target, dim = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load presaved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works by side effects\n",
    "def load_pretask_weight_from_model(model, presaved_encoder):\n",
    "    model.encoder.load_state_dict(presaved_encoder.state_dict())\n",
    "    \n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you want Initialize Our Model with encoder weights from an existing pretask encoder in memory\n",
    "def initialize_model_for_training(presaved_encoder):\n",
    "    model = KobeModel(num_classes = 10, encoder_features = 6, rm_dim = 800)\n",
    "    load_pretask_weight_from_model(model, presaved_encoder)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you want Initialize Our Model with encoder weights from a file\n",
    "def initialize_model_for_training_file(presaved_encoder_file):\n",
    "    presaved_encoder = PreTaskEncoder()\n",
    "    presaved_encoder.load_state_dict(torch.load(presaved_encoder_file))\n",
    "    presaved_encoder.eval()\n",
    "\n",
    "    \n",
    "    return initialize_model_for_training(presaved_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting predictions to the format for competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to calculate bounding boxes and such"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoadMapLoss(pred_rm, target_rm):\n",
    "    bce_loss = nn.BCELoss()\n",
    "\n",
    "    return bce_loss(pred_rm, target_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_joint_loss(yolo_loss, rm_loss, lambd):\n",
    "    return yolo_loss + lambd * rm_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop and test loops\n",
    "Not necessarily using data loader.\n",
    "\n",
    "Assuming targets are already pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo(data_loader, kobe_model, kobe_optimizer, lambd = 20):\n",
    "    kobe_model.train()\n",
    "    train_loss = 0 \n",
    "        \n",
    "    for i, data in enumerate(data_loader):\n",
    "        sample, target, road_image = data\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        target_original = target\n",
    "        target = transform_target(target_original).to(device)\n",
    "        road_image = torch.stack(road_image).float().to(device)\n",
    "\n",
    "        kobe_optimizer.zero_grad()\n",
    "\n",
    "        output_yolo, yolo_loss, output_rm, rm_loss = kobe_model(sample, yolo_targets = target, \n",
    "                                                                rm_targets = road_image)\n",
    "\n",
    "        # SHOULD GET LOWER OVER EPOCHS\n",
    "        #print(\"PRINTING output yolo after nms\")\n",
    "        #if output_yolo[0] is not None:\n",
    "        #    print(output_yolo[0].shape)\n",
    "        #    print(output_yolo[0])\n",
    "        #else:\n",
    "        #    print(\"It was none!\")\n",
    "        #    print(output_yolo)\n",
    "        #rm_loss = RoadMapLoss(outputs_rm, batch_rms)\n",
    "\n",
    "        # loss = total_joint_loss(yolo_loss, rm_loss, lambd)\n",
    "        # print(\"yolo loss\")\n",
    "        # print(yolo_loss)\n",
    "        # print('rm loss')\n",
    "        # print(lambd * rm_loss)\n",
    "        \n",
    "        predicted_bounding_boxes = output_yolo[0].cpu()\n",
    "        ats_bounding_boxes = compute_ats_bounding_boxes(predicted_bounding_boxes, target_original[0]['bounding_box'])\n",
    "        print('ats bounding box')\n",
    "        print(ats_bounding_boxes)\n",
    "        \n",
    "        ts_road_map = compute_ts_road_map(output_rm, road_image)\n",
    "        print('ts roadmap')\n",
    "        print(ts_road_map)\n",
    "        \n",
    "        \n",
    "        total_loss = total_joint_loss(yolo_loss, rm_loss, lambd)\n",
    "        train_loss += (total_loss.item())\n",
    "        total_loss.backward()\n",
    "\n",
    "        kobe_optimizer.step()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print(\"TRAIN LOSS: {}\".format(train_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-defined anchors. Should honestly come from KMeans on detection boxes but let's see how this does before going complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our YoloLayer for task of object localization\n",
    "\n",
    "Ignoring orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_HIDDEN = int(26718 / 2)\n",
    "class PreTaskEncoder(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(PreTaskEncoder, self).__init__()\n",
    "        # number of different kernels to use\n",
    "        self.n_features = n_features\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=n_features,\n",
    "                               kernel_size=5,\n",
    "                               )\n",
    "        self.conv2 = nn.Conv2d(n_features,\n",
    "                               int(n_features/2),\n",
    "                               kernel_size=5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # return an array shape\n",
    "        x = x.view(-1, ENCODER_HIDDEN)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer2d(nn.Module):\n",
    "    def __init__(self, channels, dim):\n",
    "        super(ReshapeLayer2d, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], self.channels, self.dim, self.dim)\n",
    "    \n",
    "class ReshapeLayer1d(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ReshapeLayer1d, self).__init__()\n",
    "        self.features = features\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], self.features)\n",
    "\n",
    "class YoloDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        \n",
    "        super(YoloDecoder, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # takes in dense output from encoder or shared decoder and puts into an\n",
    "        # image of dim img_dim\n",
    "\n",
    "        self.m = nn.Sequential(\n",
    "            nn.Linear(6 * ENCODER_HIDDEN, 2 * 15 * 15),\n",
    "            nn.ReLU(),\n",
    "            ReshapeLayer2d(2, 15),\n",
    "            nn.Conv2d(2, 2, kernel_size=3, stride = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride = 1),\n",
    "            ReshapeLayer1d(288),\n",
    "            nn.Linear(288, S * S * (5 * B + self.num_classes)),\n",
    "            # Sigmoid is final layer in Yolo v1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.m(x)\n",
    "\n",
    "\n",
    "        num_samples = x.shape[0]\n",
    "\n",
    "        prediction = (\n",
    "            x.view(num_samples, S, S, 5 * B + self.num_classes)\n",
    "            .contiguous()\n",
    "        )\n",
    "                \n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/motokimura/yolo_v1_pytorch/\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, feature_size=S, num_bboxes=B, num_classes=NUM_CLASSES, \n",
    "                 lambda_coord=l_coord, lambda_noobj=l_noobj):\n",
    "        \"\"\" Constructor.\n",
    "        Args:\n",
    "            feature_size: (int) size of input feature map.\n",
    "            num_bboxes: (int) number of bboxes per each cell.\n",
    "            num_classes: (int) number of the object classes.\n",
    "            lambda_coord: (float) weight for bbox location/size losses.\n",
    "            lambda_noobj: (float) weight for no-objectness loss.\n",
    "        \"\"\"\n",
    "        super(YoloLoss, self).__init__()\n",
    "\n",
    "        self.S = feature_size\n",
    "        self.B = num_bboxes\n",
    "        self.C = num_classes\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "\n",
    "\n",
    "    def compute_iou(self, bbox1, bbox2):\n",
    "        \"\"\" Compute the IoU (Intersection over Union) of two set of bboxes, each bbox format: [x1, y1, x2, y2].\n",
    "        Args:\n",
    "            bbox1: (Tensor) bounding bboxes, sized [N, 4].\n",
    "            bbox2: (Tensor) bounding bboxes, sized [M, 4].\n",
    "        Returns:\n",
    "            (Tensor) IoU, sized [N, M].\n",
    "        \"\"\"\n",
    "        N = bbox1.size(0)\n",
    "        M = bbox2.size(0)\n",
    "\n",
    "        # Compute left-top coordinate of the intersections\n",
    "        lt = torch.max(\n",
    "            bbox1[:, :2].unsqueeze(1).expand(N, M, 2), # [N, 2] -> [N, 1, 2] -> [N, M, 2]\n",
    "            bbox2[:, :2].unsqueeze(0).expand(N, M, 2)  # [M, 2] -> [1, M, 2] -> [N, M, 2]\n",
    "        )\n",
    "        # Conpute right-bottom coordinate of the intersections\n",
    "        rb = torch.min(\n",
    "            bbox1[:, 2:].unsqueeze(1).expand(N, M, 2), # [N, 2] -> [N, 1, 2] -> [N, M, 2]\n",
    "            bbox2[:, 2:].unsqueeze(0).expand(N, M, 2)  # [M, 2] -> [1, M, 2] -> [N, M, 2]\n",
    "        )\n",
    "        # Compute area of the intersections from the coordinates\n",
    "        wh = rb - lt   # width and height of the intersection, [N, M, 2]\n",
    "        wh[wh < 0] = 0 # clip at 0\n",
    "        inter = wh[:, :, 0] * wh[:, :, 1] # [N, M]\n",
    "\n",
    "        # Compute area of the bboxes\n",
    "        area1 = (bbox1[:, 2] - bbox1[:, 0]) * (bbox1[:, 3] - bbox1[:, 1]) # [N, ]\n",
    "        area2 = (bbox2[:, 2] - bbox2[:, 0]) * (bbox2[:, 3] - bbox2[:, 1]) # [M, ]\n",
    "        area1 = area1.unsqueeze(1).expand_as(inter) # [N, ] -> [N, 1] -> [N, M]\n",
    "        area2 = area2.unsqueeze(0).expand_as(inter) # [M, ] -> [1, M] -> [N, M]\n",
    "\n",
    "        # Compute IoU from the areas\n",
    "        union = area1 + area2 - inter # [N, M, 2]\n",
    "        iou = inter / union           # [N, M, 2]\n",
    "\n",
    "        return iou\n",
    "\n",
    "    def forward(self, pred_tensor, target_tensor):\n",
    "        \"\"\" Compute loss for YOLO training.\n",
    "        Args:\n",
    "            pred_tensor: (Tensor) predictions, sized [n_batch, S, S, Bx5+C], 5=len([x, y, w, h, conf]).\n",
    "            target_tensor: (Tensor) targets, sized [n_batch, S, S, Bx5+C].\n",
    "        Returns:\n",
    "            (Tensor): loss, sized [1, ].\n",
    "        \"\"\"\n",
    "        # TODO: Romove redundant dimensions for some Tensors.\n",
    "\n",
    "        S, B, C = self.S, self.B, self.C\n",
    "        \n",
    "        N = 5 * B + C\n",
    "\n",
    "        batch_size = pred_tensor.size(0)\n",
    "        coord_mask = target_tensor[:, :, :, 4] > 0  # mask for the cells which contain objects. [n_batch, S, S]\n",
    "        noobj_mask = target_tensor[:, :, :, 4] == 0 # mask for the cells which do not contain objects. [n_batch, S, S]\n",
    "        coord_mask = coord_mask.unsqueeze(-1).expand_as(target_tensor) # [n_batch, S, S] -> [n_batch, S, S, N]\n",
    "        noobj_mask = noobj_mask.unsqueeze(-1).expand_as(target_tensor) # [n_batch, S, S] -> [n_batch, S, S, N]\n",
    "\n",
    "        coord_pred = pred_tensor[coord_mask].view(-1, N)            # pred tensor on the cells which contain objects. [n_coord, N]\n",
    "                                                                    # n_coord: number of the cells which contain objects.\n",
    "        bbox_pred = coord_pred[:, :5*B].contiguous().view(-1, 5)    # [n_coord x B, 5=len([x, y, w, h, conf])]\n",
    "        class_pred = coord_pred[:, 5*B:]                            # [n_coord, C]\n",
    "\n",
    "        coord_target = target_tensor[coord_mask].view(-1, N)        # target tensor on the cells which contain objects. [n_coord, N]\n",
    "                                                                    # n_coord: number of the cells which contain objects.\n",
    "        bbox_target = coord_target[:, :5*B].contiguous().view(-1, 5)# [n_coord x B, 5=len([x, y, w, h, conf])]\n",
    "        class_target = coord_target[:, 5*B:]                        # [n_coord, C]\n",
    "\n",
    "        # Compute loss for the cells with no object bbox.\n",
    "        noobj_pred = pred_tensor[noobj_mask].view(-1, N)        # pred tensor on the cells which do not contain objects. [n_noobj, N]\n",
    "                                                                # n_noobj: number of the cells which do not contain objects.\n",
    "        noobj_target = target_tensor[noobj_mask].view(-1, N)    # target tensor on the cells which do not contain objects. [n_noobj, N]\n",
    "                                                                # n_noobj: number of the cells which do not contain objects.\n",
    "        noobj_conf_mask = BoolTensor(noobj_pred.size()).fill_(0) # [n_noobj, N]\n",
    "        for b in range(B):\n",
    "            noobj_conf_mask[:, 4 + b*5] = 1 # noobj_conf_mask[:, 4] = 1; noobj_conf_mask[:, 9] = 1\n",
    "        noobj_pred_conf = noobj_pred[noobj_conf_mask]       # [n_noobj, 2=len([conf1, conf2])]\n",
    "        noobj_target_conf = noobj_target[noobj_conf_mask]   # [n_noobj, 2=len([conf1, conf2])]\n",
    "        loss_noobj = F.mse_loss(noobj_pred_conf, noobj_target_conf, reduction='sum')\n",
    "\n",
    "        # Compute loss for the cells with objects.\n",
    "        coord_response_mask = BoolTensor(bbox_target.size()).fill_(0)    # [n_coord x B, 5]\n",
    "        coord_not_response_mask = BoolTensor(bbox_target.size()).fill_(1)# [n_coord x B, 5]\n",
    "        bbox_target_iou = torch.zeros(bbox_target.size())                    # [n_coord x B, 5], only the last 1=(conf,) is used\n",
    "\n",
    "        # Choose the predicted bbox having the highest IoU for each target bbox.\n",
    "        for i in range(0, bbox_target.size(0), B):\n",
    "            pred = bbox_pred[i:i+B] # predicted bboxes at i-th cell, [B, 5=len([x, y, w, h, conf])]\n",
    "            pred_xyxy = Variable(torch.FloatTensor(pred.size())) # [B, 5=len([x1, y1, x2, y2, conf])]\n",
    "            # Because (center_x,center_y)=pred[:, 2] and (w,h)=pred[:,2:4] are normalized for cell-size and image-size respectively,\n",
    "            # rescale (center_x,center_y) for the image-size to compute IoU correctly.\n",
    "            pred_xyxy[:,  :2] = pred[:, 2]/float(S) - 0.5 * pred[:, 2:4]\n",
    "            pred_xyxy[:, 2:4] = pred[:, 2]/float(S) + 0.5 * pred[:, 2:4]\n",
    "\n",
    "            target = bbox_target[i] # target bbox at i-th cell. Because target boxes contained by each cell are identical in current implementation, enough to extract the first one.\n",
    "            target = bbox_target[i].view(-1, 5) # target bbox at i-th cell, [1, 5=len([x, y, w, h, conf])]\n",
    "            target_xyxy = Variable(torch.FloatTensor(target.size())) # [1, 5=len([x1, y1, x2, y2, conf])]\n",
    "            # Because (center_x,center_y)=target[:, 2] and (w,h)=target[:,2:4] are normalized for cell-size and image-size respectively,\n",
    "            # rescale (center_x,center_y) for the image-size to compute IoU correctly.\n",
    "            target_xyxy[:,  :2] = target[:, 2]/float(S) - 0.5 * target[:, 2:4]\n",
    "            target_xyxy[:, 2:4] = target[:, 2]/float(S) + 0.5 * target[:, 2:4]\n",
    "\n",
    "            iou = self.compute_iou(pred_xyxy[:, :4], target_xyxy[:, :4]) # [B, 1]\n",
    "            max_iou, max_index = iou.max(0)\n",
    "            max_index = max_index.data\n",
    "\n",
    "            coord_response_mask[i+max_index] = 1\n",
    "            coord_not_response_mask[i+max_index] = 0\n",
    "\n",
    "            # \"we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth\"\n",
    "            # from the original paper of YOLO.\n",
    "            bbox_target_iou[i+max_index, LongTensor([4])] = (max_iou).data\n",
    "        bbox_target_iou = Variable(bbox_target_iou).to(device)\n",
    "\n",
    "        # BBox location/size and objectness loss for the response bboxes.\n",
    "        bbox_pred_response = bbox_pred[coord_response_mask].view(-1, 5)      # [n_response, 5]\n",
    "        bbox_target_response = bbox_target[coord_response_mask].view(-1, 5)  # [n_response, 5], only the first 4=(x, y, w, h) are used\n",
    "        target_iou = bbox_target_iou[coord_response_mask].view(-1, 5)        # [n_response, 5], only the last 1=(conf,) is used\n",
    "        loss_xy = F.mse_loss(bbox_pred_response[:, :2], bbox_target_response[:, :2], reduction='sum')\n",
    "        loss_wh = F.mse_loss(torch.sqrt(bbox_pred_response[:, 2:4]), torch.sqrt(bbox_target_response[:, 2:4]), reduction='sum')\n",
    "        loss_obj = F.mse_loss(bbox_pred_response[:, 4], target_iou[:, 4], reduction='sum')\n",
    "\n",
    "        # Class probability loss for the cells which contain objects.\n",
    "        loss_class = F.mse_loss(class_pred, class_target, reduction='sum')\n",
    "\n",
    "        # Total loss\n",
    "        loss = self.lambda_coord * (loss_xy + loss_wh) + loss_obj + self.lambda_noobj * loss_noobj + loss_class\n",
    "        loss = loss / float(batch_size)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RmDecoder(nn.Module):\n",
    "    def __init__(self, rm_dim):\n",
    "        super(RmDecoder, self).__init__()\n",
    "        \n",
    "        self.rm_dim = 800\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(6 * ENCODER_HIDDEN, 2 * 15 * 15),\n",
    "            nn.ReLU(),\n",
    "            ReshapeLayer2d(2, 15),\n",
    "            nn.ConvTranspose2d(2, 2, kernel_size=4, stride = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(2, 2, kernel_size=10, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=4),\n",
    "            nn.ConvTranspose2d(2, 2, kernel_size=4, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 1, kernel_size = 3, stride = 1),\n",
    "            # Sigmoid is final layer in Yolo v1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # [N, 1, 800, 800]\n",
    "        # [N, 800, 800]\n",
    "        x = self.model(x)\n",
    "        x = x.squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Model does that does both tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KobeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, encoder_features, rm_dim):\n",
    "        super(KobeModel, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.encoder = PreTaskEncoder(encoder_features)\n",
    "        \n",
    "        \n",
    "        #self.shared_decoder = nn.Sequential()\n",
    "        \n",
    "        self.yolo_decoder = YoloDecoder(num_classes = num_classes)\n",
    "        \n",
    "        self.yolo_loss = YoloLoss(feature_size=S, num_bboxes=B, num_classes=num_classes, \n",
    "                                  lambda_coord=l_coord, lambda_noobj = l_noobj)\n",
    "        \n",
    "        self.rm_decoder = RmDecoder(rm_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \n",
    "        # get all the representations laid out like this\n",
    "        x = torch.cat([self.encoder(x[:, i, :]) for i in range(6)], dim = 1)\n",
    "            \n",
    "            \n",
    "        #convert from dense representation from encoder into an image\n",
    "        # x.view(...)\n",
    "        \n",
    "        #x = self.shared_decoder(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, yolo_targets = None, rm_targets = None ):\n",
    "        encoding = self.encode(x)\n",
    "        \n",
    "        output_1, yolo_loss = self.get_bounding_boxes(x, encoding = encoding, targets = yolo_targets)\n",
    "        \n",
    "        # roadmap decoder\n",
    "        output_2, rm_loss = self.get_road_map(x, encoding, targets = rm_targets)\n",
    "        \n",
    "        # output1 is not in the context of our bounding boxes\n",
    "        #return output_1, output_2, yolo_loss, rm_loss\n",
    "        return output_1, yolo_loss, output_2, rm_loss\n",
    "    \n",
    "    # for easy use for competition\n",
    "    # in competition, encoding is None\n",
    "    def get_bounding_boxes(self, x, encoding = None, targets = None):\n",
    "        if encoding is None:\n",
    "            encoding = self.encode(x)\n",
    "        \n",
    "        outputs = self.yolo_decoder(encoding)\n",
    "        \n",
    "        if targets is not None:\n",
    "            yoloLossValue = self.yolo_loss(outputs, targets)\n",
    "        else:\n",
    "            yoloLossValue = 0\n",
    "        \n",
    "        \n",
    "        boxes = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            # Get detected boxes_detected, labels, confidences, class-scores.\n",
    "            boxes_normalized_all, class_labels_all, confidences_all, class_scores_all = pred_decode(output)\n",
    "            if boxes_normalized_all.size(0) == 0:\n",
    "                return ()\n",
    "\n",
    "            # Apply non maximum supression for boxes of each class.\n",
    "            boxes_normalized, class_labels, probs = [], [], []\n",
    "\n",
    "            for class_label in range(self.num_classes):\n",
    "                mask = (class_labels_all == class_label)\n",
    "                if torch.sum(mask) == 0:\n",
    "                    continue # if no box found, skip that class.\n",
    "\n",
    "                boxes_normalized_masked = boxes_normalized_all[mask]\n",
    "                class_labels_maked = class_labels_all[mask]\n",
    "                confidences_masked = confidences_all[mask]\n",
    "                class_scores_masked = class_scores_all[mask]\n",
    "\n",
    "                ids = nms(boxes_normalized_masked, confidences_masked)\n",
    "\n",
    "                boxes_normalized.append(boxes_normalized_masked[ids])\n",
    "                class_labels.append(class_labels_maked[ids])\n",
    "                probs.append(confidences_masked[ids] * class_scores_masked[ids])\n",
    "\n",
    "            boxes_normalized = torch.cat(boxes_normalized, 0)\n",
    "            class_labels = torch.cat(class_labels, 0)\n",
    "            probs = torch.cat(probs, 0)\n",
    "        \n",
    "\n",
    "            better_coordinates = FloatTensor(boxes_normalized.shape[0], 2, 4)\n",
    "            translation = FloatTensor(boxes_normalized.shape[0], 2, 4)\n",
    "            translation[:, 0, :].fill_(-40)\n",
    "            translation[:, 1, :].fill_(40)\n",
    "\n",
    "            center_x = (boxes_normalized[:, 0] + boxes_normalized[:, 2]) / 2 * WIDTH\n",
    "            center_y = (boxes_normalized[:, 1] + boxes_normalized[:, 3]) / 2 * HEIGHT\n",
    "            width = (boxes_normalized[:, 2] - boxes_normalized[:,0]) * WIDTH\n",
    "            height = (boxes_normalized[:, 3] - boxes_normalized[:,1]) * HEIGHT\n",
    "            \n",
    "            x1 = center_x - width/2\n",
    "            x2 = center_x + width/2\n",
    "            x3 = center_x - width/2\n",
    "            x4 = center_x + width/2\n",
    "            \n",
    "            \n",
    "            y1 = center_y - height/2\n",
    "            y2 = center_y + height/2\n",
    "            y3 = center_y + height/2\n",
    "            y4 = center_y - height/2\n",
    "            \n",
    "            better_coordinates[:, 0, 0] = x1\n",
    "            better_coordinates[:, 0, 1] = x2\n",
    "            better_coordinates[:, 0, 2] = x3\n",
    "            better_coordinates[:, 0, 3] = x4\n",
    "            \n",
    "            better_coordinates[:, 1, 0] = y1\n",
    "            better_coordinates[:, 1, 1] = y2\n",
    "            better_coordinates[:, 1, 2] = y3\n",
    "            better_coordinates[:, 1, 3] = y4\n",
    "            \n",
    "            better_coordinates[:, 1, :].mul_(-1)\n",
    "            # shift back!\n",
    "            better_coordinates += translation\n",
    "            \n",
    "            boxes.append(better_coordinates)\n",
    "        \n",
    "        #print('got this incoming')\n",
    "        #print(x.shape)\n",
    "        #print('got these many boxes to look at {}'.format(len(boxes)))\n",
    "        ##print('it has this many detections {} at one site'.format(boxes[0].shape[0]))\n",
    "        #print(\"looks like this\")\n",
    "        #print(boxes[0])\n",
    "        #print(\"Bounding box outputs\")\n",
    "        #print(tuple(boxes))\n",
    "        return tuple(boxes), yoloLossValue\n",
    "    \n",
    "    def get_road_map(self, x, encoding = None, targets = None):\n",
    "        if encoding is None:\n",
    "            encoding = self.encode(x)\n",
    "        \n",
    "        outputs = self.rm_decoder(encoding)\n",
    "        bce_loss = nn.BCELoss()\n",
    "        if targets is not None:\n",
    "            loss = bce_loss(outputs, targets) / x.shape[0]\n",
    "        else:\n",
    "            loss = 0\n",
    "        #print('roadmap outputs')\n",
    "        #print(outputs)\n",
    "        return outputs, loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kobe_model = KobeModel(num_classes = 10, encoder_features = 6, rm_dim = 800)\n",
    "#kobe_model = initialize_model_for_training_file(FILE_OF_PRETRAINING_ENCODER)\n",
    "\n",
    "kobe_model.to(device)\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "\n",
    "kobe_optimizer = torch.optim.Adam(kobe_model.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'data'\n",
    "annotation_csv = 'data/annotation.csv'\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, \n",
    "                                          batch_size=1, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2, \n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for sample, target, road_image, extra in trainloader:\n",
    "    sample = torch.stack(sample).to(device)\n",
    "    target = transform_target(target).to(device)\n",
    "    road_image = torch.stack(road_image)\n",
    "    \n",
    "    output_yolo, yolo_loss = kobe_model(sample, yolo_target = target)\n",
    "    \n",
    "    # SHOULD GET LOWER OVER EPOCHS\n",
    "    print(output_yolo[0].shape)\n",
    "    yolo_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "Original\n",
      "({'bounding_box': tensor([[[ 28.5206,  28.4480,  34.0207,  33.9480],\n",
      "         [ 25.4046,  27.4720,  25.5992,  27.6666]],\n",
      "\n",
      "        [[-12.7610, -12.7237, -20.2440, -20.2067],\n",
      "         [ -2.1773,  -4.2857,  -2.3113,  -4.4197]],\n",
      "\n",
      "        [[ 17.2145,  17.2755,  12.8376,  12.8987],\n",
      "         [ -2.3159,  -4.0525,  -2.4707,  -4.2074]],\n",
      "\n",
      "        [[  9.3104,   9.2026,  14.3000,  14.1921],\n",
      "         [ 25.0430,  27.0919,  25.3068,  27.3557]],\n",
      "\n",
      "        [[ 13.8485,  13.8331,   9.5281,   9.5127],\n",
      "         [  4.5497,   2.6700,   4.5843,   2.7046]],\n",
      "\n",
      "        [[  1.4591,   1.4596,  -3.9763,  -3.9758],\n",
      "         [ -2.1587,  -4.3284,  -2.1612,  -4.3309]],\n",
      "\n",
      "        [[-33.2156, -33.2151, -38.4560, -38.4556],\n",
      "         [ -2.0136,  -4.1033,  -2.0160,  -4.1057]],\n",
      "\n",
      "        [[ -6.1836,  -6.1841,  -1.1811,  -1.1816],\n",
      "         [ 25.4249,  27.5666,  25.4272,  27.5689]],\n",
      "\n",
      "        [[ 22.4349,  22.4515,  17.7106,  17.7272],\n",
      "         [  8.0780,   6.2793,   8.0333,   6.2346]],\n",
      "\n",
      "        [[-21.2364, -21.1352, -26.0173, -25.9161],\n",
      "         [ -2.2440,  -4.1660,  -2.4967,  -4.4188]],\n",
      "\n",
      "        [[ 12.4173,  12.4178,   7.5209,   7.5213],\n",
      "         [ -2.4159,  -4.3606,  -2.4181,  -4.3629]],\n",
      "\n",
      "        [[ 30.0802,  30.1145,  24.4816,  24.5160],\n",
      "         [ -2.0733,  -4.0167,  -2.1736,  -4.1170]],\n",
      "\n",
      "        [[  3.5715,   3.5878,  -0.8428,  -0.8266],\n",
      "         [  8.0024,   6.2417,   7.9607,   6.2000]],\n",
      "\n",
      "        [[ 35.8483,  35.9752,  31.4176,  31.5446],\n",
      "         [ -2.1734,  -3.9826,  -2.4852,  -4.2945]],\n",
      "\n",
      "        [[ 31.2166,  31.2136,  30.3078,  30.3047],\n",
      "         [ -8.7145,  -9.5054,  -8.7112,  -9.5021]],\n",
      "\n",
      "        [[-29.6915, -29.6750, -34.3978, -34.3813],\n",
      "         [  1.3604,  -0.4333,   1.3158,  -0.4779]],\n",
      "\n",
      "        [[  7.1674,   7.1678,   2.3899,   2.3903],\n",
      "         [ -2.3250,  -4.1128,  -2.3272,  -4.1150]],\n",
      "\n",
      "        [[-26.9856, -26.9139, -32.0939, -32.0222],\n",
      "         [ -2.0514,  -4.0929,  -2.2322,  -4.2736]],\n",
      "\n",
      "        [[ -5.4758,  -5.4754, -11.0303, -11.0298],\n",
      "         [ -2.2394,  -4.3641,  -2.2419,  -4.3667]],\n",
      "\n",
      "        [[ 23.3499,  23.3833,  18.5872,  18.6206],\n",
      "         [ -1.9154,  -3.8058,  -2.0007,  -3.8912]]], dtype=torch.float64), 'category': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2])},)\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2852, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[ -5.3225,  -5.4189,  -0.8090,  -0.9054],\n",
      "         [ 18.9893,  20.6638,  19.2492,  20.9237]],\n",
      "\n",
      "        [[ -8.9113,  -8.8608, -12.9686, -12.9181],\n",
      "         [  1.0116,  -0.7290,   0.8936,  -0.8470]],\n",
      "\n",
      "        [[-34.3140, -32.4618, -33.7735, -31.9212],\n",
      "         [ -9.4935,  -9.2762, -14.0990, -13.8816]],\n",
      "\n",
      "        [[  7.8929,   7.7809,  12.6233,  12.5113],\n",
      "         [ 12.2336,  13.9923,  12.5349,  14.2936]],\n",
      "\n",
      "        [[ 14.9534,  14.9865,  10.8010,  10.8341],\n",
      "         [  9.0366,   7.1297,   8.9643,   7.0574]],\n",
      "\n",
      "        [[-25.3550, -25.4504, -20.3205, -20.4160],\n",
      "         [ 11.0878,  12.9626,  11.3442,  13.2190]],\n",
      "\n",
      "        [[ 33.2528,  33.2545,  38.4538,  38.4554],\n",
      "         [ 27.3090,  29.2952,  27.3052,  29.2914]],\n",
      "\n",
      "        [[-31.3527, -29.5570, -30.8931, -29.0975],\n",
      "         [ -9.3444,  -9.1337, -13.2599, -13.0491]],\n",
      "\n",
      "        [[-16.9671, -17.0342, -12.2915, -12.3587],\n",
      "         [ 25.1257,  26.8707,  25.3060,  27.0509]],\n",
      "\n",
      "        [[-10.9745, -11.0478,  -5.5298,  -5.6032],\n",
      "         [ 25.5510,  27.6509,  25.7415,  27.8413]],\n",
      "\n",
      "        [[  1.1803,   0.9221,   5.6966,   5.4383],\n",
      "         [ 11.1976,  12.8660,  11.8964,  13.5648]],\n",
      "\n",
      "        [[ 10.3753,  10.4380,   6.2040,   6.2667],\n",
      "         [  5.0687,   3.3286,   4.9183,   3.1781]],\n",
      "\n",
      "        [[-27.0488, -27.2037, -18.1507, -18.3057],\n",
      "         [ 24.4277,  27.1942,  24.9263,  27.6928]],\n",
      "\n",
      "        [[ -5.7345,  -6.1485,  -0.7811,  -1.1952],\n",
      "         [ 13.0159,  14.7658,  14.1873,  15.9372]],\n",
      "\n",
      "        [[ -3.6439,  -3.7591,   8.1110,   7.9958],\n",
      "         [ 25.4595,  28.1148,  25.9701,  28.6254]],\n",
      "\n",
      "        [[-23.5131, -23.5898, -18.4952, -18.5719],\n",
      "         [ 18.4651,  20.3798,  18.6664,  20.5811]],\n",
      "\n",
      "        [[ 17.3867,  17.3256,  22.7156,  22.6545],\n",
      "         [ 19.6047,  21.7660,  19.7557,  21.9169]],\n",
      "\n",
      "        [[ -7.6283,  -7.5076, -15.5986, -15.4779],\n",
      "         [  5.3559,   2.7698,   4.9837,   2.3976]],\n",
      "\n",
      "        [[  3.0554,   3.3505,  -1.5587,  -1.2636],\n",
      "         [-19.4367, -21.0899, -20.2602, -21.9133]],\n",
      "\n",
      "        [[ -1.3664,   0.5338,  -2.3951,  -0.4949],\n",
      "         [-10.0906, -10.5006, -14.8529, -15.2629]],\n",
      "\n",
      "        [[  0.4969,   0.5765,   4.7805,   4.8601],\n",
      "         [ 16.2166,  17.9821,  16.0240,  17.7894]],\n",
      "\n",
      "        [[-11.7966, -12.0626,  -6.8349,  -7.1010],\n",
      "         [ 11.9889,  13.8896,  12.6832,  14.5839]],\n",
      "\n",
      "        [[-18.8117, -18.9797, -13.4854, -13.6534],\n",
      "         [ 11.1980,  13.1520,  11.6561,  13.6101]],\n",
      "\n",
      "        [[ 32.9628,  33.0504,  26.5493,  26.6369],\n",
      "         [  6.3796,   4.2613,   6.1142,   3.9959]],\n",
      "\n",
      "        [[ 10.1327,  10.0516,  16.7491,  16.6680],\n",
      "         [ 26.0675,  28.5311,  26.2856,  28.7492]],\n",
      "\n",
      "        [[-26.2261, -24.3291, -25.9348, -24.0377],\n",
      "         [ -9.4822,  -9.3597, -13.9969, -13.8744]]], dtype=torch.float64), 'category': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 5, 2, 2, 4, 2, 2, 2, 2, 2, 2,\n",
      "        0, 2])},)\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.3353, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[ 13.2642,  15.0871,  14.5596,  16.3826],\n",
      "         [-21.6459, -21.1593, -26.5000, -26.0134]],\n",
      "\n",
      "        [[ 33.0494,  33.0517,  39.1287,  39.1309],\n",
      "         [ 24.8950,  26.9990,  24.8884,  26.9924]],\n",
      "\n",
      "        [[ 16.7219,  16.7488,  21.9258,  21.9527],\n",
      "         [ 17.7644,  19.8782,  17.6978,  19.8116]],\n",
      "\n",
      "        [[ -5.9104,  -4.6049,  -3.0982,  -1.7927],\n",
      "         [ 20.6114,  21.7372,  17.3494,  18.4752]],\n",
      "\n",
      "        [[ 15.0727,  17.0512,  16.3653,  18.3438],\n",
      "         [-28.5881, -28.0600, -33.4315, -32.9035]],\n",
      "\n",
      "        [[ 11.4328,  13.2240,  12.4622,  14.2534],\n",
      "         [-14.8887, -14.4473, -19.0658, -18.6243]],\n",
      "\n",
      "        [[-33.7178, -33.5604, -28.3822, -28.2247],\n",
      "         [ 34.6400,  36.7611,  34.2437,  36.3648]],\n",
      "\n",
      "        [[ 33.8834,  33.5246,  32.1582,  31.7994],\n",
      "         [-12.1375, -12.8018, -11.2055, -11.8698]],\n",
      "\n",
      "        [[-34.0567, -34.0798, -38.3008, -38.3239],\n",
      "         [  7.4655,   5.6557,   7.5198,   5.7100]],\n",
      "\n",
      "        [[-23.5494, -23.5150, -31.9486, -31.9143],\n",
      "         [ -2.1563,  -4.7701,  -2.2665,  -4.8803]],\n",
      "\n",
      "        [[ 17.4659,  19.2416,  18.5604,  20.3361],\n",
      "         [-24.9235, -24.4858, -29.3646, -28.9269]],\n",
      "\n",
      "        [[ 10.0729,  11.9320,  11.2619,  13.1210],\n",
      "         [ -8.5766,  -8.1184, -13.4012, -12.9430]],\n",
      "\n",
      "        [[-28.3879, -28.3252, -29.2939, -29.2312],\n",
      "         [ -9.4513, -10.2398,  -9.5233, -10.3119]],\n",
      "\n",
      "        [[ 30.7083,  31.4477,  31.1034,  31.8428],\n",
      "         [ -7.8197,  -7.6676,  -9.7405,  -9.5884]],\n",
      "\n",
      "        [[ 33.4781,  33.8368,  35.2033,  35.5620],\n",
      "         [-12.1435, -11.4792, -13.0755, -12.4112]],\n",
      "\n",
      "        [[ 32.1542,  31.4147,  31.7590,  31.0196],\n",
      "         [ -9.7405,  -9.8927,  -7.8198,  -7.9719]],\n",
      "\n",
      "        [[ 15.2340,  15.9984,  15.6406,  16.4050],\n",
      "         [-17.1156, -16.9126, -18.6475, -18.4446]],\n",
      "\n",
      "        [[ 19.5249,  19.5554,  24.3379,  24.3684],\n",
      "         [ 14.3989,  16.7967,  14.3373,  16.7351]],\n",
      "\n",
      "        [[ 22.1743,  22.2043,  27.0993,  27.1293],\n",
      "         [ 20.8998,  23.2526,  20.8368,  23.1896]],\n",
      "\n",
      "        [[ 16.2558,  16.2898,  22.1575,  22.1914],\n",
      "         [ 20.9860,  23.2867,  20.8986,  23.1994]],\n",
      "\n",
      "        [[ 25.6698,  25.6972,  30.3638,  30.3912],\n",
      "         [ 18.0064,  20.1562,  17.9463,  20.0961]],\n",
      "\n",
      "        [[-25.2626, -25.2541, -29.9689, -29.9605],\n",
      "         [  1.0029,  -0.7911,   0.9809,  -0.8130]]], dtype=torch.float64), 'category': tensor([2, 2, 2, 2, 2, 2, 2, 1, 2, 4, 2, 2, 3, 1, 1, 1, 1, 2, 0, 2, 2, 2])},)\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.3546, grad_fn=<DivBackward0>)\n",
      "Original\n",
      "({'bounding_box': tensor([[[ 19.5394,  19.4686,  24.3477,  24.2769],\n",
      "         [ 26.0547,  27.8949,  26.2404,  28.0806]],\n",
      "\n",
      "        [[-18.2824, -17.4382, -18.7544, -17.9101],\n",
      "         [-10.1678, -10.3856, -11.9956, -12.2134]],\n",
      "\n",
      "        [[-21.2721, -19.5209, -22.4454, -20.6942],\n",
      "         [-10.0973, -10.5923, -14.2437, -14.7388]],\n",
      "\n",
      "        [[ 25.9287,  25.9883,  30.9731,  31.0327],\n",
      "         [ 25.9603,  27.8589,  25.8028,  27.7015]],\n",
      "\n",
      "        [[-20.8448, -20.8228, -25.6233, -25.6013],\n",
      "         [  0.5315,  -1.3409,   0.4746,  -1.3978]],\n",
      "\n",
      "        [[ 12.3660,  12.3279,  16.9098,  16.8718],\n",
      "         [ 11.4989,  13.3130,  11.5950,  13.4092]],\n",
      "\n",
      "        [[-19.6585, -19.6718, -24.0433, -24.0565],\n",
      "         [  7.9724,   6.0639,   8.0021,   6.0936]],\n",
      "\n",
      "        [[ 28.2502,  28.2111,  32.4791,  32.4400],\n",
      "         [ 11.5196,  13.3808,  11.6091,  13.4702]],\n",
      "\n",
      "        [[ -4.6209,  -4.6908,  -0.1193,  -0.1893],\n",
      "         [ 11.0007,  12.8189,  11.1745,  12.9928]],\n",
      "\n",
      "        [[-23.8139, -22.1128, -25.0243, -23.3231],\n",
      "         [-18.5095, -18.9904, -22.7868, -23.2677]],\n",
      "\n",
      "        [[-18.7942, -16.4644, -21.1554, -18.8255],\n",
      "         [-17.4547, -18.1720, -25.1168, -25.8341]],\n",
      "\n",
      "        [[ 12.8715,  12.8632,   8.1547,   8.1464],\n",
      "         [  7.7823,   5.9037,   7.8022,   5.9236]],\n",
      "\n",
      "        [[-14.7313, -14.7320,  -9.9335,  -9.9342],\n",
      "         [ 14.7827,  16.6613,  14.7853,  16.6639]],\n",
      "\n",
      "        [[-15.9629, -14.2118, -17.2318, -15.4806],\n",
      "         [ -9.3788,  -9.8739, -13.8629, -14.3580]],\n",
      "\n",
      "        [[ 20.0043,  19.9653,  24.8731,  24.8340],\n",
      "         [ 11.2533,  13.1145,  11.3564,  13.2175]],\n",
      "\n",
      "        [[ -0.9613,  -0.9292,   3.8358,   3.8679],\n",
      "         [ 22.0385,  23.9168,  21.9574,  23.8357]],\n",
      "\n",
      "        [[-24.2537, -24.2681, -28.8375, -28.8518],\n",
      "         [  4.5956,   2.5311,   4.6266,   2.5621]],\n",
      "\n",
      "        [[  6.1659,   6.1916,  10.8383,  10.8640],\n",
      "         [ 26.4505,  28.2919,  26.3863,  28.2277]],\n",
      "\n",
      "        [[  4.1572,   4.1163,   9.1399,   9.0990],\n",
      "         [ 11.0903,  13.0404,  11.1957,  13.1458]],\n",
      "\n",
      "        [[  2.0164,   2.0171,  -3.4954,  -3.4947],\n",
      "         [  4.1884,   2.3099,   4.1855,   2.3069]]], dtype=torch.float64), 'category': tensor([2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2])},)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.3617, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7132054b3847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPOCH: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkobe_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkobe_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-e1de6089471c>\u001b[0m in \u001b[0;36mtrain_yolo\u001b[0;34m(data_loader, kobe_model, kobe_optimizer, lambd)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mkobe_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(\"EPOCH: {}\".format(epoch))\n",
    "    train_yolo(trainloader, kobe_model, kobe_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to debug architecture\n",
    "z = torch.rand(10 , 5 * 15 * 15)\n",
    "z = ReshapeLayer2d(5, 15)(z)\n",
    "z = nn.Conv2d(5, 5, kernel_size=3, stride = 1)(z)\n",
    "\n",
    "z = nn.MaxPool2d(kernel_size=2, stride = 1)(z)\n",
    "z = nn.Conv2d(5, 5, kernel_size=3, stride = 1)(z)\n",
    "\n",
    "z = nn.MaxPool2d(kernel_size = 2, stride = 1)(z)\n",
    "z = ReshapeLayer1d(405)(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Model Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2560)\n",
      "torch.Size([1, 21, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2560)\n",
      "torch.Size([1, 24, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2561)\n",
      "torch.Size([1, 25, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2561)\n",
      "torch.Size([1, 25, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2562)\n",
      "torch.Size([1, 23, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2562)\n",
      "torch.Size([1, 25, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2563)\n",
      "torch.Size([1, 23, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2563)\n",
      "torch.Size([1, 23, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2563)\n",
      "torch.Size([1, 22, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2563)\n",
      "torch.Size([1, 21, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2563)\n",
      "torch.Size([1, 21, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2564)\n",
      "torch.Size([1, 19, 2, 4])\n",
      "ats bounding box\n",
      "tensor(0.)\n",
      "ts roadmap\n",
      "tensor(0.2565)\n",
      "torch.Size([1, 18, 2, 4])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-46fd19bfa03f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpredicted_bounding_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_yolo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mats_bounding_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_ats_bounding_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_bounding_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bounding_box'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ats bounding box'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mats_bounding_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/DeepLearning/hw/final_proj/deep-learning-project/helper.py\u001b[0m in \u001b[0;36mcompute_ats_bounding_boxes\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_boxes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcondition_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0miou_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_iou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0miou_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miou_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/NYU/SecondYear/SecondSemester/DeepLearning/hw/final_proj/deep-learning-project/helper.py\u001b[0m in \u001b[0;36mcompute_iou\u001b[0;34m(box1, box2)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvex_hull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marea\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marea\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/shapely/geometry/base.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;34m\"\"\"Returns the union of the geometries (Shapely geometry)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgeom_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'union'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;31m# Unary predicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/shapely/geometry/base.py\u001b[0m in \u001b[0;36mgeom_factory\u001b[0;34m(g, parent)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mgeom_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         )\n\u001b[1;32m     86\u001b[0m     \u001b[0mob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_transform_task1(): \n",
    "    return torchvision.transforms.ToTensor()\n",
    "# For road map task\n",
    "def get_transform_task2(): \n",
    "    return torchvision.transforms.ToTensor()\n",
    "\n",
    "labeled_trainset_task1 = LabeledDataset(\n",
    "    image_folder=image_folder,\n",
    "    annotation_file=annotation_csv,\n",
    "    scene_index=labeled_scene_index,\n",
    "    transform=get_transform_task1(),\n",
    "    extra_info=False\n",
    "    )\n",
    "dataloader_task1 = torch.utils.data.DataLoader(\n",
    "    labeled_trainset_task1,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    "    )\n",
    "\n",
    "\n",
    "total = 0\n",
    "total_ats_bounding_boxes = 0\n",
    "total_ts_road_map = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataloader_task1):\n",
    "        total += 1\n",
    "        sample, target, road_image = data\n",
    "        output_yolo, _, output_rm, _ = kobe_model(sample)\n",
    "        \n",
    "        predicted_bounding_boxes = output_yolo[0].cpu()\n",
    "        ats_bounding_boxes = compute_ats_bounding_boxes(predicted_bounding_boxes, target['bounding_box'][0])\n",
    "        print('ats bounding box')\n",
    "        print(ats_bounding_boxes)\n",
    "        \n",
    "        ts_road_map = compute_ts_road_map(output_rm, road_image)\n",
    "        print('ts roadmap')\n",
    "        print(ts_road_map)\n",
    "        \n",
    "        \n",
    "        print(target['bounding_box'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
