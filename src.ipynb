{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to keep the code here. And then export into a .py file.\n",
    "\n",
    "Do not try to modify the .py file directly until this is notebook is gone from each branch.\n",
    "\n",
    "Remember to clear output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils and calculating loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming Coordinates\n",
    "\n",
    "Define the given coordinates as world coordinates\n",
    "\n",
    "Define normalized from upper left bound of world coordinates (translate to there, rotate, and normalize) as our normalized image coordinates (or image coordinates for short).\n",
    "\n",
    "Always facing right in world coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 40\n",
    "WIDTH = 2 * 40\n",
    "HEIGHT = 2 * 40\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "#cuda = torch.cuda.is_available()\n",
    "cuda = False\n",
    "\n",
    "device = 'cuda:0' if cuda else 'cpu'\n",
    "FloatTensor = FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "## input is the \n",
    "# want output that is\n",
    "# [batch_index, class_index, x_center, y_center, width, height, tx1, tx2, tx3, tx4, ty1, ty2, ty3, ty4]\n",
    "# where the tXN ranges from -1 to 1 and is the location of that coordinate in terms of +- w/2 or h/2\n",
    "def transform_target(in_target):\n",
    "    \n",
    "    out_target = []\n",
    "    \n",
    "    for tgt_index in range(len(in_target)):\n",
    "        \n",
    "        #how many boxes for these target\n",
    "        nbox = in_target[tgt_index]['bounding_box'].shape[0]\n",
    "        individual_target = FloatTensor(nbox, 14).fill_(0)\n",
    "        \n",
    "        # CONVERT ALL THE BOUNDING BOXES for an individual sample at once\n",
    "        \n",
    "        bbox = in_target[tgt_index]['bounding_box'].to(device)\n",
    "        translation = FloatTensor(bbox.shape[0], bbox.shape[1], bbox.shape[2])\n",
    "        translation[:, 0, :].fill_(-40)\n",
    "        translation[:, 1, :].fill_(40)\n",
    "\n",
    "        # translate to uppert left\n",
    "        box = bbox - translation\n",
    "        # reflect y\n",
    "        box[:, 1, :].mul_(-1)\n",
    "\n",
    "        x_min = box[:, 0].min(dim = 1)[0]\n",
    "        y_min = box[:, 1].min(dim = 1)[0]\n",
    "        x_max = box[:, 0].max(dim = 1)[0]\n",
    "        y_max = box[:, 1].max(dim = 1)[0]\n",
    "\n",
    "        x_center = ((x_min + x_max) / 2)\n",
    "        y_center = ((y_min + y_max) / 2)\n",
    "        width = (x_max - x_min)\n",
    "        height = (y_max - y_min)\n",
    "\n",
    "        # already normalized\n",
    "        tx = (box [:, 0, :] - x_center.view(-1, 1)) / (width.view(-1, 1) / 2)\n",
    "        ty = (box [:, 1, :] - y_center.view(-1, 1)) / (height.view(-1, 1) / 2)\n",
    "\n",
    "        x_center_n = x_center / WIDTH\n",
    "        y_center_n = y_center / HEIGHT\n",
    "        width_n = width / WIDTH\n",
    "        height_n = height / HEIGHT\n",
    "\n",
    "        individual_target[:, 2] = x_center_n\n",
    "        individual_target[:, 3] = y_center_n\n",
    "        individual_target[:, 4] = width_n\n",
    "        individual_target[:, 5] = height_n\n",
    "        \n",
    "        individual_target[:, 6:10] = tx\n",
    "        individual_target[:, 10:14] = ty\n",
    "        for box_index in range(nbox):\n",
    "            \n",
    "            \n",
    "            category = in_target[tgt_index]['category'][box_index]\n",
    "            \n",
    "            # from which sample in the batch\n",
    "            individual_target[box_index, 0] = tgt_index\n",
    "            # class\n",
    "            individual_target[box_index, 1] = category\n",
    "            \n",
    "        \n",
    "        out_target.append(individual_target)\n",
    "        \n",
    "    return torch.cat(out_target, dim = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load presaved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works by side effects\n",
    "def load_pretask_weight_from_model(model, presaved_encoder):\n",
    "    model.encoder.load_state_dict(presaved_encoder.state_dict())\n",
    "    \n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you want Initialize Our Model with encoder weights from an existing pretask encoder in memory\n",
    "def initialize_model_for_training(presaved_encoder):\n",
    "    model = KobeModel()\n",
    "    load_pretask_weight_from_model(model, presaved_encoder)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this if you want Initialize Our Model with encoder weights from a file\n",
    "def initialize_model_for_training_file(presaved_encoder_file):\n",
    "    presaved_encoder = PreTaskEncoder()\n",
    "    presaved_encoder.load_state_dict(torch.load(presaved_encoder_file))\n",
    "    presaved_encoder.eval()\n",
    "\n",
    "    \n",
    "    return initialize_model_for_training(presaved_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting predictions to the format for competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to calculate bounding boxes and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/master/utils/utils.py\n",
    "\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    y = x.new(x.shape)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
    "    return y\n",
    "\n",
    "\n",
    "def bbox_wh_iou(wh1, wh2):\n",
    "    wh2 = wh2.t()\n",
    "    w1, h1 = wh1[0], wh1[1]\n",
    "    w2, h2 = wh2[0], wh2[1]\n",
    "    inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n",
    "    union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n",
    "    return inter_area / union_area\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes\n",
    "    \"\"\"\n",
    "    if not x1y1x2y2:\n",
    "        # Transform from center and width to exact coordinates\n",
    "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "    else:\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "\n",
    "    # get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "    # Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n",
    "        inter_rect_y2 - inter_rect_y1 + 1, min=0\n",
    "    )\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4):\n",
    "    \"\"\"\n",
    "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
    "    Non-Maximum Suppression to further filter detections.\n",
    "    Returns detections with shape:\n",
    "        (x1, y1, x2, y2, x1, x2, x3, x4, y1, y2, y3, y4, object_conf, class_score, class_pred)\n",
    "\n",
    "        \n",
    "        # where the x1, ..., x4 and y1, ... y4 are stll from -1 to 1\n",
    "        # first x1, y1, x2, y2 are in the grid coordinates and need to be converted back\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "    \n",
    "    #print(\"Are my width's off?\")\n",
    "    print(prediction[..., :4])\n",
    "    \n",
    "    prediction[..., :4] = xywh2xyxy(prediction[..., :4])\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for image_i, image_pred in enumerate(prediction):\n",
    "        # Filter out confidence scores below threshold\n",
    "        image_pred = image_pred[image_pred[:, 12] >= conf_thres]\n",
    "        # If none are remaining => process next image\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        # Object confidence times class confidence\n",
    "        score = image_pred[:, 12] * image_pred[:, 13:].max(1)[0]\n",
    "        # Sort by it\n",
    "        image_pred = image_pred[(-score).argsort()]\n",
    "        class_confs, class_preds = image_pred[:, 13:].max(1, keepdim=True)\n",
    "        detections = torch.cat((image_pred[:, :13], class_confs.float(), class_preds.float()), 1)\n",
    "        # Perform non-maximum suppression\n",
    "        keep_boxes = []\n",
    "        #print(\"DETECTIONS BEFORE NMS\")\n",
    "        #print(detections[:, 0].min(), detections[:, 0].max())\n",
    "        #print(detections[:, 1].min(), detections[:, 1].max())\n",
    "        #print(detections[:, 2].min(), detections[:, 2].max())\n",
    "        #print(detections[:, 3].min(), detections[:, 3].max())\n",
    "        while detections.size(0):\n",
    "            large_overlap = bbox_iou(detections[0, :4].unsqueeze(0), detections[:, :4]) > nms_thres\n",
    "            label_match = detections[0, -1] == detections[:, -1]\n",
    "            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n",
    "            invalid = large_overlap & label_match\n",
    "            weights = detections[invalid, 12:13]\n",
    "            # Merge overlapping bboxes by order of confidence\n",
    "            detections[0, :12] = (weights * detections[invalid, :12]).sum(0) / weights.sum()\n",
    "            keep_boxes += [detections[0]]\n",
    "            detections = detections[~invalid]\n",
    "        if keep_boxes:\n",
    "            output[image_i] = torch.stack(keep_boxes)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n",
    "\n",
    "    BoolTensor = torch.cuda.BoolTensor if pred_boxes.is_cuda else torch.BoolTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n",
    "\n",
    "    nB = pred_boxes.size(0)\n",
    "    nA = pred_boxes.size(1)\n",
    "    nC = pred_cls.size(-1)\n",
    "    nG = pred_boxes.size(2)\n",
    "\n",
    "    # Output tensors\n",
    "    obj_mask = BoolTensor(nB, nA, nG, nG).fill_(0)\n",
    "    noobj_mask = BoolTensor(nB, nA, nG, nG).fill_(1)\n",
    "    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    \n",
    "    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n",
    "    \n",
    "    ### predict additional coordinates for the center within the anchor box\n",
    "    ##### THIS IS OUR ADDITION\n",
    "    ##### RANGE OF THESE VALUES TO BE DETERMINED\n",
    "    tx1 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx2 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx3 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx4 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty1 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty2 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty3 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty4 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "\n",
    "    # Convert to position relative to box\n",
    "    target_boxes = target[:, 2:6] * nG\n",
    "    gxy = target_boxes[:, :2]\n",
    "    gwh = target_boxes[:, 2:]\n",
    "\n",
    "    \n",
    "    \n",
    "    # Get anchors with best iou\n",
    "    ious = torch.stack([bbox_wh_iou(anchor, gwh) for anchor in anchors])\n",
    "    best_ious, best_n = ious.max(0)\n",
    "    # Separate target values\n",
    "    b, target_labels = target[:, :2].long().t()\n",
    "    gx, gy = gxy.t()\n",
    "    gw, gh = gwh.t()\n",
    "    gi, gj = gxy.long().t()\n",
    "    # Set masks\n",
    "    obj_mask[b, best_n, gj, gi] = 1\n",
    "    noobj_mask[b, best_n, gj, gi] = 0\n",
    "\n",
    "    # Set noobj mask to zero where iou exceeds ignore threshold\n",
    "    for i, anchor_ious in enumerate(ious.t()):\n",
    "        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n",
    "\n",
    "    # Coordinates\n",
    "    tx[b, best_n, gj, gi] = gx - gx.floor()\n",
    "    ty[b, best_n, gj, gi] = gy - gy.floor()\n",
    "    \n",
    "    \n",
    "    tx1[b, best_n, gj, gi] = target[:, 6]\n",
    "    tx2[b, best_n, gj, gi] = target[:, 7]\n",
    "    tx3[b, best_n, gj, gi] = target[:, 8]\n",
    "    tx4[b, best_n, gj, gi] = target[:, 9]\n",
    "    \n",
    "    ty1[b, best_n, gj, gi] = target[:, 10]\n",
    "    ty2[b, best_n, gj, gi] = target[:, 11]\n",
    "    ty3[b, best_n, gj, gi] = target[:, 12]\n",
    "    ty4[b, best_n, gj, gi] = target[:, 13]\n",
    "    \n",
    "    # Width and height\n",
    "    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n",
    "    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1] + 1e-16)\n",
    "    # One-hot encoding of label\n",
    "    tcls[b, best_n, gj, gi, target_labels] = 1\n",
    "    # Compute label correctness and iou at best anchor\n",
    "    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n",
    "    # iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=False)\n",
    "\n",
    "    tconf = obj_mask.float()\n",
    "    return (class_mask, obj_mask, noobj_mask, \n",
    "            tx, ty, tw, th, \n",
    "            tx1, tx2, tx3, tx4, \n",
    "            ty1, ty2, ty3, ty4, \n",
    "            tcls, tconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoadMapLoss(pred_rm, target_rm):\n",
    "    bce_loss = nn.BCELoss()\n",
    "\n",
    "    return bce_loss(pred_rm, target_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_joint_loss(yolo_loss, rm_loss, lambd):\n",
    "    return yolo_loss + lambd * rm_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Loop and test loops\n",
    "Not necessarily using data loader.\n",
    "\n",
    "Assuming targets are already pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_yolo(data_loader, kobe_model, kobe_optimizer, lambd = 0.5):\n",
    "    kobe_model.train()\n",
    "    train_loss = 0 \n",
    "        \n",
    "    for sample, target, road_image, extra in trainloader:\n",
    "        sample = torch.stack(sample).to(device)\n",
    "        target = transform_target(target).to(device)\n",
    "        road_image = torch.stack(road_image)\n",
    "\n",
    "        kobe_optimizer.zero_grad()\n",
    "\n",
    "        output_yolo, yolo_loss = kobe_model(sample, yolo_targets = target)\n",
    "\n",
    "        # SHOULD GET LOWER OVER EPOCHS\n",
    "        #print(\"PRINTING output yolo after nms\")\n",
    "        #if output_yolo[0] is not None:\n",
    "        #    print(output_yolo[0].shape)\n",
    "        #    print(output_yolo[0])\n",
    "        #else:\n",
    "        #    print(\"It was none!\")\n",
    "        #    print(output_yolo)\n",
    "        #rm_loss = RoadMapLoss(outputs_rm, batch_rms)\n",
    "\n",
    "        #loss = total_joint_loss(yolo_loss, rm_loss, lambd)\n",
    "\n",
    "        train_loss += yolo_loss.item()\n",
    "        yolo_loss.backward()\n",
    "\n",
    "        kobe_optimizer.step()\n",
    "        \n",
    "    print(\"TRAIN LOSS: {}\".format(train_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-defined anchors. Should honestly come from KMeans on detection boxes but let's see how this does before going complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the code uses only the last 3 anchors so let ssee what this does\n",
    "# width, height\n",
    "\n",
    "#### anchors are supposed to be in terms of number of grid points it would take\n",
    "#### in a 416x416 image (assuming using default of YOLO)\n",
    "#### we are given 80x80\n",
    "### we match in the 416x416 space though\n",
    "### so scale what esteban gave by 5 (5.2 actualy but wtv)\n",
    "anchors = [(5,5), (25, 12), (12, 25), (100, 25), (50, 12), (40, 60)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our YoloLayer for task of object localization\n",
    "\n",
    "Ignoring orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_HIDDEN = 26718\n",
    "class PreTaskEncoder(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(PreTaskEncoder, self).__init__()\n",
    "        # number of different kernels to use\n",
    "        self.n_features = n_features\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=n_features,\n",
    "                               kernel_size=5,\n",
    "                               )\n",
    "        self.conv2 = nn.Conv2d(n_features,\n",
    "                               n_features,\n",
    "                               kernel_size=5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # return an array shape\n",
    "        x = x.view(-1, ENCODER_HIDDEN)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer2d(nn.Module):\n",
    "    def __init__(self, channels, dim):\n",
    "        super(ReshapeLayer2d, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], self.channels, self.dim, self.dim)\n",
    "    \n",
    "class ReshapeLayer1d(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ReshapeLayer1d, self).__init__()\n",
    "        self.features = features\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], self.features)\n",
    "\n",
    "class YoloDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, anchors, num_classes, img_dim=416):\n",
    "        \n",
    "        super(YoloDecoder, self).__init__()\n",
    "        \n",
    "        self.anchors = anchors\n",
    "        self.num_anchors = len(anchors)\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_thres = 0.5\n",
    "        \n",
    "        self.obj_scale = 1\n",
    "        # 100 originally here\n",
    "        # 5 gives about 60 objects detected per image\n",
    "        # 20 gives about 10 per image\n",
    "        # 10 gives about 45\n",
    "        self.noobj_scale = 5\n",
    "        \n",
    "        self.img_dim = img_dim\n",
    "        self.grid_size = 8\n",
    "        \n",
    "        # takes in dense output from encoder or shared decoder and puts into an\n",
    "        # image of dim img_dim\n",
    "\n",
    "        self.m = nn.Sequential(\n",
    "            nn.Linear(6 * ENCODER_HIDDEN, 5 * 15 * 15),\n",
    "            nn.ReLU(),\n",
    "            ReshapeLayer2d(5, 15),\n",
    "            nn.Conv2d(5, 5, kernel_size=3, stride = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride = 1),\n",
    "            nn.Conv2d(5, 5, kernel_size=3, stride = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 1), \n",
    "            ReshapeLayer1d(405),\n",
    "            nn.Linear(405, self.num_anchors * (self.num_classes + 5 + 8) * self.grid_size * self.grid_size)\n",
    "        )\n",
    "        \n",
    "        self.compute_grid_offsets(self.grid_size, cuda)\n",
    "        \n",
    "    def compute_grid_offsets(self, grid_size, cuda=True):\n",
    "        g = self.grid_size\n",
    "        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "        self.stride = self.img_dim / self.grid_size\n",
    "        # Calculate offsets for each grid\n",
    "        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n",
    "        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n",
    "        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])\n",
    "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
    "        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
    "        \n",
    "    def forward(self, x, targets = None):\n",
    "        # Tensors for cuda support\n",
    "        # Tensors for cuda support\n",
    "        x = self.m(x)\n",
    "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
    "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
    "        BoolTensor = torch.cuda.BoolTensor if x.is_cuda else torch.BoolTensor\n",
    "\n",
    "        num_samples = x.shape[0]\n",
    "\n",
    "        prediction = (\n",
    "            x.view(num_samples, self.num_anchors, self.num_classes + 5 + 8, self.grid_size, self.grid_size)\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "            .contiguous()\n",
    "        )\n",
    "\n",
    "        # Get outputs\n",
    "        xc = torch.sigmoid(prediction[..., 0])  # Center x\n",
    "        yc = torch.sigmoid(prediction[..., 1])  # Center y\n",
    "        w = prediction[..., 2]  # Width\n",
    "        h = prediction[..., 3]  # Height\n",
    "        \n",
    "        #### get x1, x2, x3, x4, y1, y2, y3, y4\n",
    "        \n",
    "        x1 = torch.tanh(prediction[..., 4])\n",
    "        x2 = torch.tanh(prediction[..., 5])\n",
    "        x3 = torch.tanh(prediction[..., 6])\n",
    "        x4 = torch.tanh(prediction[..., 7])\n",
    "        y1 = torch.tanh(prediction[..., 8])\n",
    "        y2 = torch.tanh(prediction[..., 9])\n",
    "        y3 = torch.tanh(prediction[..., 10])\n",
    "        y4 = torch.tanh(prediction[..., 11])\n",
    "        \n",
    "        pred_conf = torch.sigmoid(prediction[..., 12])  # Conf\n",
    "        pred_cls = torch.sigmoid(prediction[..., 13:])  # Cls pred.\n",
    "\n",
    "        # Add offset and scale with anchors\n",
    "        # mulitply by stride to convert from grid to image coordinates (of YOLO img of 416)\n",
    "        pred_boxes = FloatTensor(prediction[..., :12].shape)\n",
    "        \n",
    "        pred_boxes[..., 0] = (xc.data + self.grid_x) * self.stride\n",
    "        pred_boxes[..., 1] = (yc.data + self.grid_y) * self.stride\n",
    "        \n",
    "        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w * self.stride\n",
    "        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h * self.stride\n",
    "        \n",
    "        pred_boxes[..., 4] = x1\n",
    "        pred_boxes[..., 5] = x2\n",
    "        pred_boxes[..., 6] = x3\n",
    "        pred_boxes[..., 7] = x4\n",
    "        \n",
    "        pred_boxes[..., 8] = y1\n",
    "        pred_boxes[..., 9] = y2\n",
    "        pred_boxes[..., 10] = y3\n",
    "        pred_boxes[..., 11] = y4\n",
    "        ### need to figure out what to do with all the x1, x2, x3, x4 etc\n",
    "\n",
    "        # ORIGINAL OUTPUTS IN TERMS OF GRID SIZES, DO NOT FORGET TO CONVERT BACK\n",
    "        output = torch.cat(\n",
    "            (\n",
    "                pred_boxes.view(num_samples, -1, 12),\n",
    "                pred_conf.view(num_samples, -1, 1),\n",
    "                pred_cls.view(num_samples, -1, self.num_classes),\n",
    "            ),\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "        if targets is None:\n",
    "            return output, 0\n",
    "        else:\n",
    "            mse_loss = nn.MSELoss()\n",
    "            bce_loss = nn.BCELoss()\n",
    "    \n",
    "            class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tx1, tx2, tx3, tx4, ty1, ty2, ty3, ty4, tcls, tconf = build_targets(\n",
    "                        pred_boxes=pred_boxes,\n",
    "                        pred_cls=pred_cls,\n",
    "                        target=targets,\n",
    "                        anchors=self.scaled_anchors,\n",
    "                        ignore_thres=self.ignore_thres,\n",
    "                    )\n",
    "\n",
    "            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n",
    "            loss_xc = mse_loss(xc[obj_mask], tx[obj_mask])\n",
    "            loss_yc = mse_loss(yc[obj_mask], ty[obj_mask])\n",
    "        \n",
    "\n",
    "            loss_x1 = mse_loss(x1[obj_mask], tx1[obj_mask])\n",
    "            loss_x2 = mse_loss(x2[obj_mask], tx2[obj_mask])\n",
    "            loss_x3 = mse_loss(x3[obj_mask], tx3[obj_mask])\n",
    "            loss_x4 = mse_loss(x4[obj_mask], tx4[obj_mask])\n",
    "            \n",
    "            loss_y1 = mse_loss(y1[obj_mask], ty1[obj_mask])\n",
    "            loss_y2 = mse_loss(y2[obj_mask], ty2[obj_mask])\n",
    "            loss_y3 = mse_loss(y3[obj_mask], ty3[obj_mask])\n",
    "            loss_y4 = mse_loss(y4[obj_mask], ty4[obj_mask])\n",
    "            \n",
    "            loss_w = mse_loss(w[obj_mask], tw[obj_mask])\n",
    "            loss_h = mse_loss(h[obj_mask], th[obj_mask])\n",
    "\n",
    "            loss_conf_obj = bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n",
    "            loss_conf_noobj = bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n",
    "            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n",
    "            loss_cls = bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n",
    "            \n",
    "            #print(\"Losses xc {}, yc {}, x1 {}, x2 {}, x3 {}, x4 {}, y1 {}, y2 {}, y3 {}, y4 {}, \\\n",
    "            #       w {}, h {}, conf {}, cls {}\".format(loss_xc.item(), loss_yc.item(), loss_x1.item(),\n",
    "            #                                          loss_x2.item(), loss_x3.item(), loss_x4.item(),\n",
    "            #                                          loss_y1.item(), loss_y2.item(), loss_y3.item(),\n",
    "            #                                          loss_y4.item(), loss_w.item(), loss_h.item(),\n",
    "            #                                          loss_conf.item(), loss_cls.item()))\n",
    "            total_loss = 25*(loss_xc + loss_yc + \\\n",
    "                         #loss_x1 + loss_x2 + loss_x3 + loss_x4 + \\\n",
    "                         #loss_y1 + loss_y2 + loss_y3 + loss_y4 + \\\n",
    "                         loss_w + loss_h) + loss_conf + 10*loss_cls\n",
    "\n",
    "            return output, total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Model does that does both tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KobeModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, encoder_features, yolo_dim, rm_dim):\n",
    "        super(KobeModel, self).__init__()\n",
    "        \n",
    "        self.yolo_dim = yolo_dim\n",
    "        \n",
    "        self.encoder = PreTaskEncoder(encoder_features)\n",
    "        \n",
    "        \n",
    "        #self.shared_decoder = nn.Sequential()\n",
    "        \n",
    "        self.yolo_decoder = YoloDecoder(anchors = anchors, num_classes = num_classes, img_dim=yolo_dim)\n",
    "        \n",
    "        #self.rm_decoder = RmDecoder(rm_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \n",
    "        # get all the representations laid out like this\n",
    "        x = torch.cat([self.encoder(x[:, i, :]) for i in range(6)], dim = 1)\n",
    "            \n",
    "            \n",
    "        #convert from dense representation from encoder into an image\n",
    "        # x.view(...)\n",
    "        \n",
    "        #x = self.shared_decoder(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, yolo_targets = None, rm_targets = None ):\n",
    "        encoding = self.encode(x)\n",
    "        \n",
    "        output_1, yolo_loss = self.get_bounding_boxes(x, encoding = encoding, targets = yolo_targets)\n",
    "        \n",
    "        # roadmap decoder\n",
    "        #output_2, rm_loss = self.rm_decoder(x, encoding, target = rm_targets)\n",
    "        \n",
    "        # output1 is not in the context of our bounding boxes\n",
    "        #return output_1, output_2, yolo_loss, rm_loss\n",
    "        return output_1, yolo_loss\n",
    "    \n",
    "    # for easy use for competition\n",
    "    # in competition, encoding is None\n",
    "    def get_bounding_boxes(self, x, encoding = None, targets = None):\n",
    "        if encoding is None:\n",
    "            encoding = self.encode(x)\n",
    "        \n",
    "        outputs, yolo_loss = self.yolo_decoder(encoding, targets=targets)\n",
    "\n",
    "        outputs = non_max_suppression(outputs)\n",
    "        \n",
    "        boxes = []\n",
    "        \n",
    "        for output in outputs:\n",
    "            # let's convert it back to center_x, center_y, width and height\n",
    "            if output is None:\n",
    "                boxes.append(None)\n",
    "                continue\n",
    "\n",
    "            better_coordinates = FloatTensor(len(output), 2, 4)\n",
    "            translation = FloatTensor(len(output), 2, 4)\n",
    "            translation[:, 0, :].fill_(-40)\n",
    "            translation[:, 1, :].fill_(40)\n",
    "\n",
    "            center_x = (output[:, 0] + output[:, 2]) / 2 / 416 * WIDTH\n",
    "            center_y = (output[:, 1] + output[:, 3]) / 2 / 416 * HEIGHT\n",
    "            width = output[:, 2] - output[:,0] / 416 * WIDTH\n",
    "            height = output[:, 3] - output[:,1] / 416 * HEIGHT\n",
    "            \n",
    "            #x1 = center_x + output[:, 4] * width/2\n",
    "            #x2 = center_x + output[:, 5] * width/2\n",
    "            #x3 = center_x + output[:, 6] * width/2\n",
    "            #x4 = center_x + output[:, 7] * width/2\n",
    "            x1 = center_x - width/2\n",
    "            x2 = center_x + width/2\n",
    "            x3 = center_x - width/2\n",
    "            x4 = center_x + width/2\n",
    "            \n",
    "            #y1 = center_y + output[:, 8] * height/2\n",
    "            #y2 = center_y + output[:, 9] * height/2\n",
    "            #y3 = center_y + output[:, 10] * height/2\n",
    "            #y4 = center_y + output[:, 11] * height/2  \n",
    "            \n",
    "            y1 = center_y - height/2\n",
    "            y2 = center_y + height/2\n",
    "            y3 = center_y + height/2\n",
    "            y4 = center_y - height/2\n",
    "            \n",
    "            better_coordinates[:, 0, 0] = x1\n",
    "            better_coordinates[:, 0, 1] = x2\n",
    "            better_coordinates[:, 0, 2] = x3\n",
    "            better_coordinates[:, 0, 3] = x4\n",
    "            \n",
    "            better_coordinates[:, 1, 0] = y1\n",
    "            better_coordinates[:, 1, 1] = y2\n",
    "            better_coordinates[:, 1, 2] = y3\n",
    "            better_coordinates[:, 1, 3] = y4\n",
    "            \n",
    "            better_coordinates[:, 1, :].mul_(-1)\n",
    "            # shift back!\n",
    "            better_coordinates += translation\n",
    "            \n",
    "            boxes.append(better_coordinates)\n",
    "            \n",
    "        return tuple(boxes), yolo_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kobe_model = KobeModel(10, 6, 416, 800)\n",
    "kobe_model.to(device)\n",
    "lr = 0.0001\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "\n",
    "kobe_optimizer = torch.optim.Adam(kobe_model.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'data'\n",
    "annotation_csv = 'data/annotation.csv'\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for sample, target, road_image, extra in trainloader:\n",
    "    sample = torch.stack(sample).to(device)\n",
    "    target = transform_target(target).to(device)\n",
    "    road_image = torch.stack(road_image)\n",
    "    \n",
    "    output_yolo, yolo_loss = kobe_model(sample, yolo_target = target)\n",
    "    \n",
    "    # SHOULD GET LOWER OVER EPOCHS\n",
    "    print(output_yolo[0].shape)\n",
    "    yolo_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "tensor([[[ 25.9175,  25.9806,   5.2881,   5.1575],\n",
      "         [ 77.8056,  26.7491,   4.8686,   5.1147],\n",
      "         [131.0104,  25.8035,   4.8143,   5.3228],\n",
      "         ...,\n",
      "         [286.4659, 389.4755,  44.6992,  60.0090],\n",
      "         [337.2295, 389.3102,  38.7945,  58.5879],\n",
      "         [390.2791, 390.7005,  40.0187,  60.4453]],\n",
      "\n",
      "        [[ 25.8836,  25.9350,   5.2948,   5.1469],\n",
      "         [ 77.8246,  26.7647,   4.8516,   5.0990],\n",
      "         [130.9698,  25.7867,   4.8063,   5.3215],\n",
      "         ...,\n",
      "         [286.4187, 389.4896,  44.6454,  60.1032],\n",
      "         [337.2434, 389.3155,  38.6022,  58.4766],\n",
      "         [390.3419, 390.6937,  39.9629,  60.3238]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.7090,  26.0637,   5.6650,   4.7714],\n",
      "         [ 77.5535,  25.3065,   4.7679,   4.7486],\n",
      "         [132.9959,  25.0103,   4.5563,   4.9700],\n",
      "         ...,\n",
      "         [285.9158, 390.2384,  49.3755,  53.6838],\n",
      "         [336.4105, 390.3615,  38.8415,  55.8733],\n",
      "         [388.2345, 391.7006,  39.6427,  63.8727]],\n",
      "\n",
      "        [[ 26.5979,  25.8543,   5.6679,   4.6976],\n",
      "         [ 77.5185,  25.4790,   4.8153,   4.9347],\n",
      "         [132.8930,  25.0458,   4.6722,   5.0079],\n",
      "         ...,\n",
      "         [285.8470, 390.0976,  48.6348,  54.8183],\n",
      "         [336.5052, 390.0471,  38.5344,  55.6638],\n",
      "         [388.6046, 391.6063,  40.2875,  63.1315]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 27.7731,  28.0703,   6.6939,   5.0919],\n",
      "         [ 76.8131,  22.8944,   4.3785,   5.1352],\n",
      "         [134.2504,  25.0102,   3.8173,   6.2551],\n",
      "         ...,\n",
      "         [285.4218, 390.2250,  50.1599,  45.3985],\n",
      "         [333.5099, 387.2075,  37.6680,  58.9934],\n",
      "         [386.9489, 390.4386,  34.2294,  61.6027]],\n",
      "\n",
      "        [[ 27.6923,  28.0612,   6.6613,   5.0431],\n",
      "         [ 76.8910,  22.9759,   4.3500,   5.1531],\n",
      "         [134.1786,  24.7849,   3.8563,   6.2529],\n",
      "         ...,\n",
      "         [285.4088, 390.2305,  50.6152,  45.4660],\n",
      "         [333.5562, 387.0843,  37.6312,  58.8906],\n",
      "         [387.0051, 390.3796,  34.3109,  61.6518]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 27.6206,  27.0683,   5.8619,   4.6317],\n",
      "         [ 76.3361,  21.4431,   4.5840,   5.1679],\n",
      "         [135.0310,  26.0976,   3.9131,   6.1218],\n",
      "         ...,\n",
      "         [284.5396, 390.3477,  41.0243,  43.1327],\n",
      "         [332.7643, 388.9155,  38.5596,  48.1957],\n",
      "         [385.7167, 387.9240,  37.0043,  60.4122]],\n",
      "\n",
      "        [[ 27.6619,  27.0836,   5.9894,   4.6041],\n",
      "         [ 76.3346,  21.2007,   4.5835,   5.1545],\n",
      "         [135.1146,  26.1024,   3.8583,   6.0669],\n",
      "         ...,\n",
      "         [284.3998, 390.3699,  41.1225,  42.7007],\n",
      "         [332.7384, 389.0350,  39.1646,  47.7421],\n",
      "         [385.5702, 388.0929,  37.6055,  60.7952]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 27.5621,  26.1016,   6.2649,   4.7882],\n",
      "         [ 74.9294,  20.9312,   5.1890,   5.2830],\n",
      "         [135.6659,  25.9102,   3.8905,   5.6192],\n",
      "         ...,\n",
      "         [283.4414, 389.7320,  42.0796,  43.9521],\n",
      "         [331.7226, 389.6772,  42.5025,  44.9405],\n",
      "         [384.2305, 387.7023,  43.5826,  67.3967]],\n",
      "\n",
      "        [[ 27.1734,  26.0914,   6.0838,   4.7750],\n",
      "         [ 75.3160,  21.6836,   5.2040,   5.3068],\n",
      "         [134.7770,  25.7981,   4.0323,   5.6380],\n",
      "         ...,\n",
      "         [283.9282, 389.7636,  41.8938,  46.4739],\n",
      "         [332.4143, 389.4452,  42.1718,  47.0727],\n",
      "         [385.2360, 387.9827,  42.7582,  66.3110]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.9256,  25.9107,   5.6948,   5.8111],\n",
      "         [ 75.5342,  22.3666,   5.9657,   4.7913],\n",
      "         [134.4258,  24.7781,   4.0337,   4.8796],\n",
      "         ...,\n",
      "         [283.6738, 387.9735,  42.5149,  47.7869],\n",
      "         [333.5395, 390.4598,  43.2275,  52.4704],\n",
      "         [385.0177, 388.7353,  40.8534,  64.1649]],\n",
      "\n",
      "        [[ 26.9657,  26.0711,   5.7334,   5.8717],\n",
      "         [ 75.5004,  22.0777,   5.9897,   4.7659],\n",
      "         [134.7434,  24.6562,   3.9891,   4.8775],\n",
      "         ...,\n",
      "         [283.5761, 387.9268,  42.5667,  46.8402],\n",
      "         [333.3282, 390.6103,  43.5600,  51.9872],\n",
      "         [384.6129, 388.8205,  41.1800,  64.1008]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.8047,  26.1606,   5.5350,   6.2784],\n",
      "         [ 75.6282,  22.6051,   6.7065,   4.4204],\n",
      "         [134.7614,  24.5582,   4.2583,   4.6833],\n",
      "         ...,\n",
      "         [283.3304, 387.9721,  41.1391,  44.6056],\n",
      "         [334.2684, 391.8597,  41.2034,  52.1216],\n",
      "         [384.8087, 390.0747,  39.9018,  56.2510]],\n",
      "\n",
      "        [[ 26.8872,  26.0004,   5.5752,   6.3015],\n",
      "         [ 75.6425,  22.5841,   6.6754,   4.3795],\n",
      "         [134.5482,  24.4609,   4.2542,   4.6397],\n",
      "         ...,\n",
      "         [283.3585, 388.0151,  41.0353,  45.1949],\n",
      "         [334.2576, 391.5843,  41.5909,  51.8747],\n",
      "         [385.0273, 389.9439,  40.0525,  56.2500]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.6909,  26.7122,   5.3611,   5.9274],\n",
      "         [ 77.3924,  22.7318,   6.4959,   4.1449],\n",
      "         [135.0907,  23.2387,   5.2114,   4.8814],\n",
      "         ...,\n",
      "         [284.1323, 388.1197,  48.7394,  45.6703],\n",
      "         [335.8229, 391.7076,  38.0759,  53.7774],\n",
      "         [386.2505, 389.9547,  40.6447,  57.5439]],\n",
      "\n",
      "        [[ 27.1228,  26.6727,   5.4762,   6.0872],\n",
      "         [ 77.2605,  22.2716,   6.7358,   3.9370],\n",
      "         [135.4402,  22.7818,   5.1970,   4.6949],\n",
      "         ...,\n",
      "         [283.8826, 387.8836,  48.6001,  44.4355],\n",
      "         [335.5274, 391.8666,  37.6811,  52.0677],\n",
      "         [385.6906, 389.8449,  40.8178,  57.5019]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 25.7233,  27.3047,   5.3301,   5.5705],\n",
      "         [ 77.8211,  23.3094,   6.7535,   4.1694],\n",
      "         [134.6495,  22.3482,   5.4669,   4.8054],\n",
      "         ...,\n",
      "         [284.1724, 388.0356,  53.8876,  41.8194],\n",
      "         [336.2412, 390.9437,  40.1779,  56.1337],\n",
      "         [386.4156, 390.2609,  43.6193,  55.9024]],\n",
      "\n",
      "        [[ 25.5182,  27.2072,   5.3294,   5.4252],\n",
      "         [ 77.9588,  23.6048,   6.5451,   4.3448],\n",
      "         [134.2738,  22.7256,   5.3820,   4.9025],\n",
      "         ...,\n",
      "         [284.3946, 388.1895,  53.1481,  43.9534],\n",
      "         [336.3110, 390.7839,  40.5672,  55.8167],\n",
      "         [386.7902, 390.1375,  43.3132,  56.0438]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 25.8083,  26.7873,   5.1716,   5.4769],\n",
      "         [ 78.0947,  24.5629,   6.3963,   4.4346],\n",
      "         [133.8900,  22.2076,   5.2195,   5.0935],\n",
      "         ...,\n",
      "         [284.8393, 388.6577,  57.8391,  44.2630],\n",
      "         [335.7572, 389.6520,  40.8208,  56.4237],\n",
      "         [386.8905, 389.9681,  42.7428,  53.5808]],\n",
      "\n",
      "        [[ 25.9182,  26.7274,   5.2052,   5.6623],\n",
      "         [ 77.9636,  24.2024,   6.5644,   4.3110],\n",
      "         [134.1347,  21.7129,   5.2480,   4.9972],\n",
      "         ...,\n",
      "         [284.6479, 388.6006,  59.0715,  43.5383],\n",
      "         [335.6158, 389.8446,  40.5017,  55.7556],\n",
      "         [386.4711, 389.8578,  42.7815,  52.9070]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 25.8667,  26.7162,   5.0524,   5.3111],\n",
      "         [ 78.6596,  25.3935,   6.2467,   4.5882],\n",
      "         [134.2039,  21.6485,   4.9485,   5.1866],\n",
      "         ...,\n",
      "         [284.4877, 388.7746,  63.0458,  41.6297],\n",
      "         [335.4210, 388.9695,  41.6041,  56.1878],\n",
      "         [386.2696, 390.3060,  42.2709,  50.2983]],\n",
      "\n",
      "        [[ 25.7701,  26.5212,   5.0896,   5.3802],\n",
      "         [ 78.6890,  25.0102,   6.4071,   4.5516],\n",
      "         [134.3185,  21.2906,   4.9713,   5.1067],\n",
      "         ...,\n",
      "         [284.5068, 388.8399,  64.0359,  41.6605],\n",
      "         [335.3575, 388.9181,  41.1988,  55.1098],\n",
      "         [386.1336, 390.1195,  42.7339,  50.4470]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.3999,  26.0872,   5.1027,   5.2893],\n",
      "         [ 77.5167,  25.9144,   6.0214,   4.7270],\n",
      "         [133.0154,  23.5040,   4.8231,   5.0344],\n",
      "         ...,\n",
      "         [284.2451, 389.7366,  55.0788,  47.6880],\n",
      "         [336.1150, 388.8959,  39.5387,  54.0151],\n",
      "         [388.2157, 390.2246,  43.8960,  53.5270]],\n",
      "\n",
      "        [[ 26.2048,  26.4353,   5.1170,   5.2622],\n",
      "         [ 77.7333,  25.8524,   6.1715,   4.6517],\n",
      "         [133.6061,  22.8527,   4.7878,   5.0328],\n",
      "         ...,\n",
      "         [284.0294, 389.6129,  57.8827,  44.5107],\n",
      "         [335.8398, 388.7357,  39.8142,  53.6785],\n",
      "         [387.6583, 390.4474,  44.0086,  52.6310]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.4695,  26.3171,   5.2261,   5.2940],\n",
      "         [ 76.9552,  27.1436,   5.8862,   4.8688],\n",
      "         [133.0279,  23.6414,   4.7239,   4.8382],\n",
      "         ...,\n",
      "         [283.7957, 389.9049,  54.6765,  45.9486],\n",
      "         [336.4178, 388.5892,  40.8389,  53.1594],\n",
      "         [389.1494, 390.9856,  45.4286,  54.7349]],\n",
      "\n",
      "        [[ 26.2704,  26.1180,   5.2274,   5.2086],\n",
      "         [ 77.1167,  26.8804,   5.8926,   4.8511],\n",
      "         [132.8854,  23.5912,   4.7064,   4.8748],\n",
      "         ...,\n",
      "         [284.1031, 389.9274,  54.1154,  47.9820],\n",
      "         [336.5692, 388.7446,  39.7362,  52.7706],\n",
      "         [389.3544, 390.8169,  45.6400,  55.1329]]], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 25.8942,  25.8670,   5.3113,   5.2809],\n",
      "         [ 76.6841,  27.2582,   5.6548,   5.0352],\n",
      "         [132.5363,  23.8014,   4.6515,   4.7573],\n",
      "         ...,\n",
      "         [284.1088, 389.8065,  50.6626,  49.2039],\n",
      "         [336.9559, 388.5555,  39.6052,  53.0070],\n",
      "         [389.9203, 391.1047,  43.9229,  56.9467]],\n",
      "\n",
      "        [[ 25.9309,  25.7752,   5.3744,   5.3698],\n",
      "         [ 76.4867,  27.3305,   5.6898,   5.0119],\n",
      "         [132.6293,  23.5706,   4.6118,   4.7196],\n",
      "         ...,\n",
      "         [283.8746, 389.8037,  51.5170,  49.0889],\n",
      "         [336.7998, 388.5479,  39.7708,  52.4850],\n",
      "         [389.8449, 391.3102,  44.0570,  56.3004]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.0094,  25.6785,   5.3713,   5.2791],\n",
      "         [ 76.3874,  27.3776,   5.4552,   5.0574],\n",
      "         [132.7134,  23.8817,   4.4832,   4.6897],\n",
      "         ...,\n",
      "         [284.1229, 390.1589,  50.2700,  50.4382],\n",
      "         [336.9072, 388.4066,  39.9423,  52.7577],\n",
      "         [389.8074, 391.4793,  43.0897,  57.0054]],\n",
      "\n",
      "        [[ 25.9820,  25.7524,   5.3441,   5.2884],\n",
      "         [ 76.4402,  27.3143,   5.3929,   5.1177],\n",
      "         [132.5270,  24.0947,   4.5187,   4.7307],\n",
      "         ...,\n",
      "         [284.2444, 390.0079,  49.3053,  50.8022],\n",
      "         [336.9753, 388.5277,  40.1115,  53.4604],\n",
      "         [389.8931, 391.4120,  42.5377,  56.9105]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.0479,  25.9306,   5.3519,   5.1200],\n",
      "         [ 76.1991,  26.9120,   5.2537,   5.2138],\n",
      "         [132.4609,  24.6400,   4.4437,   4.6101],\n",
      "         ...,\n",
      "         [284.2508, 389.7569,  48.5087,  53.9943],\n",
      "         [337.0877, 388.7568,  40.6962,  54.5709],\n",
      "         [390.0946, 391.8121,  42.2701,  57.3374]],\n",
      "\n",
      "        [[ 26.4703,  25.3865,   5.3629,   5.2723],\n",
      "         [ 76.1950,  26.7708,   5.2369,   5.2958],\n",
      "         [132.2131,  24.3712,   4.5207,   4.5870],\n",
      "         ...,\n",
      "         [284.4687, 389.7889,  47.5545,  55.8764],\n",
      "         [336.9334, 388.7386,  40.9187,  53.8013],\n",
      "         [390.0807, 391.3467,  41.2845,  56.7040]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 25.9784,  26.3260,   5.3295,   5.0168],\n",
      "         [ 76.3395,  26.8010,   5.1892,   5.2329],\n",
      "         [132.3491,  25.1257,   4.3049,   4.7114],\n",
      "         ...,\n",
      "         [284.2871, 389.6216,  49.1468,  56.5989],\n",
      "         [337.4437, 388.7393,  41.3206,  55.5516],\n",
      "         [390.4502, 392.0145,  43.1330,  58.0138]],\n",
      "\n",
      "        [[ 26.4147,  26.0086,   5.3378,   5.1298],\n",
      "         [ 76.2479,  26.7635,   5.1878,   5.3266],\n",
      "         [132.3116,  25.0115,   4.3885,   4.6656],\n",
      "         ...,\n",
      "         [284.3745, 389.6077,  48.6497,  57.1879],\n",
      "         [337.3576, 388.8234,  41.6861,  55.4572],\n",
      "         [390.5265, 391.8293,  42.3017,  57.4413]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 25.7691,  26.4399,   5.3146,   4.9085],\n",
      "         [ 76.4631,  26.6469,   5.1393,   5.3081],\n",
      "         [132.2857,  25.3231,   4.2428,   4.7591],\n",
      "         ...,\n",
      "         [284.3199, 389.3364,  48.7244,  56.4881],\n",
      "         [337.3880, 388.8571,  41.3033,  55.6835],\n",
      "         [390.6185, 392.1248,  43.1138,  58.2854]],\n",
      "\n",
      "        [[ 25.8274,  26.4578,   5.3161,   4.9162],\n",
      "         [ 76.4841,  26.7225,   5.1152,   5.2933],\n",
      "         [132.1709,  25.2850,   4.2521,   4.8050],\n",
      "         ...,\n",
      "         [284.4193, 389.3630,  48.8592,  56.5248],\n",
      "         [337.4106, 388.7540,  41.2821,  55.3894],\n",
      "         [390.5865, 392.0331,  43.2083,  58.5129]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 25.5208,  26.2811,   5.2917,   4.8294],\n",
      "         [ 76.3454,  26.7208,   5.2317,   5.4308],\n",
      "         [132.1573,  25.2024,   4.3825,   4.6849],\n",
      "         ...,\n",
      "         [284.3835, 389.2397,  48.6430,  56.4783],\n",
      "         [337.1727, 388.9949,  41.2551,  54.3174],\n",
      "         [390.5342, 392.0475,  44.0586,  59.1294]],\n",
      "\n",
      "        [[ 25.8020,  26.2306,   5.2942,   4.8988],\n",
      "         [ 76.5935,  26.7743,   5.1938,   5.4362],\n",
      "         [132.0093,  25.2637,   4.4642,   4.7326],\n",
      "         ...,\n",
      "         [284.5283, 389.2274,  48.0677,  57.1956],\n",
      "         [337.2770, 389.0381,  41.4861,  54.6182],\n",
      "         [390.4633, 391.8440,  43.2394,  59.0213]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 25.4479,  25.9428,   5.3368,   4.8055],\n",
      "         [ 76.4614,  26.7000,   5.3443,   5.5289],\n",
      "         [132.3762,  25.0086,   4.4778,   4.6159],\n",
      "         ...,\n",
      "         [284.3281, 389.5465,  48.8744,  55.9304],\n",
      "         [336.9728, 389.4644,  40.9149,  54.0921],\n",
      "         [390.3287, 392.1766,  44.4056,  58.6462]],\n",
      "\n",
      "        [[ 25.3828,  25.9665,   5.3208,   4.8066],\n",
      "         [ 76.5405,  26.6595,   5.3382,   5.5295],\n",
      "         [132.1434,  25.2366,   4.5387,   4.6577],\n",
      "         ...,\n",
      "         [284.5168, 389.5014,  48.0229,  56.3932],\n",
      "         [337.1185, 389.5354,  40.7622,  55.1705],\n",
      "         [390.2947, 391.8773,  44.1266,  58.7702]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 25.6592,  25.5191,   5.2841,   4.8772],\n",
      "         [ 76.5292,  26.6642,   5.4750,   5.6941],\n",
      "         [132.2773,  25.1218,   4.6535,   4.5986],\n",
      "         ...,\n",
      "         [284.5736, 389.5711,  46.3002,  56.4565],\n",
      "         [336.6897, 389.6864,  41.5514,  55.5360],\n",
      "         [389.8417, 391.8947,  44.0935,  58.7043]],\n",
      "\n",
      "        [[ 25.7009,  25.5168,   5.2924,   4.8803],\n",
      "         [ 76.6809,  26.6690,   5.4488,   5.6764],\n",
      "         [132.1029,  25.1417,   4.6951,   4.6687],\n",
      "         ...,\n",
      "         [284.6302, 389.6648,  46.0087,  56.6888],\n",
      "         [336.7483, 389.6839,  41.2594,  55.5821],\n",
      "         [389.8711, 391.7344,  43.7776,  59.1830]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 25.9556,  25.1061,   5.3501,   4.9537],\n",
      "         [ 76.4401,  26.6115,   5.5298,   5.8454],\n",
      "         [132.3460,  24.8811,   4.7447,   4.5567],\n",
      "         ...,\n",
      "         [284.5091, 389.8538,  46.1100,  55.2493],\n",
      "         [336.1065, 389.7350,  41.5798,  54.8288],\n",
      "         [389.4795, 391.8992,  44.6958,  59.0562]],\n",
      "\n",
      "        [[ 25.9393,  25.0700,   5.3165,   4.9494],\n",
      "         [ 76.5009,  26.5787,   5.5175,   5.8416],\n",
      "         [132.2214,  24.9409,   4.7543,   4.5826],\n",
      "         ...,\n",
      "         [284.6210, 389.7861,  46.0135,  55.4350],\n",
      "         [336.1270, 389.6612,  41.5240,  55.0915],\n",
      "         [389.5359, 391.8076,  44.2249,  59.3834]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.1079,  24.8132,   5.2167,   5.0065],\n",
      "         [ 76.6642,  26.3653,   5.4476,   5.8170],\n",
      "         [131.7290,  25.1173,   4.8732,   4.6285],\n",
      "         ...,\n",
      "         [284.9006, 389.7860,  45.1379,  55.8964],\n",
      "         [335.9765, 389.7348,  40.7135,  55.2334],\n",
      "         [389.5087, 391.3203,  42.4565,  60.6971]],\n",
      "\n",
      "        [[ 25.9411,  24.8605,   5.2560,   4.9505],\n",
      "         [ 76.4203,  26.2720,   5.5225,   5.8538],\n",
      "         [132.0591,  24.8911,   4.8374,   4.5571],\n",
      "         ...,\n",
      "         [284.7733, 389.9758,  45.4278,  54.9110],\n",
      "         [335.6372, 389.9538,  41.2736,  55.1897],\n",
      "         [389.2592, 391.4559,  43.5089,  60.5624]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 25.9796,  24.3131,   5.2238,   4.9804],\n",
      "         [ 76.1940,  26.1001,   5.6233,   6.0261],\n",
      "         [132.5489,  24.7162,   4.9242,   4.2406],\n",
      "         ...,\n",
      "         [284.6924, 390.1218,  45.6389,  52.7183],\n",
      "         [334.7558, 390.3362,  40.8322,  53.9490],\n",
      "         [388.5953, 391.6144,  43.4266,  60.9259]],\n",
      "\n",
      "        [[ 26.0469,  24.2539,   5.1953,   5.0034],\n",
      "         [ 76.1968,  26.0956,   5.6117,   6.0054],\n",
      "         [132.5031,  24.6852,   4.9021,   4.2571],\n",
      "         ...,\n",
      "         [284.7285, 390.0824,  45.6657,  52.8261],\n",
      "         [334.8256, 390.3026,  40.9457,  53.8590],\n",
      "         [388.6094, 391.5878,  43.0628,  60.9910]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.2794,  24.1353,   5.1621,   5.0733],\n",
      "         [ 75.9235,  26.0742,   5.4644,   6.0054],\n",
      "         [132.4152,  24.6856,   4.9788,   4.3165],\n",
      "         ...,\n",
      "         [285.0441, 389.9963,  46.0804,  52.2833],\n",
      "         [334.7969, 390.2422,  40.6124,  53.1924],\n",
      "         [388.7324, 391.4137,  42.1259,  60.4781]],\n",
      "\n",
      "        [[ 26.1273,  24.0581,   5.1513,   5.0417],\n",
      "         [ 75.8502,  25.9767,   5.5518,   6.0942],\n",
      "         [132.6044,  24.6354,   4.9758,   4.2931],\n",
      "         ...,\n",
      "         [284.9126, 390.2583,  45.9827,  50.8059],\n",
      "         [334.4995, 390.4282,  40.8494,  53.1919],\n",
      "         [388.6122, 391.4772,  42.7649,  60.8682]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.4419,  23.6275,   5.1492,   5.1175],\n",
      "         [ 75.5974,  25.7507,   5.3874,   6.1398],\n",
      "         [132.7278,  24.3886,   4.9420,   4.3011],\n",
      "         ...,\n",
      "         [284.9588, 390.2541,  45.7407,  49.8694],\n",
      "         [334.1478, 390.3727,  40.8086,  52.3831],\n",
      "         [388.4268, 391.4465,  41.9060,  61.3688]],\n",
      "\n",
      "        [[ 26.4002,  23.7066,   5.1566,   5.1206],\n",
      "         [ 75.6794,  25.7745,   5.3597,   6.0910],\n",
      "         [132.5533,  24.4427,   4.9672,   4.3254],\n",
      "         ...,\n",
      "         [285.0779, 390.1486,  45.6823,  50.7944],\n",
      "         [334.2919, 390.3540,  40.4635,  52.4100],\n",
      "         [388.5459, 391.3315,  41.3463,  61.3395]]], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 26.5929,  23.4708,   5.1546,   5.1870],\n",
      "         [ 75.2707,  25.6625,   5.2909,   6.1815],\n",
      "         [132.9496,  24.3504,   4.9453,   4.2697],\n",
      "         ...,\n",
      "         [285.0164, 390.1450,  44.9858,  48.6291],\n",
      "         [334.0743, 390.2827,  40.2790,  52.1124],\n",
      "         [388.5005, 391.3679,  41.8017,  61.5098]],\n",
      "\n",
      "        [[ 26.4421,  23.6023,   5.1714,   5.1832],\n",
      "         [ 75.3665,  25.7093,   5.2882,   6.1847],\n",
      "         [132.8865,  24.4247,   4.9406,   4.3109],\n",
      "         ...,\n",
      "         [284.9799, 390.1356,  44.7029,  49.1063],\n",
      "         [334.1483, 390.2796,  40.3201,  52.2629],\n",
      "         [388.4935, 391.4127,  41.8787,  61.6540]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.5921,  23.5670,   5.1195,   5.2536],\n",
      "         [ 75.4334,  25.5167,   5.1352,   6.1279],\n",
      "         [132.8737,  24.4927,   4.9300,   4.5039],\n",
      "         ...,\n",
      "         [285.1304, 390.1676,  44.8294,  49.1490],\n",
      "         [334.2016, 390.1698,  40.1050,  52.8995],\n",
      "         [388.7791, 391.1086,  40.5412,  61.4033]],\n",
      "\n",
      "        [[ 26.6750,  23.7382,   5.1553,   5.2715],\n",
      "         [ 75.5164,  25.5744,   5.1020,   6.0766],\n",
      "         [132.7475,  24.6377,   4.9266,   4.5372],\n",
      "         ...,\n",
      "         [285.1297, 390.1107,  44.4271,  49.4647],\n",
      "         [334.5365, 389.9885,  40.2072,  53.0275],\n",
      "         [388.8354, 391.0374,  40.6800,  61.4424]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.6313,  23.3464,   5.1957,   5.4971],\n",
      "         [ 74.7210,  25.0352,   5.0862,   6.2549],\n",
      "         [133.1972,  24.5310,   4.8760,   4.5435],\n",
      "         ...,\n",
      "         [284.9145, 390.2525,  44.5472,  47.2344],\n",
      "         [334.0085, 389.8270,  41.5001,  53.3645],\n",
      "         [388.6601, 390.9742,  40.6888,  60.6700]],\n",
      "\n",
      "        [[ 26.7407,  23.4922,   5.1459,   5.3932],\n",
      "         [ 74.9646,  25.0137,   5.0363,   6.1620],\n",
      "         [133.2304,  24.4283,   4.9180,   4.5235],\n",
      "         ...,\n",
      "         [284.9798, 390.2062,  44.4829,  47.3735],\n",
      "         [334.0053, 389.8948,  40.4533,  52.9375],\n",
      "         [388.5928, 391.0611,  40.7536,  61.0989]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.5967,  23.4276,   5.1517,   5.4585],\n",
      "         [ 74.9160,  24.7526,   4.9423,   6.1554],\n",
      "         [133.3430,  24.5545,   4.8955,   4.6401],\n",
      "         ...,\n",
      "         [285.0669, 390.1531,  44.3271,  46.3042],\n",
      "         [333.9653, 389.7619,  40.4076,  53.3147],\n",
      "         [388.4618, 391.0772,  40.7496,  60.6479]],\n",
      "\n",
      "        [[ 26.7401,  23.2202,   5.1393,   5.5363],\n",
      "         [ 74.8039,  24.5594,   4.9340,   6.1672],\n",
      "         [133.3468,  24.4783,   4.8846,   4.6274],\n",
      "         ...,\n",
      "         [285.0477, 390.1439,  44.2830,  45.9817],\n",
      "         [333.9706, 389.6983,  40.1876,  52.8092],\n",
      "         [388.4228, 390.9802,  40.4535,  60.1200]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.8287,  23.1624,   5.1402,   5.6128],\n",
      "         [ 74.7063,  24.3828,   4.8892,   6.1749],\n",
      "         [133.4583,  24.7055,   4.8416,   4.6591],\n",
      "         ...,\n",
      "         [285.0456, 390.0131,  43.8810,  44.5113],\n",
      "         [333.8930, 389.5293,  40.1656,  53.4961],\n",
      "         [388.2294, 391.1314,  41.2915,  59.2691]],\n",
      "\n",
      "        [[ 26.7182,  23.2892,   5.1540,   5.6120],\n",
      "         [ 75.1787,  24.6717,   4.9351,   6.0053],\n",
      "         [132.9947,  24.7028,   4.7926,   4.7452],\n",
      "         ...,\n",
      "         [285.2270, 389.9542,  43.9382,  47.0698],\n",
      "         [334.5512, 389.4784,  39.4951,  53.8236],\n",
      "         [388.5892, 390.7349,  40.3998,  58.9497]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 26.5347,  23.5838,   5.1861,   5.6332],\n",
      "         [ 75.2584,  24.7863,   4.9038,   5.9796],\n",
      "         [133.0677,  25.0227,   4.7787,   4.7844],\n",
      "         ...,\n",
      "         [285.1779, 389.9033,  43.5387,  46.6375],\n",
      "         [334.7929, 389.2057,  39.5120,  54.8560],\n",
      "         [388.7004, 390.8932,  40.6314,  59.4495]],\n",
      "\n",
      "        [[ 26.7776,  23.0781,   5.1913,   5.7698],\n",
      "         [ 74.6952,  24.3645,   4.9151,   6.1544],\n",
      "         [133.4691,  24.8159,   4.7614,   4.6493],\n",
      "         ...,\n",
      "         [284.9866, 389.9517,  43.5613,  44.3707],\n",
      "         [334.2483, 389.1319,  39.5728,  54.2833],\n",
      "         [388.2875, 391.0446,  41.2988,  58.5750]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 27.1881,  22.9067,   5.1689,   5.7881],\n",
      "         [ 74.6863,  24.1262,   4.8689,   6.2198],\n",
      "         [133.7789,  24.9415,   4.7107,   4.6224],\n",
      "         ...,\n",
      "         [284.8760, 389.8429,  43.5853,  43.2356],\n",
      "         [334.0391, 389.0131,  38.2004,  54.0512],\n",
      "         [388.1207, 391.2495,  42.1165,  58.9305]],\n",
      "\n",
      "        [[ 26.9956,  23.0411,   5.1736,   5.7724],\n",
      "         [ 74.7670,  24.1435,   4.8725,   6.2129],\n",
      "         [133.7361,  24.9705,   4.7168,   4.6360],\n",
      "         ...,\n",
      "         [284.8781, 389.8120,  43.4710,  43.5392],\n",
      "         [334.0168, 389.0293,  38.3616,  54.2145],\n",
      "         [388.1953, 391.2630,  42.3906,  59.1004]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 27.5021,  22.6666,   5.1554,   5.8993],\n",
      "         [ 74.8189,  23.7904,   4.8050,   6.2023],\n",
      "         [134.1191,  24.8212,   4.6388,   4.4838],\n",
      "         ...,\n",
      "         [284.7743, 389.6114,  43.2791,  43.2865],\n",
      "         [334.0583, 388.8710,  36.6010,  52.9671],\n",
      "         [387.8970, 391.1107,  42.5580,  59.1720]],\n",
      "\n",
      "        [[ 27.4605,  22.8852,   5.1846,   5.8381],\n",
      "         [ 74.9494,  23.9937,   4.7924,   6.0769],\n",
      "         [133.9083,  24.8808,   4.6329,   4.5153],\n",
      "         ...,\n",
      "         [284.8182, 389.5264,  42.8130,  44.2530],\n",
      "         [334.4611, 388.8044,  36.9047,  53.3250],\n",
      "         [388.0916, 390.9519,  42.4337,  59.8308]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 27.6013,  22.8117,   5.1700,   5.8743],\n",
      "         [ 74.7605,  23.8238,   4.8351,   6.0786],\n",
      "         [134.1727,  25.1122,   4.5576,   4.4605],\n",
      "         ...,\n",
      "         [284.7673, 389.6391,  42.1139,  43.7860],\n",
      "         [334.5703, 388.8210,  36.4958,  53.5783],\n",
      "         [388.0943, 390.8195,  42.2945,  59.9756]],\n",
      "\n",
      "        [[ 27.4157,  22.9395,   5.1489,   5.8019],\n",
      "         [ 75.0518,  23.9618,   4.8530,   6.0361],\n",
      "         [134.0509,  25.1087,   4.5878,   4.5277],\n",
      "         ...,\n",
      "         [284.9479, 389.5671,  42.9147,  44.6536],\n",
      "         [334.5342, 388.9037,  36.0278,  53.7473],\n",
      "         [388.2277, 390.7595,  41.9752,  60.5578]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 27.4697,  22.8502,   5.1488,   5.7844],\n",
      "         [ 74.6488,  24.0246,   4.8479,   5.9681],\n",
      "         [134.2236,  25.2440,   4.4917,   4.4700],\n",
      "         ...,\n",
      "         [284.8920, 389.5420,  42.3273,  45.0041],\n",
      "         [334.7248, 388.8474,  35.9991,  53.8212],\n",
      "         [388.3898, 390.5313,  41.5745,  61.0606]],\n",
      "\n",
      "        [[ 27.3833,  22.9923,   5.1705,   5.7292],\n",
      "         [ 74.7657,  24.0680,   4.8527,   5.9463],\n",
      "         [134.1612,  25.3355,   4.5109,   4.5388],\n",
      "         ...,\n",
      "         [284.9317, 389.5282,  42.4171,  45.3533],\n",
      "         [334.7313, 388.8118,  35.6831,  54.2524],\n",
      "         [388.5341, 390.5599,  41.5920,  61.5486]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.0163,  22.3236,   5.2039,   5.7982],\n",
      "         [ 73.8409,  23.5382,   4.7316,   5.9688],\n",
      "         [135.0020,  25.4324,   4.3229,   4.3175],\n",
      "         ...,\n",
      "         [284.5355, 389.4307,  41.2035,  43.5145],\n",
      "         [334.4840, 388.5127,  34.6015,  53.4497],\n",
      "         [388.2139, 390.6182,  41.6643,  61.2249]],\n",
      "\n",
      "        [[ 27.7204,  22.6425,   5.2066,   5.6894],\n",
      "         [ 74.0999,  23.7644,   4.7523,   5.9660],\n",
      "         [134.6614,  25.4884,   4.4386,   4.4424],\n",
      "         ...,\n",
      "         [284.6412, 389.4058,  41.4548,  43.9969],\n",
      "         [334.5378, 388.5972,  34.8235,  53.6412],\n",
      "         [388.4640, 390.5883,  41.7601,  61.7555]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 27.9349,  22.8439,   5.1856,   5.4972],\n",
      "         [ 74.0498,  23.9172,   4.6320,   5.8749],\n",
      "         [134.7453,  25.6230,   4.4240,   4.5451],\n",
      "         ...,\n",
      "         [284.5256, 389.2241,  40.8608,  45.1207],\n",
      "         [334.5714, 388.4688,  34.4601,  52.9116],\n",
      "         [388.6970, 390.5788,  41.6216,  62.4815]],\n",
      "\n",
      "        [[ 27.6958,  23.0820,   5.1861,   5.4564],\n",
      "         [ 74.2864,  24.1594,   4.6429,   5.8287],\n",
      "         [134.4680,  25.6260,   4.4470,   4.5850],\n",
      "         ...,\n",
      "         [284.6130, 389.2538,  41.1444,  46.1236],\n",
      "         [334.7946, 388.5344,  34.6973,  53.2006],\n",
      "         [388.8845, 390.5254,  41.2392,  62.3580]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 27.8178,  23.0381,   5.1055,   5.4927],\n",
      "         [ 74.0998,  24.1069,   4.6093,   5.8462],\n",
      "         [134.7501,  25.8461,   4.5325,   4.6449],\n",
      "         ...,\n",
      "         [284.4863, 389.1418,  40.3647,  45.3136],\n",
      "         [334.4268, 388.5983,  34.4545,  52.2209],\n",
      "         [388.7877, 390.5985,  41.5591,  62.5268]],\n",
      "\n",
      "        [[ 28.3599,  22.5321,   5.1025,   5.5777],\n",
      "         [ 73.7756,  23.8119,   4.5851,   5.8649],\n",
      "         [135.1829,  25.7700,   4.4067,   4.5312],\n",
      "         ...,\n",
      "         [284.2991, 389.1187,  40.0569,  44.2489],\n",
      "         [334.2281, 388.5036,  33.8545,  51.5389],\n",
      "         [388.3646, 390.6981,  41.6645,  62.2441]]], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 28.6708,  22.6273,   5.0218,   5.5732],\n",
      "         [ 73.7143,  23.8137,   4.7144,   5.8285],\n",
      "         [134.6085,  25.8973,   4.3852,   4.6247],\n",
      "         ...,\n",
      "         [284.5594, 389.2187,  42.0190,  44.0605],\n",
      "         [334.2288, 388.6607,  32.9948,  51.5704],\n",
      "         [388.6807, 391.2787,  41.9651,  63.4005]],\n",
      "\n",
      "        [[ 28.4743,  22.7420,   5.0262,   5.5664],\n",
      "         [ 73.7113,  23.9115,   4.7604,   5.8579],\n",
      "         [134.3735,  25.9407,   4.4754,   4.6746],\n",
      "         ...,\n",
      "         [284.6093, 389.2577,  41.8938,  43.9108],\n",
      "         [334.2446, 388.6947,  33.4069,  51.9140],\n",
      "         [388.7943, 391.1723,  42.1775,  63.1500]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.4096,  23.1744,   4.9601,   5.4309],\n",
      "         [ 74.1313,  24.0834,   4.8670,   5.6961],\n",
      "         [133.7515,  25.9664,   4.4566,   4.7326],\n",
      "         ...,\n",
      "         [285.0010, 389.1458,  44.1829,  45.0489],\n",
      "         [334.3570, 388.7625,  33.6575,  51.8976],\n",
      "         [388.9450, 391.4283,  42.7436,  63.6647]],\n",
      "\n",
      "        [[ 28.5054,  23.3192,   4.9542,   5.4550],\n",
      "         [ 74.2461,  24.2561,   4.8393,   5.7324],\n",
      "         [133.5924,  25.9079,   4.4168,   4.6720],\n",
      "         ...,\n",
      "         [284.9925, 389.2251,  44.1967,  45.3164],\n",
      "         [334.5488, 388.6075,  34.1705,  51.9546],\n",
      "         [388.8870, 391.5608,  42.4417,  62.5523]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.2264,  23.6648,   4.8876,   5.2909],\n",
      "         [ 74.5152,  24.4657,   5.0119,   5.5896],\n",
      "         [133.1675,  25.9754,   4.5159,   4.7782],\n",
      "         ...,\n",
      "         [285.3310, 389.1299,  45.7634,  45.9929],\n",
      "         [334.4247, 388.9365,  33.8907,  52.4883],\n",
      "         [389.1272, 391.4878,  42.1725,  63.6520]],\n",
      "\n",
      "        [[ 28.7018,  23.3103,   4.8487,   5.3812],\n",
      "         [ 74.0833,  23.9889,   5.0101,   5.6444],\n",
      "         [133.5705,  26.0617,   4.4781,   4.7167],\n",
      "         ...,\n",
      "         [285.2369, 389.0384,  45.7954,  44.0129],\n",
      "         [333.9973, 388.8377,  33.1803,  51.6650],\n",
      "         [388.8224, 391.7953,  42.9417,  64.1633]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.2715,  23.6528,   4.7967,   5.3198],\n",
      "         [ 74.2991,  24.1892,   5.2462,   5.5215],\n",
      "         [133.2270,  26.2906,   4.5852,   4.7878],\n",
      "         ...,\n",
      "         [285.4157, 389.0219,  46.6836,  44.0413],\n",
      "         [334.0422, 389.0577,  33.6285,  53.0743],\n",
      "         [388.9501, 391.8003,  43.3064,  64.6137]],\n",
      "\n",
      "        [[ 27.9775,  23.9831,   4.8398,   5.2338],\n",
      "         [ 74.6614,  24.5595,   5.1740,   5.4897],\n",
      "         [132.8691,  26.2091,   4.5816,   4.8406],\n",
      "         ...,\n",
      "         [285.4784, 389.1238,  46.0992,  46.2229],\n",
      "         [334.5133, 389.0769,  34.2580,  53.4876],\n",
      "         [389.1693, 391.4894,  42.6436,  64.2934]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.1436,  23.9415,   4.7430,   5.2985],\n",
      "         [ 74.5023,  24.2749,   5.4085,   5.4258],\n",
      "         [132.8575,  26.4970,   4.5966,   4.8752],\n",
      "         ...,\n",
      "         [285.5621, 389.1183,  47.1794,  45.0239],\n",
      "         [334.1814, 389.2746,  33.8158,  53.8286],\n",
      "         [388.9643, 391.8434,  43.1185,  64.3719]],\n",
      "\n",
      "        [[ 27.7171,  24.2187,   4.8083,   5.2211],\n",
      "         [ 74.8963,  24.5813,   5.3542,   5.4075],\n",
      "         [132.5819,  26.3922,   4.6622,   4.8876],\n",
      "         ...,\n",
      "         [285.6213, 389.1227,  46.3334,  46.7363],\n",
      "         [334.5870, 389.2645,  34.7572,  54.6634],\n",
      "         [389.2659, 391.5321,  43.0619,  63.7529]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 27.5314,  24.3677,   4.7285,   5.1982],\n",
      "         [ 75.1673,  24.6150,   5.4534,   5.3017],\n",
      "         [132.3475,  26.5683,   4.6831,   4.9184],\n",
      "         ...,\n",
      "         [285.6738, 388.9960,  47.1605,  46.9693],\n",
      "         [334.6435, 389.4179,  34.1756,  55.5357],\n",
      "         [389.2607, 391.7031,  43.1068,  63.6854]],\n",
      "\n",
      "        [[ 28.0606,  24.0605,   4.6433,   5.2986],\n",
      "         [ 74.6739,  24.0157,   5.5874,   5.3343],\n",
      "         [132.7071,  26.7029,   4.6126,   4.8574],\n",
      "         ...,\n",
      "         [285.5908, 388.9518,  47.8274,  44.4434],\n",
      "         [334.1116, 389.4426,  33.6732,  54.8683],\n",
      "         [388.7787, 392.1397,  43.9443,  64.3627]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 27.9497,  24.0340,   4.6573,   5.2742],\n",
      "         [ 74.8407,  24.2977,   5.7413,   5.3750],\n",
      "         [132.3829,  26.8833,   4.7132,   4.9843],\n",
      "         ...,\n",
      "         [285.6928, 389.0740,  48.2459,  43.7055],\n",
      "         [334.0554, 389.6897,  33.3251,  55.9513],\n",
      "         [388.7631, 392.3088,  43.5744,  63.3535]],\n",
      "\n",
      "        [[ 27.8674,  24.1259,   4.5976,   5.2599],\n",
      "         [ 75.0112,  24.2774,   5.6857,   5.2339],\n",
      "         [132.3830,  26.8792,   4.7073,   4.9816],\n",
      "         ...,\n",
      "         [285.7318, 388.9973,  48.8438,  44.8059],\n",
      "         [333.9038, 389.7797,  33.2230,  55.0109],\n",
      "         [388.6423, 392.2762,  43.8874,  63.8118]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.2037,  24.0554,   4.5054,   5.2551],\n",
      "         [ 74.8757,  24.2069,   5.7795,   5.2016],\n",
      "         [132.4264,  27.0348,   4.6443,   4.9094],\n",
      "         ...,\n",
      "         [285.5602, 389.1210,  48.5660,  44.3168],\n",
      "         [333.8306, 389.7822,  33.7143,  55.7244],\n",
      "         [388.3488, 392.3671,  43.9622,  62.2223]],\n",
      "\n",
      "        [[ 27.9974,  24.1147,   4.5731,   5.2613],\n",
      "         [ 75.0459,  24.4171,   5.6805,   5.2294],\n",
      "         [132.3258,  26.8731,   4.6477,   4.8807],\n",
      "         ...,\n",
      "         [285.5527, 389.1339,  47.4972,  45.9528],\n",
      "         [334.1595, 389.6818,  34.0906,  55.7297],\n",
      "         [388.5747, 392.1215,  43.5959,  62.1950]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.4794,  23.7636,   4.4938,   5.3227],\n",
      "         [ 74.9099,  24.3456,   5.8368,   5.1820],\n",
      "         [132.5819,  26.8994,   4.5922,   4.8450],\n",
      "         ...,\n",
      "         [285.6966, 389.2426,  48.5896,  44.1532],\n",
      "         [333.7598, 389.7031,  33.7628,  55.8051],\n",
      "         [387.9742, 392.3181,  44.3442,  61.2198]],\n",
      "\n",
      "        [[ 28.2967,  23.9259,   4.5311,   5.2601],\n",
      "         [ 75.0019,  24.5111,   5.8192,   5.1908],\n",
      "         [132.3674,  26.9543,   4.6119,   4.8702],\n",
      "         ...,\n",
      "         [285.6691, 389.2217,  48.0773,  44.4130],\n",
      "         [333.9351, 389.5760,  34.2063,  55.6822],\n",
      "         [388.2619, 392.1622,  44.3833,  61.1509]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.1185,  24.0766,   4.6461,   5.3174],\n",
      "         [ 75.3351,  25.0216,   5.7444,   5.2403],\n",
      "         [132.1894,  26.6771,   4.5508,   4.7243],\n",
      "         ...,\n",
      "         [285.7391, 389.3814,  47.1802,  46.8394],\n",
      "         [334.4193, 389.4042,  34.6520,  56.7300],\n",
      "         [388.3473, 391.8260,  43.7490,  60.5742]],\n",
      "\n",
      "        [[ 28.3837,  23.8149,   4.5569,   5.3847],\n",
      "         [ 74.9852,  24.6447,   5.8551,   5.2405],\n",
      "         [132.4350,  26.7953,   4.5532,   4.7406],\n",
      "         ...,\n",
      "         [285.6836, 389.3942,  47.2745,  44.9858],\n",
      "         [334.0623, 389.5026,  34.4019,  56.9128],\n",
      "         [388.1198, 392.0847,  44.1850,  60.5446]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.8510,  23.4899,   4.6233,   5.5180],\n",
      "         [ 74.8203,  24.6812,   5.8843,   5.2817],\n",
      "         [132.6458,  26.7653,   4.4858,   4.6007],\n",
      "         ...,\n",
      "         [285.5957, 389.4908,  46.9506,  44.3562],\n",
      "         [334.0105, 389.3214,  34.1808,  57.2371],\n",
      "         [387.7396, 392.2010,  44.9524,  59.5423]],\n",
      "\n",
      "        [[ 28.7075,  23.7829,   4.6902,   5.4835],\n",
      "         [ 75.1088,  24.8640,   5.7066,   5.2905],\n",
      "         [132.4993,  26.5932,   4.4684,   4.5615],\n",
      "         ...,\n",
      "         [285.5939, 389.5004,  46.6003,  46.5319],\n",
      "         [334.2953, 389.1790,  34.5728,  56.8691],\n",
      "         [387.8977, 391.8900,  44.3192,  59.7467]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.3169,  23.7045,   4.7718,   5.4610],\n",
      "         [ 75.1613,  24.9777,   5.7176,   5.3543],\n",
      "         [132.4708,  26.7081,   4.6075,   4.6410],\n",
      "         ...,\n",
      "         [285.6304, 389.5440,  45.7378,  46.0129],\n",
      "         [334.4049, 389.2102,  35.0953,  57.8479],\n",
      "         [388.0029, 391.7813,  44.4345,  58.7633]],\n",
      "\n",
      "        [[ 28.6303,  23.3161,   4.7214,   5.4874],\n",
      "         [ 74.9301,  24.8959,   5.8383,   5.3800],\n",
      "         [132.6067,  26.8970,   4.5832,   4.6251],\n",
      "         ...,\n",
      "         [285.7080, 389.6419,  46.8208,  44.2071],\n",
      "         [333.9808, 389.2515,  34.3442,  57.8878],\n",
      "         [387.6736, 392.0655,  45.1293,  58.2051]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.2347,  23.3097,   4.8481,   5.5121],\n",
      "         [ 75.2269,  25.0980,   5.7492,   5.5213],\n",
      "         [132.6196,  26.8343,   4.6274,   4.5721],\n",
      "         ...,\n",
      "         [285.7316, 389.6356,  46.2050,  43.2314],\n",
      "         [334.1317, 389.1386,  35.1881,  58.0951],\n",
      "         [387.7464, 391.8900,  46.4638,  57.6705]],\n",
      "\n",
      "        [[ 28.2064,  23.3923,   4.8752,   5.5153],\n",
      "         [ 75.3106,  25.2018,   5.6955,   5.5138],\n",
      "         [132.5185,  26.7672,   4.5817,   4.5307],\n",
      "         ...,\n",
      "         [285.6779, 389.7760,  45.8935,  44.2795],\n",
      "         [334.4061, 389.1017,  35.2406,  57.6712],\n",
      "         [387.7719, 391.7552,  45.6464,  57.5270]]], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 28.6445,  22.8344,   4.8192,   5.6426],\n",
      "         [ 75.0252,  25.0510,   5.7130,   5.5610],\n",
      "         [133.1118,  26.8425,   4.4955,   4.4015],\n",
      "         ...,\n",
      "         [285.7598, 389.6841,  46.2304,  40.8673],\n",
      "         [333.9990, 389.0581,  34.5332,  57.9696],\n",
      "         [387.3927, 392.1495,  47.4003,  56.8139]],\n",
      "\n",
      "        [[ 27.9619,  23.5888,   4.9294,   5.4854],\n",
      "         [ 75.5088,  25.4239,   5.5118,   5.5095],\n",
      "         [132.6772,  26.6503,   4.6212,   4.5318],\n",
      "         ...,\n",
      "         [285.6957, 389.6865,  44.9503,  44.6343],\n",
      "         [334.8429, 389.0333,  35.5432,  58.4206],\n",
      "         [388.1906, 391.5636,  45.2984,  57.4109]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.6347,  22.8138,   4.8420,   5.7192],\n",
      "         [ 75.0384,  25.0825,   5.5843,   5.5800],\n",
      "         [133.4277,  26.7942,   4.4366,   4.3630],\n",
      "         ...,\n",
      "         [285.7943, 389.5770,  45.7393,  40.2373],\n",
      "         [334.1660, 389.0793,  34.3379,  58.4582],\n",
      "         [387.4103, 392.0491,  47.5702,  56.8203]],\n",
      "\n",
      "        [[ 28.3638,  23.0534,   4.8766,   5.6689],\n",
      "         [ 75.1639,  25.1508,   5.5170,   5.5843],\n",
      "         [133.1791,  26.7741,   4.4654,   4.4244],\n",
      "         ...,\n",
      "         [285.6880, 389.6177,  45.0343,  41.1894],\n",
      "         [334.4662, 389.0159,  34.8910,  58.3337],\n",
      "         [387.6997, 391.8798,  46.6251,  56.9490]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.2119,  23.0132,   4.8958,   5.6364],\n",
      "         [ 75.2365,  25.1773,   5.3551,   5.6048],\n",
      "         [133.2877,  26.7066,   4.5071,   4.3847],\n",
      "         ...,\n",
      "         [285.7296, 389.4666,  45.1875,  40.2816],\n",
      "         [334.5813, 388.9895,  35.3776,  57.5677],\n",
      "         [387.5682, 391.7942,  46.7295,  56.0373]],\n",
      "\n",
      "        [[ 28.0839,  23.1545,   4.9280,   5.5729],\n",
      "         [ 75.3782,  25.3147,   5.3320,   5.6397],\n",
      "         [133.0721,  26.6808,   4.5178,   4.4170],\n",
      "         ...,\n",
      "         [285.6792, 389.5162,  44.9730,  41.1660],\n",
      "         [334.7546, 388.9785,  35.4236,  57.2810],\n",
      "         [387.6510, 391.6272,  45.8566,  56.1610]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.1161,  22.9396,   4.9065,   5.6239],\n",
      "         [ 75.3427,  25.1904,   5.2688,   5.5607],\n",
      "         [133.2920,  26.5572,   4.4991,   4.3746],\n",
      "         ...,\n",
      "         [285.8227, 389.4640,  45.4206,  39.2402],\n",
      "         [334.8772, 388.8572,  36.0695,  57.1626],\n",
      "         [387.5353, 391.8493,  46.5519,  55.5302]],\n",
      "\n",
      "        [[ 27.8734,  23.1491,   4.9261,   5.5569],\n",
      "         [ 75.5329,  25.3209,   5.2673,   5.6043],\n",
      "         [133.0486,  26.5626,   4.5455,   4.4457],\n",
      "         ...,\n",
      "         [285.7740, 389.6324,  45.0478,  40.2673],\n",
      "         [335.0417, 388.8705,  36.0447,  57.0420],\n",
      "         [387.6497, 391.6074,  45.4623,  55.5527]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.0608,  22.8951,   4.9286,   5.6088],\n",
      "         [ 75.5430,  25.3242,   5.2283,   5.4877],\n",
      "         [133.3176,  26.4447,   4.4465,   4.3577],\n",
      "         ...,\n",
      "         [285.8395, 389.4468,  45.1372,  38.1833],\n",
      "         [335.1877, 388.7263,  36.3618,  56.8800],\n",
      "         [387.4716, 391.9085,  46.7381,  54.9765]],\n",
      "\n",
      "        [[ 28.2581,  22.5589,   4.8834,   5.6884],\n",
      "         [ 75.3537,  25.1461,   5.2811,   5.4742],\n",
      "         [133.6387,  26.4573,   4.4121,   4.3026],\n",
      "         ...,\n",
      "         [285.8716, 389.4457,  45.6486,  36.9347],\n",
      "         [334.8703, 388.6934,  36.1394,  56.4677],\n",
      "         [387.1276, 392.0750,  47.4196,  54.6609]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 27.9642,  22.8454,   4.9020,   5.6545],\n",
      "         [ 75.6172,  25.1906,   5.3083,   5.3807],\n",
      "         [133.5451,  26.4117,   4.4457,   4.3821],\n",
      "         ...,\n",
      "         [285.9835, 389.3928,  45.4453,  37.2558],\n",
      "         [335.1611, 388.6893,  36.7507,  56.7436],\n",
      "         [387.4803, 392.0692,  47.7892,  55.2459]],\n",
      "\n",
      "        [[ 27.9523,  22.8931,   4.9574,   5.6133],\n",
      "         [ 75.8832,  25.4012,   5.2545,   5.4426],\n",
      "         [133.3628,  26.4212,   4.4561,   4.4448],\n",
      "         ...,\n",
      "         [285.7660, 389.4457,  45.1167,  36.4423],\n",
      "         [335.4229, 388.6256,  37.2234,  56.8675],\n",
      "         [387.7408, 391.8799,  47.1884,  54.2296]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.1095,  22.6637,   4.9139,   5.6632],\n",
      "         [ 75.6271,  25.1845,   5.2838,   5.3371],\n",
      "         [133.7442,  26.3420,   4.4259,   4.3384],\n",
      "         ...,\n",
      "         [285.9018, 389.4452,  45.4190,  35.7581],\n",
      "         [335.3134, 388.6564,  37.3033,  56.1421],\n",
      "         [387.3473, 392.2148,  48.1741,  53.9925]],\n",
      "\n",
      "        [[ 28.1164,  22.6335,   4.9192,   5.6940],\n",
      "         [ 75.5921,  25.1194,   5.2828,   5.3256],\n",
      "         [133.6557,  26.2916,   4.4146,   4.2814],\n",
      "         ...,\n",
      "         [285.8700, 389.3913,  45.1532,  36.2724],\n",
      "         [335.3259, 388.6800,  37.2613,  56.2192],\n",
      "         [387.2225, 392.2359,  47.8881,  54.5950]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 27.8207,  23.1040,   4.9650,   5.5760],\n",
      "         [ 75.8583,  25.4085,   5.1419,   5.3091],\n",
      "         [133.5570,  26.1267,   4.5131,   4.3950],\n",
      "         ...,\n",
      "         [286.0066, 389.3748,  44.9480,  37.1817],\n",
      "         [335.7701, 388.6364,  37.9995,  55.5177],\n",
      "         [387.6151, 391.9906,  46.8527,  54.7967]],\n",
      "\n",
      "        [[ 28.1941,  22.6329,   4.9177,   5.6758],\n",
      "         [ 75.5826,  25.1788,   5.1790,   5.2597],\n",
      "         [133.8675,  26.1359,   4.4098,   4.2929],\n",
      "         ...,\n",
      "         [285.9799, 389.4222,  44.6651,  35.5401],\n",
      "         [335.5661, 388.5645,  37.7034,  55.3377],\n",
      "         [387.1586, 392.2090,  47.8312,  54.1870]]], grad_fn=<SliceBackward>)\n",
      "tensor([[[ 28.3114,  22.5843,   4.8791,   5.6405],\n",
      "         [ 75.6574,  25.0666,   5.2180,   5.2271],\n",
      "         [133.9553,  26.2105,   4.4261,   4.2490],\n",
      "         ...,\n",
      "         [286.0956, 389.2838,  45.1890,  34.0075],\n",
      "         [335.5812, 388.6487,  37.7573,  53.8768],\n",
      "         [387.1036, 392.5349,  48.3957,  54.8018]],\n",
      "\n",
      "        [[ 28.0331,  23.1608,   4.9240,   5.5750],\n",
      "         [ 75.7766,  25.3080,   5.1303,   5.2274],\n",
      "         [133.5510,  25.9931,   4.4970,   4.3868],\n",
      "         ...,\n",
      "         [286.0758, 389.2566,  44.4386,  37.3208],\n",
      "         [335.9814, 388.6311,  38.4328,  55.1383],\n",
      "         [387.6823, 392.1248,  46.7708,  55.6641]]], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-23b40a000e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPOCH: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkobe_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkobe_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-061cd340a9db>\u001b[0m in \u001b[0;36mtrain_yolo\u001b[0;34m(data_loader, kobe_model, kobe_optimizer, lambd)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0myolo_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mkobe_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRAIN LOSS: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(\"EPOCH: {}\".format(epoch))\n",
    "    train_yolo(trainloader, kobe_model, kobe_optimizer, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to debug architecture\n",
    "z = torch.rand(10 , 5 * 15 * 15)\n",
    "z = ReshapeLayer2d(5, 15)(z)\n",
    "z = nn.Conv2d(5, 5, kernel_size=3, stride = 1)(z)\n",
    "\n",
    "z = nn.MaxPool2d(kernel_size=2, stride = 1)(z)\n",
    "z = nn.Conv2d(5, 5, kernel_size=3, stride = 1)(z)\n",
    "\n",
    "z = nn.MaxPool2d(kernel_size = 2, stride = 1)(z)\n",
    "z = ReshapeLayer1d(405)(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
